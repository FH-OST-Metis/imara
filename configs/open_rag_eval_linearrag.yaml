# Evaluation config for running open-rag-eval on the LinearRAG pipeline

input_queries: "data/eval/queries.csv"
results_folder: "data/eval/linearrag"

# This is the CSV produced by src/app/eval_open_rag.py when --system linearrag
generated_answers: "generated_answers.csv"

# Consolidated evaluation results and metrics plot
# (relative to results_folder)
eval_results_file: "results.csv"
metrics_file: "metrics.png"

evaluator:
  - type: "GoldenAnswerEvaluator"
    model:
      type: "GeminiModel"
      name: "gemini-2.5-flash"
      api_key: ${oc.env:GEMINI_API_KEY}
    embedding_model:
      type: "GeminiModel"
      name: "bge-m3:567m"
      api_key: ${oc.env:GEMINI_API_KEY}
    options:
      run_consistency: true
      metrics_to_run_consistency:
        - "semantic_similarity"
        - "factual_correctness_f1"
