## Super-hedging-pricing formulas and Immediate-Profit arbitrage for market models under random horizon

Tahir Choulli

Mathematical and Statistical Sciences Dept. University of Alberta, Edmonton, Canada

Emmanuel Lepinette

Ceremade, UMR CNRS 7534, Paris Dauphine University, PSL National Research, Place du Mar´ echal De Lattre De Tassigny, 75775 Paris cedex 16, France and Gosaef, Faculty of Sciences of Tunis, Tunisia. Email: emmanuel.lepinette@ceremade.dauphine.fr

January 12, 2024

## Abstract

In this paper, we consider the discrete-time setting, and the market model described by ( S, F , τ ). Herein F is the 'public' flow of information which is available to all agents overtime, S is the discounted price process of d -tradable assets, and τ is an arbitrary random time whose occurrence might not be observable via F . Thus, we consider the larger flow G which incorporates F and makes τ an observable random time. This framework covers the credit risk theory setting, the life insurance setting and the setting of employee stock option valuation. For the stopped model ( S τ , G ) and for various vulnerable claims, based on this model, we address the super-hedging pricing valuation problem and its intrinsic Immediate-Profit arbitrage (IP hereafter for short). Our first main contribution lies in singling out the impact of change of prior and/or information on conditional essential supremum , which is a vital tool in super-hedging pricing. The second main contribution consists of describing as explicit as possible how the set of super-hedging prices expands under the stochasticity of τ and its risks, and we address the IP arbitrage for ( S τ , G ) as well. The third main contribution resides in elaborating as explicit as possible pricing formulas for vulnerable claims, and singling out the various informational risks in the prices' dynamics. Random horizon default/death time vulnerable claims/options progressive enlargement of filtration super-hedging pricing conditional essential supremumImmediate-profit arbitrage.

## 1 Introduction

In this paper, we consider a general initial discrete-time model represented by the pair ( S, F ) and an arbitrary random horizon τ . Herein, S is the assets' price process and F is the 'public' flow of information which is available to all agents overtime, while the random time τ might not be observable via the flow F . This random time represents the default of a firm in credit risk theory, the death time of an insured in life insurance, the job's termination time of an employee-stock-option 's holder (called ESO hereafter) in finance, ..., etcetera. Hence, our current setting covers these three aforementioned

frameworks, and for more about these we refer the reader to [3, 5, 28, 29, 30, 31, 26] and the references therein to cite a few. As a random time can not be observable before its occurrence, the progressive enlargement approach of F with τ seems the tailor-fit method for our setting in modelling mathematically the large flow of information which incorporates both F and τ . This larger flow will be denoted throughout the paper by G and will be defined more precisely in the next section. Therefore, our main objective lies in addressing the evaluation problem for the stopped financial model ( S τ , G , P ), and focus on the super-hedging pricing approach and its intrinsic arbitrage called immediate-profit .

The super-hedging price of the financial claim/asset C is the infinimum amount required to initiate a hedging strategy for C . The super-hedging pricing concept was introduced in Bensaid et al. in [4] for the binomial framework with transaction costs. Afterwards, the characterization and/or computation of the super-hedging price became a central problem in mathematical finance, and we refer the reader to [17, 25] and the references therein to cite a few. These studies were carried out under the main assumption of no-arbitrage using the fundamental theorem of asset pricing theorem (FTAP hereafter). Using this FTAP, the main results on the super-hedging pricing consist of establishing a dual formulation for the price using martingale measures or deflators, see [16, 27, 18] and the references therein.

Recently in [6], the authors consider the discrete-time model without transaction costs and addressed this super-hedging pricing issue differently and without any non-arbitrage assumption. As a result, the authors discovered that this infimum price is in fact a price if and only if the model fulfills a weaker form of non-arbitrage, called Absence of Immediate Profit (AIP hereafter for short). This novel notion of non-arbitrage is weaker than the classical non-arbitrage concepts which all coincide in discrete-time setting. Besides the AIP concept, using the conditional essential supremum as their main mathematical tool, the authors derived a backward equation/algorithm for calculating the superhedging price process ̂ P = ( P t ) t =0 ,...,T as follows

Here ̂ P t,t +1 ( · ) is the one-period super-hedging pricing operator for the period t , which can also be seen as the concave envelop of the payoff relatively to the convex envelop of the conditional support. The conditional essential supremum notion was introduced in [2], [23] and developed in [15], [1], [6], [24] and the references therein to cite a few.

$$\text {automatic tool, the authors derived a backward equation/algorithm for calculating the super-} \\ \text {price process} \, \widehat { \mathcal { P } } = ( \widehat { \mathcal { P } } _ { t } ) _ { t = 0 , \dots , T } \text { as follows} \\ \widehat { \mathcal { P } } _ { t } = \widehat { \mathcal { P } } _ { t , t + 1 } \left ( \widehat { \mathcal { P } } _ { t + 1 } \right ) , \quad t = 0 , \dots , T - 1 , \quad \text {and} \quad \widehat { \mathcal { P } } _ { T } = C . \\ ( 1 \cdot ) \, \text {is the one-period super-hedging pricing operator for the period} \, t , \text { which can also be seen} \\ \text {wave evolution of the navoff relatively to the convex evolution of the conditional support} \, \text { The}$$

What are our achievements? In this informational setting, generated by the random horizon τ , we describe explicitly the expansion of the set of super-hedging prices for various vulnerable claims. This expansion is quantified using processes under F and/or super-hedging prices of models under the flow F . Besides, this shows how τ affect the valuation process, which is an important step towards addressing Immediate-Profit arbitrage for the stopped model ( S τ , G ). Precisely, we connect oneto-one the G -Immediate-Profit arbitrage for the latter model to the Immediate-Profit arbitrage for ( T r ( S ) , F , ̂ Q ), where T r ( S ) is a transformed model of S and ̂ Q is a probability measure quantifying the correlation risks generated by τ and F . In this spirit, we show that the impact of τ on classical arbitrage differs tremendously from its impact on immediate-profit arbitrage. Our last achievement resides in determining the pricing formulas for vulnerable claims, in different manners. On the one hand, we show that for any vulnerable claim H G , there exists a unique pair ( f ( t, ω, x ) , H F ) such that the super-hedging price process of this claim coincide on [ [0 , τ [ [ with the solution of the following backward stochastic differential equations

$$X _ { t } = \widehat { \mathcal { P } } _ { t , t + 1 } \left ( f ( t + 1 , X _ { t + 1 } ) \right ) , \quad X _ { T } = H ^ { \mathbb { F } } ) .$$

Here f ( t, ω, x ) is an F -adapted functional intimately associated to the payment's policy of the claim and τ , and it is not linear in x in general. This extends the Carassus-Lepinette's pricing formula (1.1) to more complex situation, and shows that (1.1) remains valid for a subclass of vulnerable options only. On the other hand, we describe the dynamics of the super-hedging price process of the vulnerable claim and single out precisely the various induced informational risks. This latter decomposition formula is vital for risk management in the extended markets, in particular for the securitization of mortality and/or longevity risks in life insurance. All these aforementioned results rely essentially on understanding how conditional essential supremum behave under additional information and/or change of priors.

The paper has five sections including the current introductory section. The second section presents the economical and mathematical model and gives some preliminaries. The third section addresses the essential supremum under change of probability and/or change of filtration. The fourth section discusses the super-hedging prices' set and the immediate-profit arbitrage, while the fifth section derives the pricing formulas for various vulnerable claims.

## 2 The mathematical model and preliminaries

Throughout the paper, we suppose given a complete probability space (Ω , G , P ) and a fixed investment horizon T ∈ (0 , ∞ ). On this space, we suppose given the pair ( S, F ). F := ( F t ) t =0 , ··· ,T is a filtration, which represents the flow of information available to all agents through time. S = ( S t ) t =0 ,...,T is a d -dimensional and F -adapted process with values in R d + = [0 , ∞ ) d , and represents the (discounted) prices of d -risky assets. Besides S , we suppose that there exists a bond whose (discounted) price is B t = 1. Throughout the paper, the triplet ( S, F , P ) will be called the initial model . Throughout the paper, L 0 ( E, H ) denotes the set of random variables having values in E and are H -measurable, for any set E and any subσ -algebra H . When E = I R, we simply write L 0 ( H ), while L 0 + ( H ) denotes its subset of nonnegative random variables.

## 2.1 Random horizon and its parametrization

Besides the initial model, we consider an arbitrary random time τ , which might not be seen via F when it occurs. Thus, in order to take into account the occurrence of this random time, we consider the progressive enlargement of F with τ , and this yields to a larger flow G given by

$$\mathbb { G } \coloneqq ( \mathcal { G } _ { t } ) _ { t = 0 , \dots , T } , \ \mathcal { G } _ { t } \coloneqq \mathcal { F } _ { t } \vee \sigma \left ( \{ \tau = r \} \colon r = 0 , \cdots , t \right ) , \ t = 0 , \dots , T .$$

The agent endowed with the flow F can see the occurrence of τ through the pair ( G, ˜ G ) of two processes, called Az´ ema supermatingales, given by

The following lemma is a direct consequence of [22, Lemma 4.6], see also [8, (2.7)], and is useful throughout the paper.

$$G _ { t } \coloneqq P \left ( \tau > t \right | \, \mathcal { F } _ { t } \right ) , \quad \tilde { G } _ { t } \coloneqq P \left ( \tau \geq t \right | \, \mathcal { F } _ { t } \right ) , \quad t = 0 , 1 , \cdots , T . \\ \intertext { f o w i n g l e m a i s a i d e r s o n c u n s e c h e \, o f \, [ 2 2 , \, L e m m a \, 4 . 6 ] , \, s e e a l s o \, [ 8 , \, ( 2 . 7 ) ] , \, a n d i s u e f u l } \intertext { o f t h e p a r }$$

Lemma 2.1. For any t ∈ { 1 , ..., T } , for any positive integer n , we have

$$L ^ { 0 } ( \mathbf R ^ { n } , \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \geq t \} } = L ^ { 0 } ( \mathbf R ^ { n } , \mathcal { F } _ { t - 1 } ) I _ { \{ \tau \geq t \} } \ a n d \ L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \geq t \} } = L _ { + } ^ { 0 } ( \mathcal { F } _ { t - 1 } ) I _ { \{ \tau \geq t \} } .$$

The following lemma characterizes inequalities in G using inequalities in F .

Lemma 2.2. Let t ∈ { 1 , ..., T } and ( X t , K t ) ∈ L 0 ( F t ) × L 0 ( F t ) . Then we have:

$$( a ) \ X _ { t } 1 _ { \{ \tau = t \} } & \geq K _ { t } 1 _ { \{ \tau = t \} } \ P { \cdot } { a . s . } \ \text {if} \ X _ { t } 1 _ { \{ \widetilde { G } _ { t } > G _ { t } \} } \geq K _ { t } 1 _ { \{ \widetilde { G } _ { t } > G _ { t } \} } \ P { \cdot } { a . s . } . \\ ( 1 ) \ \ U T & = \prod _ { t } K _ { t } 1 _ { \{ \widetilde { G } _ { t } > G _ { t } \} }$$

$$( b ) \ X _ { t } 1 _ { \{ \tau > t \} } & \geq K _ { t } 1 _ { \{ \tau > t \} } \ P { - a . s . } \ i f \ X _ { t } 1 _ { \{ G _ { t } > 0 \} } ^ { \dagger } \stackrel { \dots } { \geq } K _ { t } 1 _ { \{ G _ { t } > 0 \} } \ P { - a . s . } \\ \intertext { s u r g t a l l } \begin{matrix} ( b ) & X _ { t } 1 _ { \{ \tau > t \} } \geq K _ { t } 1 _ { \{ \tau > t \} } & P { - a . s . } & i f \ X _ { t } 1 _ { \{ G _ { t } > 0 \} } ^ { \dagger } \stackrel { \dots } { \geq } K _ { t } 1 _ { \{ G _ { t } > 0 \} } & P { - a . s . } \\ & & & \end{matrix}$$

$$( c ) \ X _ { t } 1 _ { \{ \tau \geq t \} } ^ { \dagger } \geq K _ { t } 1 _ { \{ \tau \geq t \} } \ P { \cdot } { a . s . } \ i f \ X _ { t } 1 _ { \{ \widetilde { G } _ { t } > 0 \} } ^ { \dagger } \geq K _ { t } 1 _ { \{ \widetilde { G } _ { t } > 0 \} } ^ { \dagger } \ P { \cdot } { a . s . } \\$$

$$( d ) \ X _ { t } 1 _ { \{ \tau \geq t \} } \geq & K _ { t } 1 _ { \{ \tau = t \} } \ P { \cdot } { a . s . } \ i f \ X _ { t } 1 _ { \{ \tilde { G } _ { t } > 0 \} } \geq K _ { t } 1 _ { \{ \tilde { G } _ { t } > G _ { t } = 0 \} } + K _ { t } ^ { + } 1 _ { \{ \tilde { G } _ { t } > G _ { t } > 0 \} } \ P { \cdot } { a . s . } .$$

Proof. It is clear that assertion (a) follows immediately due to ( τ = t ) ⊂ ( ˜ G t &gt; G t ), while assertion (b) is due to ( τ &gt; t ) ⊂ ( G t &gt; 0). Also, { τ ≥ t } ⊆ { ˜ G t &gt; 0 } hence (c) holds. Thus the rest of the proof focuses on proving assertion (d). To this end, we notice that X t I { τ ≥ t } ≥ K t I { τ = t } P -a.s. iff X t I { τ = t } ≥ K t I { τ = t } P -a.s. and X t I { τ&gt;t } ≥ 0, P -a.s.. Thus, by combining these with assertions (a) and (b), we conclude that X t I { τ ≥ t } ≥ K t I { τ = t } iff X t I { G t &gt; 0 } ≥ 0 P -a.s. and X t I { ˜ G t &gt;G t } ≥ K t I { ˜ G t &gt;G t } P .a.s.. Furthermore, it is easy to check that X t I { ˜ G t &gt;G t } ≥ K t I { ˜ G t &gt;G t } and X t I { G t &gt; 0 } ≥ 0 P -a.s. if and only if X t 1 { ˜ G t &gt; 0 } ≥ K t 1 { ˜ G t &gt;G t =0 } + K + t 1 { ˜ G t &gt;G t &gt; 0 } P -a.s.. This completes the proof of assertion (d), and ends the proof of the lemma.

## 2.2 Super-hedging prices and Immediate-profit

In this subsection, we consider an arbitrary market model ( X, H ) defined on (Ω , G , P ), where H is a filtration and X is an H -adapted process.

The following proposition states the existence of the conditional supremum and infimum of any family of random variables, as shown in [23, Section 5.3.1].

Proposition 2.3. Let H ⊆ F be two sub σ -algebras. For any family of random variables Γ ⊆ L 0 ([ -∞ , ∞ ] , F ) , there exists a unique (up to a negligible set) H -measurable random variable γ H ∈ L 0 ([ -∞ , ∞ ] , H ) such that γ H ≥ γ , for all γ ∈ Γ , and if γ 1 H ∈ L 0 ([ -∞ , ∞ ] , H ) is such that γ 1 H ≥ γ for all γ ∈ Γ , then γ 1 H ≥ γ H a.s..We call γ H the conditional supremum of Γ knowing H and we denote it by ess sup H (Γ) . Similarly, we define ess inf H (Γ) = -ess sup H ( -Γ) .

When H = G in the proposition above, we write ess sup H (Γ) = ess sup(Γ). Let P a (P) be the set of all absolutely continuous probability measures w.r.t. P.

Lemma 2.4. Let H 1 ⊆ H 2 ⊆ G be subσ -algebras and let Γ ⊆ L 0 ([ -∞ , ∞ ] , G ) be a family of random variables. Then the following properties hold.

$$\ e s \sup _ { \mathcal { H } _ { 2 } } ( \Gamma ) \leq \ e s \sup _ { \mathcal { H } _ { 1 } } ( \Gamma ) , \quad \text {and} \quad \ e s \sup _ { \mathcal { H } _ { 1 } } ( \Gamma ) = \ e s \sup _ { \mathcal { H } _ { 1 } } \left ( \text {ess} \sup _ { \mathcal { H } _ { 2 } } ( \Gamma ) \right ) .$$

Lemma 2.5. If H ⊆ G is a subσ -algebra and Γ ⊆ L 0 (( -∞ , ∞ ] , G ) , then

$$\begin{pmatrix} \text {ess} \sup _ { \mathcal { H } } ( \Gamma ) ^ { + } = \text {ess} \sup _ { \mathcal { H } } ( \Gamma ^ { + } ) , & \text {and} & \text {ess} \sup _ { \mathcal { H } } ( \Gamma ) \end{pmatrix} ^ { - } = \text {ess} \inf _ { \mathcal { H } } ( \Gamma ^ { - } ) \\$$

Proof. Note that the first equality in (2.4) can easily be shown, and hence its proof will be omitted. From ess sup H (Γ) ≥ Y P -a.s. for any Y ∈ Γ, we get ( ess sup H (Γ) ) -≤ Y -P -a.s., and hence ( ess sup H (Γ) ) -≤ ess inf H (Γ -). Then remark that ess inf H (Γ -) ≤ Y -yields Y + -ess inf H (Γ -) ≥ Y + -Y -. Thus, ess sup H (Γ + ) -ess inf H (Γ -) ≥ Y , and ess sup H (Γ + ) -ess inf H (Γ -) ≥ ess sup H (Γ). By the first

equality in (2.4), ess sup

H

(

ess sup

H

)

H

(Γ

+

) =

(Γ)

so we deduce that

-

ess inf

(Γ

-

)

≥ - ess sup

(Γ)

H

and, finally ess inf H (Γ -) ≤ ( ess sup H (Γ) ) -. Then the second equality in (2.4) follows, and the proof is complete.

This lemma can be extended easily to increasing and decreasing functions.

Recall that a self-financing portfolio process in discrete-time is a stochastic process ( V t ) T t =0 satisfying ∆ V t := V t -V t -1 = θ t -1 ∆ X t for some θ t -1 ∈ L 0 ( R d , H t -1 ). We denote by A t,u := R t,u -L 0 + ( H u ), t ≤ u ≤ T , the set of all attainable claims at time u when starting a self-financing portfolio process from the zero initial endowment at time t . By definition, V t,u ∈ R t,u if and only if V t,u = ∑ u r = t +1 θ r -1 ∆ S r for some θ r ∈ L 0 ( R d , H r ), r = t, · · · , u -1.

For any payoff ξ ∈ L 0 ( H T ), we associate the pair ( P t ( ξ ) , Π ∗ t ( ξ )) of the set of all super-hedging prices and the infimum price, for t = 0 , 1 , ..., T -1 , given by

Thus, it is easy to check that P t (0) = ( -A t,T ) ∩ L 0 ( H t ).

$$& \quad \text {at the minimum price, for } t = 0 , 1 , \dots , 1 - 1 , \text { given by} \\ & \quad \mathcal { P } _ { t } ( \xi ) \coloneqq \{ p _ { t } \in L ^ { 0 } ( \mathbf R , \mathcal { H } _ { t } ) \colon \ p _ { t } + V _ { t , T } \geq \xi , \, P \text {-a.s. for } V _ { t , T } \in \mathcal { R } _ { t , T } \} \, , \\ & \quad \widehat { \mathcal { P } } _ { t } ( \xi ) \coloneqq \text {ess if} \, \mathcal { P } _ { t } ( \xi ) , \quad t = 0 , 1 , \dots , T - 1 . \\ \intertext { u s, it is easy to check that } & \mathcal { P } _ { t } ( \{ 0 \} = ( - \mathcal { A } _ { t } , T ) \cap L ^ { 0 } ( \mathcal { H } _ { t } ) .$$

Definition 2.6. For a model ( X, H := ( H t ) T t =0 ) , a probability Q , two dates t 1 , t 2 such that 0 ≤ t 1 &lt; t 2 , and a payoff ξ ∈ L 0 ( H t 2 ) , we denote by P ( X, H ,Q ) t 1 ,t 2 ( ξ ) the set of all super-hedging prices at time t 1 of the payoff ξ . The infimum price of ξ , denoted by P ( X, H ,Q ) t 1 ,t 2 ( ξ ) is given by

When Q = P , we omit the probability in the notation, and write P ( X, H ) t 1 ,t 2 ( ξ ) and ̂ P ( X, H ) t 1 ,t 2 ( ξ ) instead. Below, we recall, from [6], the mathematical definition of the AIP concept.

$$\hat { T } _ { t _ { 2 } } ) , \, \text { we denote by } \hat { T } _ { t _ { 1 } , t _ { 2 } } \, ^ { ( \xi ) } \, \text { the set of all super-healing prices at time } t _ { 1 } \ \text { of} \\ \intertext { m u m p r i c e \, o f \, \xi , \, \text { denoted by } \hat { \mathcal { P } } _ { t _ { 1 } , t _ { 2 } } ^ { ( X , \mathbb { H } , Q ) } ( \xi ) \, \text { is given by} \\ \hat { \mathcal { P } } _ { t _ { 1 } , t _ { 2 } } ^ { ( X , \mathbb { H } , Q ) } ( \xi ) \colon = \text {ess inf } \mathcal { P } _ { t _ { 1 } , t _ { 2 } } ^ { ( X , \mathbb { H } , Q ) } ( \xi ) . \\ \intertext { t i t h e p r o b a l i t y \, i n \, the notation, \, a n d \, w i rte \, \mathcal { P } _ { t _ { 1 } , t _ { 2 } } ^ { ( X , \mathbb { H } ) } ( \xi ) \, a n d \, \hat { \mathcal { P } } _ { t _ { 1 } , t _ { 2 } } ^ { ( X , \mathbb { H } ) } ( \xi ) \, \text { instead.}$$

Definition 2.7. We say that the condition AIP holds at time t ≤ T -1 if A t,T ∩ L 0 + ( F t ) = { 0 } . We say that AIP holds if it holds at any time t ≤ T -1 .

Similarly, as for the classical non-arbitrage condition in discrete-time, the AIP concept can be checked step-by-step and in various manners. This is the aim of the following proposition.

Proposition 2.8. Let H = ( H t ) t =0 ,..,T be a filtration, and X be an H -adapted process. Then the following assertions are equivalent.

- (a) The model ( X, H ) satisfies AIP.
- (b) For t ∈ { 1 , ..., T } and θ ∈ L 0 ( R d , H t -1 ) , ess sup ( θ ∆ X t ) ≥ 0 P -a.s..

t

H

-

- (c) For any t ∈ { 1 , ..., T } , P ( X, H ) t -1 ,t (0) ⊆ L 0 + ( H t -1 ) .

(d) For any t ∈ { 1 , ..., T } , ̂ P ( X, H ) t -1 ,t (0) = 0 , P -a.s.. Proof. By [6, Proposition 2.11], a model ( X, ( H t ) T t =0 ) satisfies AIP at time t -1 ≥ 0 if and only if, 0 belongs to the closed convex hull of ¯ D t -1 = supp H t -1 (∆ X t ), the random conditional support of ∆ X t . By the Hahn-Banach separation theorem for convex sets, this is equivalent to the property that σ ¯ D t -1 ( x ) ≥ 0, for all x ∈ R d , a.s.( ω ), where σ ¯ D t -1 ( x ) = sup z ∈ ¯ D t -1 ( -xz ). Note that, by [15, Theorem 3.4], σ ¯ D t -1 ( x ) = ess sup H t -1 ( -x ∆ X t ). Therefore, a model ( X, ( H t ) T t =0 ) satisfies AIP at time t -1 ≥ 0 iff

1

+

(

)

-

ess sup H t -1 ( -x ∆ X t ) ≥ 0 for x ∈ R d , a.s.. Hence, we get ess sup H t -1 ( θ t -1 ∆ X t ) ≥ 0 for θ t -1 ∈ L 0 ( R d , H t -1 ). Indeed, the inequality ess sup H t -1 ( θ t -1 ∆ X t ) ≥ 0 is immediate by the previous reasoning if AIP holds. Reciprocally, if ess sup H t -1 ( θ t -1 ∆ X t ) ≥ 0 for θ t -1 ∈ L 0 ( R d , H t -1 ), then ess sup H t -1 ( -x ∆ X t ) ≥ 0 for all x ∈ R d , a.s.. To see it, we argue by contradiction and, by a measurable selection, it is possible to get ess sup H t -1 ( θ t -1 ∆ X t ) &lt; 0 on a non null set, for some θ t -1 ∈ L 0 ( R d , H t -1 ), i.e. a contradiction. This ends the proof of the proposition.

## 3 Essential supremum under change of priors or information

Herein, we derive novel properties on conditional essential supremum, which are very useful throughout the paper. In fact, we investigate how the conditional essential supremum behaves under a change of probability and change of information (filtration). To this end, we start with the easy but crucial lemma, which conveys that conditional essential supremum or infinimum of indicators is always an indicator.

Lemma 3.1. Let H 1 ⊆ H be two subσ -algebras, and H ∈ H . Then the following assertions hold. (a) There exists H 1 ∈ H 1 satisfying

$$\text {ess} \inf _ { \mathcal { H } _ { 1 } } ( I _ { H } ) = I _ { H _ { 1 } } = I _ { \{ \text {ess} \inf _ { \mathcal { H } _ { 1 } } ( 1 _ { H } ) > 0 \} } = I _ { \{ \text {ess} \inf _ { \mathcal { H } _ { 1 } } ( 1 _ { H } ) = 1 \} } , \quad P { \cdot } { a . s . } ,$$

and it is the largest H 1 -measurable set contained in H .

(b) There exists H 2 ∈ H 1 such that

$$\overset { \text {ess sup} ( I _ { H } ) = I _ { H _ { 2 } } = I _ { \{ \text {ess sup} ( 1 _ { H } ) = 1 \} } = I _ { \{ \text {ess sup} ( 1 _ { H } ) > 0 \} } , \quad P { \text {-a.s.} } , \\ \mathcal { H } _ { 1 }$$

and it is the smallest H 1 -measurable set containing H .

Proof. Remark that assertion (a) follows immediately from assertion (b) applied to H := Ω \ H ∈ H , and the easy fact that ess sup H 1 ( I H ) = 1 -ess inf H 1 ( I H ). Thus, the rest of this proof focuses on assertion (b). To this end, we put which belongs to H 1 . In virtue of the definition of conditional essential supremum, we always have ess sup H 1 ( I H ) ≥ I H P -a.s., and hence I H ≤ I H 2 , P -a.s.. Therefore, we get

$$H _ { 2 } \coloneqq \begin{cases} \text {ess sup} ( I _ { H } ) = 1 \end{cases} , \\ \text {of the definition of conditional ess}$$

$$\underset { \mathcal { H } _ { 1 } } { \text {esssup} } ( I _ { H } ) \leq I _ { H _ { 2 } } , \quad P \text {-a.s.}$$

As a result of this, we obtain

$$\{ \text {ess sup} ( I _ { H } ) > 0 \} = \{ \text {ess sup} ( I _ { H } ) = 1 \} = H _ { 2 } , \quad P _ { \text {a.s.} } ,$$

and (3.2) follows immediately.

This ends the proof of the lemma.

Theorem 3.2. Let H i , i = 1 , 2 be two subσ -algebras of G such that H 1 ⊆ H 2 , and Z ∈ L + ( H 2 ) with E [ Z ] = 1 , and put

$$Z ^ { \mathcal { H } _ { 1 } } \colon = E [ Z \, | \mathcal { H } _ { 1 } ] \quad a n d \quad Q \colon = Z \cdot P . \\$$

Consider Γ ⊂ L 0 ( H 2 ) , and denote by

$$\widetilde { \gamma } ^ { Q } \coloneqq & \text {ess sup} ( \Gamma ) \quad a n d \quad \widetilde { \gamma } \coloneqq \text {ess sup} ( \Gamma I _ { \{ Z > 0 \} } ) . \\ \intertext { w i n g a s e r t i o n s h o l d . } & ( a n d ) \quad \widetilde { \gamma } \coloneqq \int [ \Gamma | _ { Z } | _ { A } ] \quad \text {,} \\$$

$$\ e s s & \sup _ { \mathcal { H } _ { 1 } } ( I _ { \{ X > 0 \} } ) = I _ { \{ Y > 0 \} } , \text { and } \ e s s \inf _ { \mathcal { H } _ { 1 } } ( I _ { \{ X > 0 \} } ) = I _ { \{ P ( X > 0 | \mathcal { H } _ { 1 } ) = 1 \} } , \\ \ e s s & \sup _ { \mathcal { H } _ { 1 } } ( I _ { \{ X = 0 \} } ) = I _ { \{ P ( X = 0 | \mathcal { H } _ { 1 } ) > 0 \} } , \text { and } \ e s s \inf _ { \mathcal { H } _ { 1 } } ( I _ { \{ X = 0 \} } ) = I _ { \{ Y = 0 \} } .$$

Then the following assertions hold. (a) If X ∈ L 0 + ( H 2 ) and Y := E [ X ∣ ∣ ∣ H 1 ] , then P -a.s. we have

- (b) We always have
- ˜ ˜ ˜ (c) We have ˜ γ Q = ˜ γ , Q -a.s. on ( ˜ γ Q ≥ 0) ( i.e. ˜ γ Q I { ˜ γ Q ≥ 0 } = ˜ γI { ˜ γ Q ≥ 0 } Q -a.s.). (d) If we denote Γ + := { γ + : γ ∈ Γ } , then we get

$$\widetilde { \gamma } \geq \widetilde { \gamma } ^ { Q } \quad Q _ { \text {-a.s.} } \quad a n d \quad I _ { \{ P ( Z = 0 | \mathcal { H } _ { 1 } ) > 0 \} } \widetilde { \gamma } \geq 0 \quad P _ { \text {-a.s.} } .$$

$$( \widetilde { \gamma } ^ { Q } ) ^ { + } = \text {ess} \sup ( \Gamma ^ { + } ) = ( \widetilde { \gamma } ) ^ { + } , \quad Q _ { - } a s . \\ \mathcal { H } _ { 1 } ) = 0 ) , \ w e \ h a v e$$

- (e) On ( P ( Z = 0 | H 1 ) = 0) , we have

$$\widetilde { \gamma } ^ { Q } = \widetilde { \gamma } = \text {esssup} ( \Gamma ) \quad P \text {-a.s.} \\ 0 \ P { - a . s . } . \ F u r t h e r m o r e , \ w e \ h a v e$$

(f) If ˜ γ Q ≥ 0 Q -a.s., then γ ≥ 0 P -a.s.. Furthermore, we have

In particular, ˜ γ Q &gt; 0 Q -a.s. if and only if ˜ γ &gt; 0 Q -a.s.. (g) For P -almost surely, we have and

˜ ˜ It worths mentioning that similar results for essential infimum can be obtained easily. In fact, thanks to the fact that ess inf H 1 ( -Γ) = -ess sup H 1 (Γ) for any Γ ⊂ L 0 ( H 2 ), the following corollary can be easily proved.

$$I f \widetilde { \gamma } ^ { Q } \geq 0 \ Q { - a . s . } \text {, then } \widetilde { \gamma } \geq 0 \ P { - a . s . } \ \text { for the term more, } \text { we have} \\ ( \widetilde { \gamma } ^ { Q } > 0 ) = ( \widetilde { \gamma } > 0 ) \quad Q { - a . s . } \quad \text {and} \quad \text {ess such} \ \P ( \Gamma _ { \{ Z = 0 \} } ) \geq 0 \quad Q { - a . s . } \\ \ p a r t i c u l a r { , \, \widetilde { \gamma } ^ { Q } > 0 } \ Q { - a . s . } \ \text { if and only if } \widetilde { \gamma } > 0 \ Q { - a . s . } \\ \text { for } P { \text { almost sure } } \ w h { \, } \text { we have}$$

$$( \widetilde { \gamma } ^ { Q } < \widetilde { \gamma } ) = ( \widetilde { \gamma } ^ { Q } < 0 ) \cap ( P ( Z = 0 | \, \mathcal { H } _ { 1 } ) > 0 ) = ( \widetilde { \gamma } ^ { Q } < 0 \leq \widetilde { \gamma } ) \, , \\ ( \widetilde { \gamma } < 0 ) = \left ( P ( Z = 0 | \, \mathcal { H } _ { 1 } ) = 0 \right ) \cap ( \widetilde { \gamma } ^ { Q } < 0 ) \, .$$

$$( \tilde { \gamma } < 0 ) & = \left ( P ( Z = 0 | \mathcal { H } _ { 1 } ) = 0 \right ) \cap ( \widetilde { \gamma } ^ { 2 } < 0 ) \, . \\ \intertext { b o n g t a l $ i s $ sim l a r $ u s t l e s $ f o r $ e s s u p ( T ) $ i f $ c a n $ b e o t a i n e $ a l f i m u m \, c a n $ b e a t i o n $ e a l l $ i s $ i f $ a n t , $ h a n t $ s u p } \intertext { e s s u p ( - \Gamma ) = - e s s u p ( T ) $ i f $ c a n $ v $ l ( \mathcal { H } _ { 2 } ) , $ t h e l f o w i n g $ c o r r o l l a $ a n $ c a n $ b e a o l l $ i s $ e s s u p }$$

Corollary 3.3. Consider the notations of Theorem 3.2. Then the following assertions hold. (a) Suppose Γ ⊂ L 0 ( H 2 , P ) . Then we have

$$\ p s { \inf _ { 1 } ( \Gamma _ { 1 } ( z _ { 1 } , \Psi ) \colon \text {Tr} _ { \ } m a t h } Q } & & \inf _ { \mathcal { H } _ { 1 } } ( \Gamma _ { 1 } ( \Psi ) , \quad P _ { - } a . s . \quad o n \quad \left ( Z ^ { \mathcal { H } _ { 1 } } > 0 \right ) , \\ & \inf _ { \mathcal { H } _ { 1 } } ( \Gamma _ { 1 } ( z _ { 1 } , \Psi ) ) \leq \inf _ { \mathcal { H } _ { 1 } } ( \Gamma ) , \quad P _ { - } a . s . \quad o n \quad \left ( P \left ( Z = 0 | \mathcal { H } _ { 1 } \right ) > 0 \right ) .$$

$$( b ) \ I f \subset L ^ { 0 } ( \mathcal { H } _ { 2 } , P ) \ \ a n d \ \underset { \mathcal { H } _ { 1 } } { \text {ass} } \inf ( \Gamma ) & \leq 0 \ Q { \bar { \ } a . s . , \ t h e n } \\ & \quad Q \\ \underset { \mathcal { H } _ { 1 } } { \text {ass} } \inf ( \Gamma ) & = \underset { \mathcal { H } _ { 1 } } { \text {ass} } \inf ( \Gamma I _ { \{ Z > 0 \} } ) \quad P { \bar { \ } a . s . } \ \ o n \quad ( Z ^ { \mathcal { H } _ { 1 } } > 0 ) .$$

$$( c ) \, \text { If } \, \underset { \mathcal { H } _ { 1 } } { \text {inf} } ( \Gamma ) \leq 0 \ Q { \text {-a.s.} , \ t h e n \, \underset { \mathcal { H } _ { 1 } } { \text {inf} } ( \Gamma I _ { \{ Z > 0 \} } ) } \leq 0 \ P { \text {-a.s.} } .$$

The last two assertions in Theorem 3.2 clearly describes how the sign of ˜ γ Q is related to the sign of ˜ γ . This fact is very important, as the sign of conditional essential supremum plays a crucial role in arbitrage. In fact, the equality (3.8) conveys that the two essential supremums, ˜ γ Q and ˜ γ , have different signs if and only if they do not coincide. However, the two essential supremums coincide when they have same sign, see (3.9) and assertion (c).

The converse of the first statement in assertion (f) is not true in general. In the following, we illustrate an example to support this claim.

Example 3.4. Consider Q such that P ( Z = 0 &lt; Z H 1 ) &gt; 0 , and for some /epsilon1 ∈ (0 , ∞ ) put Γ := { -/epsilon1I { P ( Z =0 |H 1 ) &gt; 0 } } . Then direct calculations yield

$$\ e s s \sup _ { \mathcal { H } _ { 1 } } ( \Gamma _ { \{ Z > 0 \} } ) & = - \epsilon I _ { \{ P ( Z = 0 \ | \mathcal { H } _ { 1 } ) > 0 \} } \ e s s \inf _ { \mathcal { H } _ { 1 } } ( I _ { \{ Z > 0 \} } ) \\ & = - \epsilon I _ { \{ P ( Z = 0 | \mathcal { H } _ { 1 } ) > 0 \} } I _ { \{ P ( Z > 0 \ | \mathcal { H } _ { 1 } ) = 1 \} } = 0 , \ P - a . s . , \\ \ e s s \sup _ { \mathcal { H } _ { 1 } } ( \Gamma ) & = - \epsilon I _ { \{ P ( Z = 0 \ | \mathcal { H } _ { 1 } ) > 0 \} } ,$$

and Q ( P ( Z = 0 |H 1 ) &gt; 0) = E [ Z H 1 1 { P ( Z =0 |H 1 ) &gt; 0 } ] &gt; 0 by assumption. Therefore, for this choice of Γ , these latter statements prove that we have ess sup H 1 (Γ I { Z&gt; 0 } ) ≥ 0 P -a.s. and Q ( Q ess sup H 1 (Γ) &lt; 0 ) &gt; 0 . Similar conclusion holds for Γ := { -/epsilon1I { P ( Z =0 |H 1 ) &gt; 0 } + /epsilon1I { P ( Z =0 |H 1 )=0 } } .

$$( \widetilde { \gamma } ^ { Q } ) ^ { + } = \underset { \mathcal { H } _ { 1 } } { \overset { Q } { \text {ss } } } \sup ( \Gamma ^ { \prime } ) = \underset { \mathcal { H } _ { 1 } } { \overset { \text {ss } } { \text {sup } } } ( \Gamma ^ { \prime } I _ { \{ Z > 0 \} } ) = \widetilde { \gamma } I _ { \{ \widetilde { \gamma } ^ { Q } \geq 0 \} } , \ \ Q ^ { - a . s . } \\ \text {s} \, \colon \, \text {proof of assertion } ( c ) , \text { and the rest of this proof focuses on assertions } ( a ) , \, ( b ) ,$$

Proof. of Theorem 3.2. On the one hand, by combining assertions (f) and (d), we deduce that ˜ γ Q = ˜ γ Q -a.s. as soon as ˜ γ Q ≥ 0 Q -a.s.. On the other hand, in order to prove assertion (c), we apply the latter claim to Γ ′ := Γ I { ˜ γ Q ≥ 0 } , which satisfies Q ess sup H 1 (Γ ′ ) = ( ˜ γ Q ) + ≥ 0 Q -a.s., and get

This ends the proof of assertion (c), and the rest of this proof focuses on assertions (a), (b), (d), (e), (f) and (g) in three parts.

$$( X _ { n } > 0 ) = ( X > 0 ) \quad \text {and} \quad ( Y _ { n } > 0 ) = ( Y > 0 ) , \quad n \geq 1 .$$

Part 1. Hereto we prove assertion (a). Let X ∈ L 0 + ( H 2 ), put X n := min( n, X ) and Y n := E [ X n ∣ ∣ H 1 ], and remark that both X n and Y n increase to X and Y respectively, and

Thus, it is enough to prove the assertion for bounded X ∈ L 0 + ( H 2 ), and without loss of generality we assume that ‖ X ‖ ∞ = 1. On the one hand, it is clear that we always have ( Y = 0) ⊂ ( X = 0), or equivalently I { X&gt; 0 } ≤ I { Y &gt; 0 } P -a.s., which yields

$$\overset { \text {esssup} } { \mathcal { H } _ { 1 } } ( I _ { \{ X > 0 \} } ) \leq I _ { \{ Y > 0 \} } , \quad P { \text {a.s.} } . .$$

On the other hand, we have X = XI { X&gt; 0 } ≤ I { X&gt; 0 } ≤ ess sup H 1 ( I { X&gt; 0 } ) P -a.s., and hence by taking conditional expectation, we get Y ≤ ess sup H 1 ( I { X&gt; 0 } ), P -a.s.. This clearly implies that { Y &gt; 0 } ⊂ { ess sup H 1 ( I { X&gt; 0 } ) &gt; 0 } , and by combining this with Lemma 3.1-(b), we deduce that

$$I _ { \{ Y > 0 \} } \leq I _ { \{ e s s \sup ( I _ { \{ X > 0 \} } ) > 0 \} } = e s s \sup ( I _ { \{ X > 0 \} } ) .$$

Therefore, by combining this latter inequality with (3.12), the first equality in (3.4) follows immediately, while the fourth equality is a direct consequence of the first equality and

$$\underset { \mathcal { H } _ { 1 } } { \text {ess} } \inf ( I _ { \{ X = 0 \} } ) = 1 - \underset { \mathcal { H } _ { 1 } } { \text {ess} } \sup ( I _ { \{ X > 0 \} } ) .$$

Similarly, remark that the second equality in (3.4) is a direct consequence of the third equality. To prove the third equality, we remark that

$$I _ { \{ X = 0 \} } \leq I _ { \{ \text {ess} \sup _ { \mathcal { H } _ { 1 } } ( I _ { \{ X = 0 \} } ) > 0 \} } = \text {ess} \sup _ { \mathcal { H } _ { 1 } } ( I _ { \{ X = 0 \} } ) \quad P { - a . s . } .$$

Then by taking conditional expectations on both sides, we get

$$P ( X = 0 | \mathcal { H } _ { 1 } ) \leq I _ { \{ e s s \sup ( I _ { \{ X = 0 \} } ) > 0 \} } .$$

On the one hand, this clearly yields

$$d , \, \text {this clearly yields} \\ ( P ( X = 0 | \mathcal { H } _ { 1 } ) > 0 ) \subseteq \left ( \text {esssup} ( I _ { \{ X = 0 \} } ) > 0 \right ) . \\$$

On the other hand, due to the easy fact that ( X = 0) ⊂ ( P ( X = 0 |H 1 ) &gt; 0), which is equivalent to I { X =0 } ≤ I { P ( X =0 |H 1 ) &gt; 0 } P -a.s., we get

$$\underset { \mathcal { H } _ { 1 } } { \text {esssup} } ( I _ { \{ X = 0 \} } ) \leq I _ { \{ P ( X = 0 | \mathcal { H } _ { 1 } ) > 0 \} } \quad P - a . s . .$$

Thus, by combining this last inequality with (3.13) and again Lemma 3.1, the third equality in (3.4) follows immediately. This proves assertion (a).

Part 2. This part proves assertions (b) and (d). To this end, we consider Γ ⊂ L 0 ( H 2 ), Q /lessmuch P with density Z := dQ/dP , and Z H 1 := E [ Z ∣ H 1 ]. Then it is clear that

This is equivalent to

˜ γ ≥ γ Q -a.s. , for any γ ∈ Γ , and I { Z=0 } ˜ γ ≥ 0 P-a.s. (3.14) Hence, on the one hand, the first inequality above yields the first inequality in (3.5) (i.e. ˜ γ ≥ ˜ γ Q Q -a.s.). On the other hand, the second inequality in (3.14) is equivalent to I { Z =0 } ≤ I { ˜ γ ≥ 0 } P -a.s.. Then by taking conditional essential supremum and using assertion (a), the second inequality in (3.5) follows. This ends the proof of assertion (b). To prove assertion (d), on the one hand, we notice that the first equality in (3.6) is due to Lemma 2.5. On the other hand, we suppose that ˜ γ ≥ 0 P -a.s. and thanks to assertion (b) (see (3.5)) we get

$$\text {proves assertions} \ ( b ) \text { and } ( d ) . \text { to this end, we consider } I \subset L ^ { 0 } \\ P , \text { and } Z ^ { \mathcal { H } _ { 1 } } \colon = E [ Z | \mathcal { H } _ { 1 } ] . \text { Then it is clear that } \\ \intertext { s e s \sup ( \Gamma I _ { \{ Z > 0 \} } ) \geq \gamma I _ { \{ Z > 0 \} } \quad P \text {-a.s.} , \quad \forall \, \gamma \in \Gamma . }$$

$$I _ { \{ Z ^ { \mathcal { H } _ { 1 } > 0 } \} } \widetilde { \gamma } \geq ( \widetilde { \gamma } ^ { Q } ) ^ { + } I _ { \{ Z ^ { \mathcal { H } _ { 1 } > 0 } \} } , \quad P { - a s . } \\ 9$$

Furthermore, thanks to assertion (a), the definition of essential supremum, and Lemma 2.5, we get for any γ ∈ Γ,

$$I _ { \{ Z ^ { \mathcal { H } _ { 1 } > 0 \} } } ( \tilde { \gamma } ^ { Q } ) ^ { + } & = \text {ess sup} \left ( I _ { \{ Z > 0 \} } ( \tilde { \gamma } ^ { Q } ) ^ { + } \right ) \geq \gamma ^ { + } I _ { \{ Z > 0 \} } \geq \gamma I _ { \{ Z > 0 \} } \quad P \text {-a.s.} \\ \intertext { l e n s c r . b y t a k i n g c o n d i t i o n a l $ e s s e n t i o n $ } & \text {since} \, & \text {by taking conditional essential supremum, we obtain}$$

Hence, by taking conditional essential supremum, we obtain

$$I _ { \{ Z ^ { \mathcal { H } _ { 1 } > 0 \} } } ( \widetilde { \gamma } ^ { Q } ) ^ { + } \geq \text {ess} \sup _ { \mathcal { H } _ { 1 } } ( \Gamma I _ { \{ Z > 0 \} } ) \geq I _ { \{ Z ^ { \mathcal { H } _ { 1 } > 0 \} } } \widetilde { \gamma } , \quad P ^ { - a . s . } \\ \intertext { y } \text {by combining this latter inequality with } ( 3 . 1 5 ) , \ ( 3 . 6 ) \text { holds under the assumption } ( 0 . 6 ) \colon$$

$$\underset { \mathcal { H } _ { 1 } } { \text {ess sup} } ( \Gamma ^ { \prime } I _ { \{ z > 0 \} } ) = ( \widetilde { \gamma } ) ^ { + } \geq 0 \quad \text {and} \quad \underset { \mathcal { H } _ { 1 } } { \text {ass sup} } ( \Gamma ^ { \prime } ) = \widetilde { \gamma } ^ { Q } I _ { \{ \widetilde { \gamma } \geq 0 \} } . \\ \intertext { i n t i c u l a r w e h a v e s s u p ( \Gamma ^ { \prime } I _ { \{ z > 0 \} } ) } \mathcal { H } _ { 1 } \quad \mathcal { H } _ { 1 } . \quad \text {and hence we can apply} \ ( 3 . 6 )$$

Therefore, by combining this latter inequality with (3.15), (3.6) holds under the assumption ˜ γ ≥ 0 P -a.s.. To prove (3.6) in general, we put Γ ′ := Γ I { ˜ γ ≥ 0 } and derive

Thus, in particular we have ess sup H 1 (Γ ′ I { Z&gt; 0 } ) ≥ 0 P -a.s., and hence we can apply (3.6) to Γ ′ and derive

The first equality follows from ( ˜ γ Q ≥ 0) ⊂ ( ˜ γ ≥ 0) Q -a.s., which is due to assertion (b). This proves assertion (d), and ends part 2.

$$( \widetilde { \gamma } ^ { Q } ) ^ { + } & = ( \widetilde { \gamma } ^ { Q } ) ^ { + } I _ { \{ \widetilde { \gamma } \geq 0 \} } = ( \overset { Q } { \underset { \mathcal { H } _ { 1 } } { \sup } } ( \Gamma ^ { \prime } ) ) ^ { + } = \overset { \text {ess} } { \underset { \mathcal { H } _ { 1 } } { \sup } } ( \Gamma ^ { \prime } ) = ( \widetilde { \gamma } ) ^ { + } , \quad Q ^ { - a . s . } \\ \intertext { f i n s t e q u a l l y f o l l w s f i m } \text {section } ( d ) \text { and } \text {ends part } 2$$

Part 3. Herein, we prove assertions (e), (f) and (g). For assertion (e), we use Σ := ( P ( Z = 0 |H 1 ) = 0) ⊂ ( Z &gt; 0) ⊂ ( Z H 1 &gt; 0), and obtain ˜ γI Σ = ess sup H 1 (Γ I Σ ∩{ Z =0 } ) = ess sup H 1 (Γ) and

I Σ Q ess sup H 1 (Γ) = I Σ ess sup H 1 (Γ). Thus, assertion (e) follows immediately from these equalities.

To prove assertion (f), we start by noticing that due to assertion (b), the inequality ˜ γ Q ≥ 0 Q -a.s. implies that ˜ γ ≥ 0 Q -a.s., and this is equivalent to ˜ γ ≥ 0 P -a.s. on ( Z H 1 &gt; 0). Furthermore, thanks to ( Z H 1 = 0) ⊂ ( Z = 0) P -a.s., it is clear that ˜ γ = 0 P -a.s. on ( Z H 1 = 0). Thus, we get ˜ γ ≥ 0 P -a.s., and the first claim in assertion (f) is proved.

It is obvious that the first property in (3.7) follows immediately from (3.6). To prove the second property in (3.7), we use ess sup (Γ I { Z =0 } ) ≥ γI { Z =0 } P -a.s. for any γ ∈ Γ, and hence we get

1

H

ess sup H 1 (Γ I { Z =0 } ) ≥ 0 Q -a.e.. This proves the third claim in assertion (f), and completes the proof of assertion (f).

To prove assertion (g), we derive ( ˜ γ &lt; 0) ⊂ ( P ( Z = 0 |H 1 ) = 0) ∩ ( ˜ γ Q &lt; 0) due to assertion (b), while ( P ( Z = 0 |H 1 ) = 0) ∩ ( ˜ γ Q &lt; 0) ⊂ ( ˜ γ &lt; 0) follows immediately from assertion (e). This proves (3.9). Remark that ( γ Q &lt; γ ) ⊂ ( γ Q &lt; 0), in virtue of assertion (c), and due to assertion (e) we have ( γ Q &lt; γ ) ⊂ ( P ( Z = 0 |H 1 ) &gt; 0). Thus, by combining these latter two remarks and the second property of assertion (b), we obtain

( ˜ γ Q &lt; ˜ γ ) ⊂ ( ˜ γ Q &lt; 0) ∩ ( P ( Z = 0 |H 1 ) &gt; 0) ⊂ ( ˜ γ Q &lt; 0) ∩ ( ˜ γ ≥ 0) ⊂ ( ˜ γ Q &lt; ˜ γ ) . This proves (3.8) and ends the proof of assertion (g). Hence, the proof of the theorem is complete.

˜ ˜ ˜ ˜ ˜

Corollary 3.5. For any t ∈ { 0 , 1 , ..., T } , the following equalities hold.

$$\text {ess sup} ( I _ { \{ \tau \geq t \} } ) = I _ { \{ \tilde { G } _ { t } > 0 \} } , \quad \text {and} \quad \text {ess inf} ( I _ { \tau \geq t } ) = I _ { \{ \tilde { G } _ { t } = 1 \} }$$

$$\overset { \text {ess sup} ( I _ { \tau > t } ) = I _ { \{ G _ { t } > 0 \} } , \quad \text {and} \quad \overset { \text {ess inf} ( I _ { \tau > t } ) = I _ { \{ G _ { t } = 1 \} }$$

$$\underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \sup _ { t } ( I _ { \widetilde { G } _ { t } > \} } ) = I _ { \{ G _ { t - 1 } > 0 \} } , \text { and } \underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \inf _ { t } ( I _ { \{ \widetilde { G } _ { t } = 1 \} } ) = I _ { \{ G _ { t - 1 } = 1 \} }$$

$$\underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \sup _ { \mathcal { F } _ { t - 1 } } ( I _ { \{ \tau \geq t \} } ) = I _ { \{ G _ { t - 1 } > 0 \} } , \quad \text {and} \quad \underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \inf _ { \{ \tau \geq t \} } ( I _ { \{ \tau \geq t \} } ) = I _ { \{ G _ { t - 1 } = 1 \} } .$$

- Proof. 1) By considering X = 1 { τ ≥ t } (respectively X = 1 { τ&gt;t } ), H 2 = G t , H 1 = F t and Y = E [ X |F t ] = ˜ G t (respectively Y = G t ), we conclude that both (3.16) and (3.17) follow from Theorem 3.2-(a). 2) By considering X = ˜ G t , H 2 = F t , H 1 = F t -1 and Y = E [ X |F t -1 ] = G t -1 , we deduce that (3.18) follows immediately from Theorem 3.2-(a). 3) Thanks to Lemma 2.4 and both (3.16) and (3.18), we derive

$$\int \lim i t s _ { 0 } ^ { 0 } \int \lim i t s _ { 0 } ^ { 0 } \int \lim i t s _ { 0 } ^ { 0 . 1 0 } \int \lim i t s _ { 0 } ^ { 0 . 1 0 } , & \text { } w \text { } c \text { } a \text { } \intertext { s e s \sup ( I _ { \{ \tau \geq t \} } ) = \text {ess sup} \left ( \text {ess sup} ( I _ { \{ \tau \geq t \} } ) \right ) = \text {ess sup} ( I _ { \{ \widetilde { G } _ { t } > 0 \} } ) = I _ { \{ G _ { t - 1 } > 0 \} } . } \intertext { \intertext { s e s \sup ( I _ { \{ \tau \geq t \} } ) = \text {ess sup} \left ( \text {ess sup} ( I _ { \{ \tau \geq t \} } ) \right ) = \text {ess sup} ( I _ { \{ \widetilde { G } _ { t } > 0 \} } ) = I _ { \{ G _ { t - 1 } > 0 \} } . } \intertext { \intertext { s e s \sup ( I _ { \{ \tau \geq t \} } ) = \text {ess sup} \left ( \text {ess sup} ( I _ { \{ \tau \geq t \} } ) \right ) = \text {ess sup} ( I _ { \{ \widetilde { G } _ { t } > 0 \} } ) = I _ { \{ G _ { t - 1 } > 0 \} } . } }$$

This proves the first equality in (3.19), while the second equality follows from similar reasoning. This ends the proof of the corollary.

The following lemma, which is borrowed from Choulli/Deng[7], plays a central role in the rest of the paper.

Lemma 3.6. The following assertions hold.

- (a) For any t ∈ { 1 , ..., T } , we have

$$\in \{ 1 , \dots , 1 \} , \ \text {we have} \\ \left ( P ( \widetilde { G } _ { t } > 0 | \mathcal { F } _ { t - 1 } ) > 0 \right ) = ( G _ { t - 1 } > 0 ) \, , \quad P _ { \text {a.s.} } \\ \intertext { s s } Z ^ { \mathbb { F } } = ( Z _ { t } ^ { \mathbb { F } } ) _ { t = 0 , \dots , T } , \ \text {defined by } Z _ { 0 } ^ { \mathbb { F } } = 1 \text { and }$$

(b) The process Z F = ( Z F t ) t =0 ,...,T , defined by Z F 0 = 1 and

$$p _ { \ } r o c h s s \, Z _ { 1 } ^ { 2 } & = ( Z _ { 1 } ^ { 2 } t ) _ { t = 0 , \dots , T } , \, d e f n e d \, b y \, Z _ { 0 } ^ { 2 } = 1 \, a n d \\ Z _ { t } ^ { \mathbb { F } } & \coloneqq \prod _ { s = 1 } ^ { t } \left ( \frac { I _ { \{ G _ { s } > 0 \} } } { P ( \widetilde { G } _ { s } > 0 | \mathcal { F } _ { s - 1 } ) } + I _ { \{ G _ { s - 1 } = 0 \} } \right ) , \quad t = 1 , \dots , T , \\ \intertext { s p r c h s s } \ m a t r i n g l a { e , a n d h e n c } \, \widetilde { Q } \coloneqq Z _ { T } ^ { \mathbb { F } } \cdot P \, i s \, a \, w e l l \, d e f n e d \, p r o b a b i l i t y \, m e s u r e .$$

$$\tilde { \gamma } _ { t } ^ { \tilde { Q } } \coloneqq & \underset { \mathcal { F } _ { t - 1 } } { \overset { \tilde { Q } } { \sup } } ( \Gamma _ { t } ) \quad a n d \quad \tilde { \gamma } _ { t } \coloneqq \underset { \mathcal { F } _ { t - 1 } } { \text {sup } } ( \Gamma _ { t } I _ { \{ \tilde { G } _ { t } > 0 \} } ) . \\ \intertext { l o w i n g a s s e r t i o n s h o l d . } \tilde { \widetilde { \mathcal { Q } } } \coloneqq & \underset { \mathcal { F } } { \overset { \tilde { Q } } { \Delta } } D \quad & \tilde { t } _ { t } \coloneqq \underset { \mathcal { F } } { \overset { \tilde { Q } } { \Delta } } D \quad & \tilde { t } _ { t } \coloneqq \underset { \mathcal { F } } { \overset { \tilde { Q } } { \Delta } } D \\$$

is an F -martingale, and hence ˜ Q := Z F T · P is a well defined probability measure. Corollary 3.7. Let Γ t be a family of F t -measurable random variables, ˜ Q be the probability measure defined in Lemma 3.6, and

Then the following assertions hold.

(a) If I { G t -1 &gt; 0 } ˜ γ ˜ Q t ≥ 0 P -a.s., then ˜ γ t ≥ 0 P -a.s., and

(b) If ˜ γ t ≥ 0 P -a.s., then we have

$$\gamma _ { t } \geq 0 \ P { - a . s . } , \text { where } w \P p h a { v } \\ \left ( \widetilde { \gamma } _ { t } ^ { \widetilde { Q } } < 0 \right ) \cap ( G _ { t - 1 } > 0 ) \subseteq \left ( P ( \widetilde { G } _ { t } = 0 < G _ { t - 1 } | \mathcal { F } _ { t - 1 } ) > 0 ) , \quad P { - a . s . } . \\$$

$$I _ { \{ G _ { t - 1 } > 0 \} } \widetilde { \gamma } _ { t } ^ { \widetilde { Q } } & = \widetilde { \gamma } _ { t } , \quad P \, - a . s . . \\ \ w e \ h a v e & & \\$$

Proof. To prove assertion (a), we apply Theorem 3.2-(c) to (Γ , Q, H 1 , H 2 ) = ( I { G t -1 &gt; 0 } Γ t , ˜ Q, F t -1 , F t ), ˜ Q = Z F t · P on F t , and we derive

$$\overset { \tilde { Q } } { \underset { \mathcal { F } _ { t - 1 } } { \text {sup} } } ( I _ { \{ G _ { t - 1 } > 0 \} } \Gamma _ { t } ) = \underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \sup ( I _ { \{ G _ { t - 1 } > 0 \} } \Gamma _ { t } I _ { \{ Z _ { t } ^ { f } > 0 \} } ) \quad P \text {-a.s. on } ( Z _ { t - 1 } ^ { \mathbb { F } } > 0 ) .$$

As ( Z F t &gt; 0) ∩ ( G t -1 &gt; 0) = ( ˜ G t &gt; 0), we deduce that (3.23) holds on the set ( Z F t -1 &gt; 0). Due to ( Z F t -1 &gt; 0) ∩ ( G t -1 &gt; 0) = ( G t -1 &gt; 0), we conclude that ( Z F t -1 = 0) ⊂ ( G t -1 = 0). Thus, the two sides of (3.23) vanish on ( Z F t -1 = 0), and the conclusion follows. Note that the first statement of (a) is immediate from (3.23), and the proof of assertion (a) is complete.

To prove assertion (b), we suppose that ˜ γ t ≥ 0 P -a.s., and we apply Theorem 3.2-(g) to the (Γ , Q, H 1 , H 2 ) = (Γ t , ˜ Q, F t -1 , F t ) afterwards. As a result, we get

Therefore, assertion (b) follows immediately from combining this latter inclusion with (3.20) and ( G t -1 &gt; 0) ∩ ( Z F t = 0) = ( ˜ G t = 0 &lt; G t -1 ), which can be easily proved. This ends the proof of the corollary.

$$\left ( \tilde { \gamma } _ { t } ^ { \tilde { Q } } < 0 \right ) \subseteq \left ( P ( Z _ { t } ^ { \mathbb { F } } = 0 | \mathcal { F } _ { t - 1 } ) > 0 \right ) . \\ \text {follows immediately from combining this latter if} \\ ( \widetilde { G } _ { t } = 0 < G _ { t - 1 } ) , \text { which can be easily proved. } \, T$$

In the remaining part of this subsection, we elaborate the relationship between G -conditional essential supremum and the F -conditional essential supremum.

Theorem 3.8.

If Γ ⊆ L 0 ( G T ) , then the following assertions hold.

- (a) For any t ≤ T , we have P -a.s.

$$\overset { \text {ess} \sup ( \Gamma I _ { \{ \tau \geq t \} } ) } { \mathcal { G } } \overset { \text {ess} \sup ( \Gamma I _ { \{ \tau \geq t \} } ) } { \mathcal { F } } \overset { \text {ess} \sup ( \Gamma I _ { \{ \tau \geq t \} } ) } { \mathcal { F } } , \text { and } I _ { \{ \mathcal { G } _ { t - 1 } < 1 \} } \overset { \text {ess} \sup ( \Gamma I _ { \{ \tau \geq t \} } ) } { \mathcal { F } } \overset { \text {ess} \sup ( \Gamma I _ { \{ \tau \geq t \} } ) } { \mathcal { F } } \geq 0 .$$

$$( b ) \ L e t \, t \leq T \ a n d \ \Sigma _ { t } \colon = \left ( \underset { \mathcal { G } _ { t - 1 } } { \text {ess} } \sup _ { \mathcal { G } _ { t - 1 } } ( \Pi _ { \{ \tau \leq t \} } ) \geq 0 \right ) . \ \text {Then} \ P { \cdot } { a . s . } \ w e \, g e$$

$$\underset { \mathcal { I } _ { \{ G _ { t - 1 } = 1 \} } } { \text {esssup} } ( \Gamma I _ { \{ \tau \geq t \} } ) & = I _ { \{ \tau \geq t \} } \overset { \text {ess sup} } { \mathcal { I } _ { t - 1 } } \\ I _ { \{ G _ { t - 1 } = 1 \} } \overset { \text {ess sup} } { \mathcal { I } _ { t - 1 } } & = I _ { \{ G _ { t - 1 } = 1 \} } \overset { \text {ess sup} } { \mathcal { I } _ { t - 1 } } \overset { \text {ess sup} } { \mathcal { I } _ { t \leq t } } ) , \ o n \ \Omega \ \overset { \text {on} } { \Omega } , \\ \underset { \mathcal { I } _ { t - 1 } } { \text {f-} } &$$

(c) If Γ ⊆ L 0 + ( G T ) P -a.s. and t ≤ T , then P -a.s. on ( τ ≥ t ) we have

$$\text {ess sup} ( \Pi _ { \{ \tau \geq t \} } ) & = \text {ess sup} ( \Pi _ { \{ \tau \geq t \} } ) , \\ \mathcal { I } _ { \{ \tau \geq t \} } & \text {ess inf} ( \Gamma ) = I _ { \{ \tau \geq t \} } \text {ess sup} \left ( \text {ess inf} ( \Pi _ { \{ \tau \geq t \} } ) \right ) . \\$$

- (d) For any t ≤ T , P -a.s. we have

$$I _ { \{ \tau \geq t \} } \overset { I _ { \{ \tau \geq t \} } \text {ess sup} ( \Gamma ) } { \underset { \mathcal { G } _ { t - 1 } } { \text {sup} } } & = I _ { \{ \tau \geq t \} } \overset { \text {ess sup} ( \Gamma ^ { + } I _ { \{ \tau \geq t \} } ) + I _ { \{ G _ { t - 1 } = 1 \} } \overset { \text {ess sup} } { \underset { \mathcal { F } _ { t - 1 } } { \text {sup} } } ( - \Gamma ^ { - } I _ { \{ \tau \geq t \} } ) } { \underset { \mathcal { F } _ { t - 1 } } { \text {sup} } } \\ & \quad - I _ { \{ \tau \geq t \} } I _ { \{ G _ { t - 1 } < 1 \} } \overset { \text {ess sup} } { \underset { \mathcal { F } _ { t - 1 } } { \text {sup} } } \left ( \overset { \text {ess inf} } { \mathcal { G } _ { t - 1 } } ( \Gamma ^ { - } I _ { \{ \tau \geq t \} } ) \right ) .$$

The theorem clearly singles out fully the relationship between essential supremum under G and that under F . In fact, in (3.24) the theorem states that F -essential supremum is nonnegative on the set ( G t -1 &lt; 1), while it is always an upper bound for G -essential supremum. Thus, we conclude that P -a.s. on ( G t -1 &lt; 1) ∩ (ess sup G t -1 ( Y I { τ ≥ t } ) &lt; 0) we have

$$\underset { \mathcal { G } _ { t - 1 } } { \text {esssup} } ( Y I _ { \{ \tau \geq t \} } ) < 0 \leq I _ { \{ \tau \geq t \} } \overset { \text {esssup} } { \mathcal { F } } _ { t - 1 } ( Y I _ { \{ \tau \geq t \} } ) .$$

Proof. of Theorem 3.8. The proof of the theorem is divided into four parts, where we prove the four assertions respectively.

Part 1. Hereto, we prove assertion (a). To this end, we first consider the case where Γ = { Y } is a singleton. We use the definition of essential supremum, and deduce that ess sup F t -1 ( Y I { τ ≥ t } ) ∈

$$L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) \subset L ^ { 0 } ( \mathcal { G } _ { t - 1 } ) \text { and }$$

$$\sup _ { \mathcal { F } _ { t - 1 } } \sup ( Y I _ { \{ \tau \geq t \} } ) \geq Y I _ { \{ \tau \geq t \} } , \quad P _ { \text {-a.s.} } .$$

Thus, the first inequality in (3.24) follows immediately from the above inequality and the definition of essential supremum again. To prove the second inequality in (3.24), we combine its first inequality with the fact that ess sup G t -1 ( Y I { τ ≥ t } ) = I { τ ≥ t } ess sup G t -1 ( Y ) and get I { τ&lt;t } ess sup F t -1 ( Y I { τ ≥ t } ) ≥ 0 , P -a.s..

This is equivalent to I { τ&lt;t } ≤ I { ess sup F t -1 ( Y I { τ ≥ t } ) ≥ 0 } , P -a.s .. Then by taking conditional expectation on both sides of the latter inequality, we derive

$$1 - G _ { t - 1 } \leq I _ { \{ \text {ess} \sup ( Y I _ { \{ \tau \geq t \} } ) \geq 0 \} } , \quad P \text {-a.s.}$$

Therefore, the second inequality in (3.24) is a direct consequence of this inequality. This proves (3.24). For the general case, it suffices to observe that ess sup F t -1 (Γ I { τ ≥ t } ) = ess sup Y ∈ Γ ess sup F t -1 ( Y I { τ ≥ t } ) to obtain the same inequalities. This ends the proof of assertion (a).

2) This part gives the proof of assertion (b). Thanks to Lemma 2.1 when n = 1, we deduce the existence of γ t -1 ∈ L 0 ( F t -1 ) such that

$$\overset { \text {ess} \sup } { \mathcal { G } } _ { t - 1 } ( \Gamma _ { \{ \tau \geq t \} } ) = 1 _ { \{ \tau \geq t \} } \overset { \text {ess} \sup } { \mathcal { G } } _ { t - 1 } ( \Gamma ) = 1 _ { \{ \tau \geq t \} } \gamma _ { t - 1 } , \quad P \text {-a.s.}$$

As a result, we get (ess sup G t -1 (Γ1 { τ ≥ t } ) ≥ 0) = ( τ ≥ t ) ∩ ( γ t -1 ≥ 0) ∪ ( τ &lt; t ), and hence the first equality in (3.25) is equivalent to prove the equality on ( τ ≥ t ) ∩ ( γ t -1 ≥ 0) P -a.s. instead. Indeed, the equality is trivial on the set ( τ &lt; t ). To this end, we combine (3.28), the tower property of Lemma 2.4, and Corollary 3.5 and conclude that P -a.s. on ( γ t -1 ≥ 0) we have

$$\text {so} \, \underset { \mathcal { F } _ { t - 1 } } { \text {sim} } \, ( \Gamma 1 _ { \{ \tau \geq t \} } ) = \text {ess} \, \sup _ { \mathcal { F } _ { t - 1 } } \left ( \underset { \substack { \ g _ { t - 1 } \\ \ g _ { t - 1 } } } { \text {ess} } \sup _ { \{ \tau \geq t \} } \right ) = \gamma _ { t - 1 } 1 _ { \{ G _ { t - 1 } > 0 \} } .$$

Then by multiplying both sides with I { τ ≥ t } , and using ( τ ≥ t ) ⊂ ( G t -1 &gt; 0) afterwards, we obtain the first equality in (3.25). To prove the second equality in (3.25), we notice that in virtue of (3.28), we have

$$( \text {ess} \sup _ { \mathcal { G } _ { t - 1 } } ( \Gamma 1 _ { \{ \tau \geq t \} } ) < 0 ) = ( \tau \geq t ) \cap ( \gamma _ { t - 1 } < 0 ) , \text { and } ( G _ { t - 1 } = 1 ) \subset ( \tau \geq t ) .$$

Hence, the second equality in (3.25) reduces

$$\underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \sup _ { t \} \geq t \} = \underset { \mathcal { G } _ { t - 1 } } { \text {ess} } \sup _ { t } ( \Gamma I _ { \{ \tau \geq t \} } ) , \, P \text {-a.s. \ on } ( \gamma _ { t - 1 } < 0 ) \cap ( G _ { t - 1 } = 1 ) .$$

Taking the essential supremum knowing F t -1 on both sides of (3.28), we deduce by Corollary 3.5 that P -a.s. on ( γ t -1 &lt; 0) we have

$$\inf _ { \mathcal { F } _ { t - 1 } } \sup _ { \mathcal { F } _ { t - 1 } } ( \Gamma _ { \{ \tau \} t } ) = \gamma _ { t - 1 } \underset { \mathcal { F } _ { t - 1 } } { \text {issinf} } \left ( I _ { \{ \tau \geq t \} } \right ) = \gamma _ { t - 1 } 1 _ { \{ G _ { t - 1 } = 1 \} } . \\$$

As ess sup G t -1 (Γ I { τ ≥ t } ) = γ t -1 I { τ ≥ t } and I { τ ≥ t } I { G t -1 =1 } = I { G t -1 =1 } , the second equality in (3.25) is a direct consequence of (3.29). This ends the proof of assertion (b).

- 3) Herein, we prove assertion (c). To this end we suppose that Γ ⊆ L 0 + ( G T ), and remark that ess sup G t -1 (Γ I { τ ≥ t } ) ≥ ess inf G t -1 (Γ I { τ ≥ t } ) ≥ 0 P -a.s.. Then, in virtue of assertion (b), the first equality

in (3.26) follows immediately from the first equality in (3.25), while the second equality of (3.26) is also a direct consequence of the first equality of (3.25) applied to ess inf G t -1 (Γ I { τ ≥ t } ) instead of Γ. This completes the proof of assertion (c).

- 4) This part deals with assertion (d). Thanks to Lemma 2.5, we derive

$$I _ { \{ \tau \geq t \} } \exp ( \Gamma ) & = I _ { \{ \tau \geq t \} } ( \text {ess sup} ( \Gamma ) ) ^ { + } - I _ { \{ \tau \geq t \} } ( \text {ess sup} ( \Gamma ) ) ^ { - } \\ & = I _ { \{ \tau \geq t \} } \exp ( \Gamma ^ { + } ) + I _ { \{ \tau \geq t \} } \exp ( - \Gamma ^ { - } ) \\ & = \text {ess sup} ( \Gamma ^ { + } I _ { \{ \tau \geq t \} } ) + I _ { \{ G _ { t - 1 } = 1 \} } \exp ( - \Gamma ^ { - } I _ { \{ \tau \geq t \} } ) \\ & - I _ { \{ G _ { t - 1 } < 1 \} } \exp ( \Gamma ^ { - } I _ { \{ \tau \geq t \} } ) . \\$$

Thus, by combining this latter equality with assertions (b) and (c), assertion (d) follows immediately. This ends the proof of theorem.

The second equality in (3.26) and its proof convey the following interesting identity.

Corollary 3.9. For any t ∈ { 1 , .., T } and any Y ∈ L 0 + ( G t -1 ) , we have

$$Y I _ { \{ \tau \geq t \} } = I _ { \{ \tau \geq t \} } \underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \sup _ { t } ( Y I _ { \{ \tau \geq t \} } ) , \quad P - a . s . .$$

$$\text {rently} & & E \left [ Y I _ { \{ \tau \geq t \} } \left | \mathcal { F } _ { t - 1 } \right ] = G _ { t - 1 } \text {ess} \sup _ { \mathcal { F } _ { t - 1 } } ( Y I _ { \{ \tau \geq t \} } ) , \quad P { \text {a.s.} } \\ \text {ary gives more insight about the second equality in Lemma 2.1, using essentialsupremum} \\$$

Or equivalently

This corollary gives more insight about the second equality in Lemma 2.1, using essential supremum instead of conditional expectation, for any nonnegative random variable.

## 4 Super-hedging prices' set and Immediate-Profit arbitrage

In this section we will address the pricing method adopted in Carassus-Lepinette [6] for vulnerable claims. We call vulnerable claims, any claim H that involves the occurrence random time τ somehow, and hence it is a G T -measurable random variable and is characterized by a pair of F -adapted processes ( C,R ). Here C is the payoff process of the claim and R is the recovery process, while the binding relationship between ( C,R ) and H is dictated by the recovery policy . The rest of this section is divided into two subsections. The first subsection discusses the pricing set of one-step super-hedging prices for vulnerable claims, while the second subsection elaborates the IP arbitrage results.

## 4.1 Super-hedging prices' sets for vulnerable claims

Throughout the rest of the paper, we consider ( S, F ) and ( S, F ) given by

Furthermore, it is easy to see that S t := S 0 + ∑ t s =1 I { ˜ G s &gt; 0 } ∆ ˜ S s . The following theorem constitutes our main result of this subsection, and it explains how the set of super-hedging prices for various vulnerable claims expands under the effect of the randomness borne in τ .

$$\sup _ { \ } t r e g h e d g i n g \, \ p r i c { s } \, \overset { \ } s e t s \, \text { for  v u n i m b l e c h a l l } \quad & \text {
 without the rest of the paper, we consider ( \bar { \mathbb { S } } , \mathbb { F } ) \, and \, ( \tilde { \mathcal { S } } , \mathbb { F } ) \text { given by } \\ & \quad \bar { \mathcal { S } } \equiv S _ { 0 } + \sum _ { s = 1 } ^ { \dots } I _ { \{ \tilde { \mathcal { G } } _ { s } > 0 \} } \Delta S _ { s } , \quad \tilde { \mathcal { S } } \equiv S _ { 0 } + \sum _ { s = 1 } ^ { \dots } I _ { \{ \mathcal { G } _ { s - 1 } > 0 \} } \Delta S _ { s } , \\ & \quad \Delta S _ { s } \equiv S _ { s } - S _ { s - 1 } , \quad s = 1 , \dots , T , \quad \sum _ { \emptyset } = 0 . \\ \text {more, it is easy to see that } \overline { \bar { \mathcal { S } } } _ { t } \equiv S _ { 0 } + \sum _ { s = 1 } ^ { t } I _ { \{ \tilde { \mathcal { G } } _ { s } > 0 \} } \Delta \tilde { \mathcal { S } } _ { s } . \text { The following theorem constitutes}$$

Theorem 4.1. Let t ∈ { 1 , .., T } , ξ ∈ L 0 ( G t ) , ( g s ) s =0 ,...,T and ( K s ) s =0 ,....,T be two F -adapted processes, and consider the triplet ( g, κ (0) , κ ( g ) ) given by

̂ Then the following assertions hold.

$$\text {emb} \, 4 . 1 \colon \, & \, L e t \uparrow \in \{ 1 , \dots , \{ T \} , \, ( \widehat { g } _ { t } ) , ( \widehat { g } _ { s } ) _ { s = 0 , \dots , T } \, \ a n d \, ( K _ { s } ) _ { s = 0 , \dots , T } \, \ b e \, 1 0 8 \, \mathbb { I } \, \text {a} \, \ a d \, ( K _ { s } ) _ { s = 0 , \dots , T } \, \ b e \, 1 0 8 \, \mathbb { I } \, \text {a} \, \ a d \, \text {a} \, \text {process} \, \text {es} , \\ \text {and} \, & \, \text {consider the triplet} \, ( \widehat { g } , \kappa ) , \, \kappa ( g ) \, , \, \text {given} \, \ b y \\ & \quad \kappa ^ { ( g ) } \colon = \kappa ( g , K ) \colon = g _ { \{ \widetilde { G } \in G > 0 \} } + K \, I _ { \{ \widetilde { G } \geq G > 0 \} } + \max ( g , K ) \, I _ { \{ \widetilde { G } \geq G > 0 \} } , \\ & \quad \widehat { g } \colon = \kappa ( g , 0 ) , \quad \kappa ^ { ( 0 ) } \colon = \kappa ( 0 , K ) , \quad \overline { g } \colon = g _ { \{ \widetilde { G } \geq 0 \} } = \kappa ( g , g ) .$$

(a) If ξ = g t 1 { τ&gt;t } , then we have

(b) If ξ = g t I { τ ≥ t } , then we have

$$I \} \, \xi = & \, g _ { t } I _ { \{ \tau > t \} } , \, \text { then we have} \\ & \quad \mathcal { P } _ { t - 1 , t } ^ { ( \mathsf S , \mathbb { G } ) } ( \xi ) \\ & = L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } + \bigcup _ { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } \mathcal { P } _ { t - 1 , t } ^ { ( \mathsf S , \mathbb { F } ) } ( \widehat { g } _ { t + \delta } + \delta I _ { \{ \widetilde { G } _ { t } = 0 < G _ { t - 1 } \} } ) I _ { \{ \tau \geq t \} } \\ & = L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } + \mathcal { P } _ { t - 1 , t } ^ { ( \widetilde { S } , \mathbb { R } ) } ( \widehat { g } _ { t } ) I _ { \{ \tau \geq t \} } . \\ I f \, \xi = & \, g _ { t } I _ { \{ \tau \geq t \} } , \, \text { then we have} \\ & \quad \mathcal { P } _ { t } ^ { ( \mathsf S , \mathbb { G } ) } ( \xi ) - L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} }$$

$$\mathcal { P } _ { t - 1 , t } ^ { ( \mathsf S ) , \mathsf G ) } ( \xi ) - J _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } \\ = \bigcup _ { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } \mathcal { P } _ { t - 1 , t } ^ { ( \mathsf S ) , \mathsf F } ( \bar { g } _ { t } + \delta I _ { \{ \tilde { G } _ { t } = 0 < G _ { t - 1 } \} } ) I _ { \{ \tau \geq t \} } = \mathcal { P } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { F } , \tilde { Q } ) } ( \bar { g } _ { t } ) I _ { \{ \tau \geq t \} } . \\ \\ I f \mathcal { C } = K \ U \quad \text { } \mathcal { H } _ { t } \text { } \mathcal { C } _ { t }$$

(c) If ξ = K τ I { τ ≤ t } , then

$$\mathcal { P } _ { t - 1 , t } ^ { ( S T , \mathbb { G } ) } ( \xi _ { t } ) - K _ { t } I _ { \tau \lceil \tau \leq t - 1 \rceil } - L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } \\ = \bigcup _ { \delta _ { t } \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } \mathcal { P } _ { t - 1 , t } ^ { ( S , \mathbb { F } ) } ( \kappa _ { t } ^ { ( 0 ) } + \delta _ { t } I _ { \{ \tilde { G } _ { t } = 0 < G _ { t - 1 } \} } ) I _ { \{ \tau \geq t \} } \\ = \mathcal { P } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { Q } ) } ( \kappa _ { t } ^ { ( 0 ) } ) I _ { \{ \tau \geq t \} } .$$

(d) If ξ = g t I { τ&gt;t } + K τ I { τ ≤ t } , then

$$\mathcal { P } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ) - K _ { t } I _ { \{ \tau \leq t - 1 \} } - L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } \\ = \bigcup _ { \delta _ { t } \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } , \mathcal { K } _ { t } ^ { ( g ) } + \delta _ { t } I _ { \{ \tilde { G } _ { t } = 0 < G _ { t - 1 } \} } ) I _ { \{ \tau \geq t \} } } \\ = \mathcal { P } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { Q } ) } ( \kappa _ { t } ^ { ( g ) } ) I _ { \{ \tau \geq t \} } .$$

No matter what is the vulnerable claim, Theorem 4.1 shows that this expansion after τ , i.e. on the set ( τ &lt; t ), consists of adding arbitrary nonnegative price, and hence this will have no effect when taking the infinimum. However, for the part before or at τ , i.e. on the set ( τ ≥ t ), and again no matter what the vulnerable claim considered, the expansion mechanism of the super-hedging prices' set is obtained, by expanding the set of claims intrinsic to the interplay between F and τ described by the pair ( ˜ G,G ). Besides this expansion of claims due to the correlation risks generated by τ , we describe precisely the F -risks which the vulnerable claim entails, and how the set of prices are related. This precise relationship, of quantifying F -risks for the vulnerable claims, is established using the two F -models ( S, F , P ) and ( S, F , Q ).

˜ ˜ Proof. of Theorem 4.1. The proof of the theorem is divided into three parts, where we prove assertions (a)-(b), (c) and (d) respectively.

Part 1: Herein, we prove assertions (a) and (b). On the one hand, x G t -1 belongs to P ( S τ , G ) t -1 ,t ( ξ ) if and only if there exists θ G t -1 ∈ L 0 (I R d , G t -1 ) such that

$$x _ { t - 1 } ^ { \mathbb { G } } + \theta _ { t - 1 } ^ { \mathbb { G } } \Delta S _ { t } ^ { \tau } \geq g _ { t } I _ { \{ \tau > t \} } , \quad P - a . s . ,$$

$$I _ { \{ \tau \leq t - 1 \} } x _ { t - 1 } ^ { G } \geq 0 , \quad \text {and} \quad 1 _ { \{ \tau \geq t \} } \left ( x _ { t - 1 } ^ { G } + \theta _ { t - 1 } ^ { G } \Delta S _ { t } \right ) \geq g _ { t } 1 _ { \{ \tau > t \} } \ P { \text {a.s.} } . \\ \intertext { \text {the other hand. in virtue of } I { \text {lemma } } 2 . 1 . \text { there exists } ( r _ { t } ^ { \mathbb { F } } , \theta _ { t } ^ { \mathbb { F } } ) \text { which belongs } 0 \, I ^ { 0 } ( \mathcal { F } _ { t } \ 1 ) \times \\$$

$$( x _ { t - 1 } ^ { \mathbb { F } } , \theta _ { t - 1 } ^ { \mathbb { F } } ) I _ { \{ \tau \geq t \} } = ( x _ { t - 1 } ^ { \mathbb { G } } , \theta _ { t - 1 } ^ { \mathbb { G } } ) I _ { \{ \tau \geq t \} } .$$

or equivalently

On the other hand, in virtue of Lemma 2.1, there exists ( x F t -1 , θ F t -1 ) which belongs to L 0 ( F t -1 ) × L 0 (I R d , F t -1 ) satisfying

Therefore, by inserting the above equality in (4.8), we deduce that x G t -1 belongs to P ( S τ , G ) t -1 ,t ( ξ ) iff there exists ( x F t -1 , θ F t -1 ) ∈ L 0 ( F t -1 ) × L 0 (I R d , F t -1 ) such that

$$\i s \left ( x _ { t - 1 } , \theta _ { t - 1 } \right ) & \in L ^ { \mathfrak { G } } ( \mathfrak { I } _ { t - 1 } ) \times L ^ { \mathfrak { E } } ( \mathbb { R } ^ { \mathfrak { G } } , \mathcal { I } _ { t - 1 } ) \text { such that } \\ & \quad I _ { \{ \tau \leq t - 1 \} } x _ { t - 1 } ^ { \mathbb { G } } \geq 0 , \quad P _ { \cdot \cdot \cdot } a . s . \quad I _ { \{ \tau = t \} } \left ( x _ { t - 1 } ^ { \mathbb { E } } + \theta _ { t - 1 } ^ { \mathbb { E } } \Delta S _ { t } \right ) \geq 0 \quad P _ { \cdot \cdot \cdot } a . s . , \\ & \quad \text {and } \quad I _ { \{ \tau > t \} } \left ( \left ( x _ { t - 1 } ^ { \mathbb { E } } + \theta _ { t - 1 } ^ { \mathbb { E } } \Delta S _ { t } \right ) \geq g _ { t } I _ { \{ \tau > t \} } \quad P _ { \cdot \cdot \cdot } a . s . \\ \intertext { a n d } \text { ranks to Lemma 2.2, the last two equations above are equivalent to }$$

Thanks to Lemma 2.2, the last two equalities above are equivalent to

Clearly, these can be rewritten into the following equivalent form of

$$\text {thanks to Benjamin 2.2; the assessor who equidates above are eqnivent to} \\ ( \widetilde { G } _ { t } - G _ { t } ) \left ( x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { S } _ { t } \right ) \geq 0 , \quad \text {and} \ G _ { t } \left ( x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { S } _ { t } \right ) \geq g _ { t } G _ { t } . \\ \text { clearly, these can be rewritten into the following equivalent form of}$$

$$x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { \overline { S } _ { t } } \geq g _ { t } I _ { \{ \tilde { G } _ { t } = G _ { t } > 0 \} } + g _ { t } ^ { + } I _ { \{ \tilde { G } _ { t } \colon G _ { t } > 0 \} } + \delta _ { t - 1 } 1 _ { \{ \tilde { G } _ { t } = 0 \} } ,$$

for some δ t -1 ∈ L 0 ( F t -1 ). Hence, we conclude that x G t -1 ∈ P ( S τ , G ) t -1 ,t ( ξ ) if and only if there exists an F t -1 -measurable triplet ( x F t -1 , θ F t -1 , δ t -1 ) such that

$$I _ { \{ \tau \leq t - 1 \} } x _ { t - 1 } ^ { \mathbb { G } } & \geq 0 , \quad I _ { \{ \tau \geq t \} } x _ { t - 1 } ^ { \mathbb { G } } = I _ { \{ \tau \geq t \} } x _ { t - 1 } ^ { \mathbb { F } } \\ x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { S } _ { t } & \geq g _ { t } I _ { \{ \tilde { G } _ { t } = G _ { t } > 0 \} } + g _ { t } ^ { + } I _ { \{ \tilde { G } _ { t } > G _ { t } > 0 \} } + \delta _ { t - 1 } I _ { \{ \tilde { G } _ { t } = 0 \} } .$$

Or equivalently there exists ( x F t -1 , δ t -1 ) ∈ P ( S, F ) t -1 ,t ( ξ F t ) × L 0 ( F t -1 ) such that

$$I _ { \{ \tau \leq t - 1 \} } x _ { t - 1 } ^ { \mathbb { G } } \geq 0 , \text { and } I _ { \{ \tau \geq t \} } x _ { t - 1 } ^ { \mathbb { G } } = I _ { \{ \tau \geq t \} } x _ { t - 1 } ^ { \mathbb { F } } , \text { where } \xi _ { t } ^ { \mathbb { F } } \coloneqq \widehat { g } _ { t } + \delta _ { t - 1 } I _ { \{ \widetilde { G } _ { t } = 0 \} } .$$

Therefore, the first equality in (4.3) follows immediately. To prove the second equality, we combine (4.9), Lemma 2.2 again, ( τ ≥ t ) ⊂ ( G t -1 &gt; 0) and the fact that ˜ Q ( ˜ G t = 0 &lt; G t -1 ) = 0, and conclude that x G t -1 ∈ P ( S τ , G ) t -1 ,t ( ξ ) iff there exists ( x F t -1 , θ F t -1 ) ∈ L 0 ( F t -1 ) × L 0 (I R d , F t -1 ) such that

$$I _ { \{ \tau \leq t - 1 \} } x _ { t - 1 } ^ { \mathbb { G } } & \geq 0 \quad P _ { \text {-a.s.} } , \quad I _ { \{ \tau \geq t \} } x _ { t - 1 } ^ { \mathbb { G } } = I _ { \{ \tau \geq t \} } x _ { t - 1 } ^ { \mathbb { F } } , \quad P _ { \text {-a.s.} } , \\ \text {and} \quad x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \widetilde { S } _ { t } & \geq \widehat { g } _ { t } , \quad \widetilde { \mathcal { Q } } _ { \text {a.s.} } .$$

˜ ̂ ˜ This proves the second equality and ends the proof of assertion (a). The proof of assertion (b) mimics exactly the proof of assertion (a) and will be omitted.

Part 2: Here we prove assertion (c). Suppose that ξ t = K τ 1 { τ ≤ t } . Then x G t -1 ∈ P ( S τ , G ) t -1 ,t ( ξ t ) if and only if there exists θ G t -1 ∈ L 0 (I R d , G t -1 ) such that x G t -1 + θ G t -1 ∆ S τ t ≥ K τ 1 { τ ≤ t } . Again thanks to Lemma 2.1, we deduce the existence of a pair ( x F t -1 , θ F t -1 ) ∈ L 0 ( F t -1 ) × L 0 (I R d , F t -1 ) such that

$$x _ { t - 1 } ^ { \mathbb { G } } I _ { \{ \tau \geq t \} } = x _ { t - 1 } ^ { \mathbb { F } } I _ { \{ \tau \geq t \} } , \quad \text {and} \quad \theta _ { t - 1 } ^ { \mathbb { G } } I _ { \{ \tau \geq t \} } = \theta _ { t - 1 } ^ { \mathbb { F } } I _ { \{ \tau \geq t \} } .$$

Hence, we deduce that x G t -1 ∈ P ( S τ , G ) t -1 ,t ( ξ t ) is equivalent to

$$& t = 1 , \quad t = 1 , \L ( \mathfrak { x } ) = 1 , \ \mathfrak { x } = \int _ { \mathfrak { x } _ { 0 } - 1 } \mathbb { E } _ { t } + \theta _ { t - 1 } ^ { \mathbb { E } } \Delta \overline { S } _ { t } \overline { S } _ { t } \right ) 1 _ { \{ \tau \geq t \} } \geq K _ { t } 1 _ { \{ \tau = t \} } , \\ & ( x _ { t - 1 } ^ { \mathbb { G } } + \theta _ { t - 1 } ^ { \mathbb { G } } \Delta S _ { t } ) I _ { \{ \tau \geq t \} } = \left ( x _ { t - 1 } ^ { \mathbb { E } } + \theta _ { t - 1 } ^ { \mathbb { E } } \Delta \overline { S } _ { t } \right ) 1 _ { \{ \tau \geq t \} } \geq K _ { t } 1 _ { \{ \tau = t \} } , \\ & \text { and } \quad x _ { t - 1 } ^ { \mathbb { G } } 1 _ { \{ \tau \leq t - 1 \} } \geq K _ { \tau } 1 _ { \{ \tau \leq t - 1 \} } .$$

Thus, on the one hand, the second inequality in (4.14) is equivalent to

$$x _ { t - 1 } ^ { \mathbb { G } } 1 _ { \{ \tau \leq t - 1 \} } - K _ { \tau } 1 _ { \{ \tau \leq t - 1 \} } \in L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } .$$

On the other hand, in virtue of Lemma 2.2, the first inequality in (4.14) is equivalent to x F t -1 + θ F t -1 ∆ S t ≥ κ (0) t + x F t -1 I { ˜ G t =0 } , or equivalently

$$x _ { t - 1 } ^ { \mathbb { F } } \in \bigcup _ { \delta _ { t - 1 } \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } \mathcal { P } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } , P ) } ( \kappa _ { t } ^ { ( 0 ) } + \delta _ { t - 1 } ^ { \mathbb { F } } I _ { \{ \widetilde { G } _ { t } = 0 \} } ) .$$

Therefore, by combining this last fact with (4.15) and (4.14), the first equality in (4.5) follows immediately, while the proof for the second equality mimics the proof of the second equality in assertion (a) (see part 1). This ends the proof of assertion (c).

Part 3. Hereto, we prove assertion (d). Thus, consider ξ = g t I { τ&gt;t } + K τ I { τ ≤ t } . Then x G t -1 ∈ P ( S τ , G ) t -1 ,t ( ξ ) iff there exists θ G t -1 ∈ L 0 (I R d , G t -1 ) such that

$$x _ { t - 1 } ^ { \mathbb { G } } + \theta _ { t - 1 } ^ { \mathbb { G } } \Delta S _ { t } ^ { \tau } \geq g _ { t } I _ { \{ \tau > t \} } + K _ { \tau } 1 _ { \{ \tau \leq t \} } \quad P _ { \mu } a . s . .$$

On the one hand, due to Lemma 2.1, there exists a pair ( x F t -1 , θ F t -1 ), which belongs to L 0 ( F t -1 ) × L 0 (I R d , F t -1 ) and satisfies

$$( x _ { t - 1 } ^ { \mathbb { G } } , \theta _ { t - 1 } ^ { \mathbb { G } } ) I _ { \{ \tau \geq t \} } = ( x _ { t - 1 } ^ { \mathbb { F } } , \theta _ { t - 1 } ^ { \mathbb { F } } ) I _ { \{ \tau \geq t \} } .$$

By inserting these in (4.16), we conclude that x G t -1 ∈ P ( S τ , G ) t -1 ,t ( ξ ) iff P -a.s. x G t -1 I { τ ≤ t -1 } ≥ K τ I { τ ≤ t -1 } and ( x F t -1 + θ F t -1 ∆ S t ) I { τ ≥ t } ≥ g t I { τ&gt;t } + K τ 1 { τ ≤ t } . Or equivalently P -a.s. we have

$$x _ { t - 1 } ^ { \mathbb { G } } I _ { \{ \tau \leq t - 1 \} } & \geq K _ { \tau } I _ { \{ \tau \leq t - 1 \} } , \ ( x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { S } ) I _ { \{ \tau = t \} } \geq K _ { t } 1 _ { \{ \tau = t \} } \\ \text {and } ( x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { S } ) I _ { \{ \tau > t \} } & \geq g _ { t } 1 _ { \{ \tau > t \} } .$$

Thanks to Lemma 2.2-(a)-(b), this equivalent to P -a.s.

$$x _ { t - 1 } ^ { \mathbb { G } } I _ { \{ \tau \leq t - 1 \} } & \geq K _ { \tau } I _ { \{ \tau \leq t - 1 \} } , \ ( x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { \mathbb { S } } _ { t } ) I _ { \{ G _ { t } > 0 \} } \geq g _ { t } 1 _ { \{ G _ { t } > 0 \} } , \\ \text {and } ( x _ { t - 1 } ^ { \mathbb { F } } + \theta _ { t - 1 } ^ { \mathbb { F } } \Delta \overline { S } _ { t } ) I _ { \{ \widetilde { G } _ { t } > G _ { t } \} } & \geq K _ { t } 1 _ { \{ \widetilde { G } _ { t } > C _ { t } \} } .$$

Furthermore, it is easy to check that the two last inequalities above are equivalent to ( x F t -1 + θ F t -1 ∆ S t ) I { ˜ G t &gt; 0 } ≥ κ ( g ) t 1 { ˜ G t &gt; 0 } P -a.s.,or equivalently

$$x _ { t - 1 } ^ { \mathbb { F } } \in \bigcup _ { \delta _ { t } \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } \mathcal { P } _ { t - 1 , t } ^ { ( \overline { S } , \overline { \mathbb { F } } ) } ( \kappa _ { t } ^ { ( g ) } + \delta _ { t } I _ { \{ \widetilde { G } _ { t } = 0 \} } ) . \\$$

By combining all these facts, the first equality in (4.6) follows, while the proof of the second equality mimics exactly the proof of the second equality of assertion (a). This proves assertion (d), and ends the proof of theorem.

## 4.2 The Immediate-Profit arbitrage under random horizon

This section analyzes the impact of random horizon on the absence-of-immediate-profit arbitrage (called AIP hereafter), in many aspects.

The following theorem is our first main result of this subsection, and it fully charaterizes the AIP for the model ( S τ , G , P ) using the model ( S, F , Q ).

- (b) ( ˜ S, F , ˜ Q ) fulfills the AIP condition (c) ( S, F , P ) satisfies the AIP condition. Then (a) ⇐⇒ (b) and (b) = ⇒ (c).

˜ ˜ Theorem 4.2. Let ( S, F , P ) and ( ˜ S, F , ˜ Q ) be the models given by (4.1) and (3.21)-(4.1) respectively, and consider the following assertions. (a) ( S τ , G , P ) satisfies the AIP condition

Proof. of Theorem 4.2. The proof of the theorem will be given in two parts, where we prove (a) ⇐⇒ (b) and (b) = ⇒ (c) respectively.

Part 1. Hereto, we prove (a) ⇐⇒ (b). To this end, we suppose that assertion (a) holds. Thus, in virtue of Proposition 2.8-(b), assertion (a) is equivalent to

$$\mathcal { P } _ { t - 1 , t } ^ { ( S ^ { \tau } , \mathbb { G } ) } ( 0 ) \subseteq L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) , \ \ t \in \{ 1 , \dots , T \} .$$

Thus, thanks to Theorem 4.1-(a), these inclusions imply that for t ∈ { 1 , ..., T } ,

$$\mathcal { P } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { F } , \tilde { Q } ) } ( 0 ) I _ { \{ \tau \geq t \} } & \subseteq \mathcal { P } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { F } , \tilde { Q } ) } ( 0 ) I _ { \{ \tau \geq t \} } + L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } \\ & = \mathcal { P } _ { t - 1 , t } ^ { ( S ^ { \tau } , \tilde { G } ) } ( 0 ) \subseteq L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) .$$

Taking conditional expectation knowing F t -1 , we get P ( ˜ S, F , ˜ Q ) t -1 ,t (0) G t -1 ≥ 0. Therefore, by combining these with P ( ˜ S, F , ˜ Q ) t -1 ,t (0) I { G t -1 =0 } ⊆ L 0 + ( F t -1 ), we deduce that P ( ˜ S, F , ˜ Q ) t -1 ,t (0) ⊆ L 0 + ( F t -1 ) ⊆ L 0 + ( F t -1 , ˜ Q ), and assertion (b) follows. This proves (a)= ⇒ (b). To prove the reverse, we assume that assertion (b) holds. In virtue of Proposition 2.8-(b), this assumption is equivalent to P ( ˜ S, F , ˜ Q ) t -1 ,t (0) ⊆ L 0 + ( F t -1 , ˜ Q ). Then, thanks to Theorem 4.1, we derive

$$\mathcal { P } _ { t - 1 , t } ^ { ( S ^ { r } , \mathbb { G } ) } ( 0 ) & = L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } + \mathcal { P } _ { t - 1 , t } ^ { ( \widetilde { S } , \widetilde { F } , \widetilde { Q } ) } ( 0 ) I _ { \{ \tau \geq t \} } \\ & \subseteq L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } + L _ { + } ^ { 0 } ( \mathcal { F } _ { t - 1 } ) I _ { \{ \tau \geq t \} } = L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) ,$$

where the last equality is a consequence of Lemma 2.1. This ends part 1.

Part 2. This part proves (b) = ⇒ (c). Thus, we suppose that assertion (b) holds. Then, in virtue of Proposition 2.8-(c), this is equivalent to

$$\overset { \widetilde { Q } } { \underset { \mathcal { F } _ { t - 1 } } { \text {sup} } } ( \theta \Delta \widetilde { S } _ { t } ) & \geq 0 , \quad \widetilde { Q } \text {-a.s.} , \quad \text {for any } ( t , \theta _ { t - 1 } ) \in \{ 1 , \dots , T \} \times L ^ { 0 } ( \mathbb { R } ^ { d } , \mathcal { F } _ { t - 1 } ) . \\ \\ \text {We do} \quad \text {that} \quad \mathbb { F } ^ { \mathbb { F } } _ { \ } \text {osc} \{ \theta \} \cdot \Delta \widetilde { S } \Big ) & > 0 \quad \text {As } ( \mathbb { F } ^ { \mathbb { F } } \Big ) \geq 0 \bigcup ( C \cdot \Big ) \to 0 \Big ) = ( C \cdot \Big ) \to 0 \Big )$$

that 1 { G t -1 &gt; 0 } ˜ Q ess sup F t -1 ( θ t -1 ∆ ˜ S t ) ≥ 0 and Corollary 3.7-(a) applies. Therefore, for any ( t, θ t -1 ) ∈ { 1 , ..., T } × L 0 (I R d , F t -1 ), we obtain

We deduce that Z F t -1 ˜ Q ess sup F t -1 ( θ t -1 ∆ ˜ S t ) ≥ 0. As ( Z F t -1 &gt; 0) ∩ ( G t -1 &gt; 0) = ( G t -1 &gt; 0), we get

$$\underset { \mathcal { F } _ { t - 1 } } { \text {ess sup} } ( \theta _ { t - 1 } \Delta \overline { S } _ { t } ) = \underset { \mathcal { F } _ { t - 1 } } { \text {ess sup} } ( \theta _ { t - 1 } \Delta \widetilde { S } _ { t } I _ { \{ \widetilde { G } _ { t } > 0 \} } ) \geq 0 . \\ \intertext { b ) f o l l o w s f o r m o b i n g i n g t h i n s i q u a l l y w i t h P r o s i t i o n 2 . 8 - ( c )$$

Thus, assertion (b) follows from combining this inequality with Proposition 2.8-(c). This ends the second part, and completes the proof of the theorem.

Theorem 4.2 gives a complete and full characterization of AIP for the stopped model ( S τ , G ) using the model ( ˜ S, F , ˜ Q ), and shows that this is sufficient for the AIP fulfillment of the model ( S, F , P ). However, in general and in contrast to the classical non-arbitrage (called NA afterwards), the AIP of ( S, F , P ) does not give enough information about the AIP for the stopped model. This shows how the impact of τ can deepen the difference between AIP and NA, which is pointed out in [6, Section 2.4].

Corollary 4.3. If the pair ( S, τ ) satisfies

/negationslash then the three assertions of Theorem 4.2 are equivalent. More precisely,

/negationslash

$$P ( \bigcup _ { t = 1 } ^ { T } ( ( \Delta S _ { t } \neq 0 ) \cap ( \widetilde { G } _ { t } = 0 < G _ { t - 1 } ) ) ) = 0 , \\ \intertext { s s e r t i o n s o f The o r e m $ 4 . 2 a r e $ e q u i v a l e n t . $ $ M o r e $ p r e c i s e l y , }$$

/negationslash

/negationslash

$$\underset { \mathcal { Y } \ \theta \in L ^ { 0 } ( \mathbb { R } ^ { d } , \mathcal { F } _ { t - 1 } ) . \ T h e r e f o r e , \, t \, c o n c l u s i o n \, f o l l o w s \, i m m e d i a t y \, f o r \, \mathbf s u p e q u a l t i c h e q u a l t i o n .$$

( ˜ S, F , ˜ Q ) fulfills AIP iff ( S, F , P ) fulfills AIP iff ( S τ , G , P ) satisfies AIP . Proof. Thanks to Theorem 4.2, the proof of this corollary boils down to prove that, under (4.20), the AIP of ( S, F , P ) implies the AIP of ( ˜ S, F , ˜ Q ). To this end, note that ∆ ˜ S t = I { G t -1 &gt; 0 } ∆ S t = I { G t -1 &gt; 0 } ∆ S t I { ∆ S t =0 } . By assumption, I { G t -1 &gt; 0 } I { ∆ S t =0 } = I { ∆ S t =0 } 1 { G t -1 &gt; 0 } I { ˜ G t &gt; 0 } . Due to ( ˜ G t &gt; 0) ⊆ ( G t -1 &gt; 0), we get ∆ ˜ S t = I { ˜ G t &gt; 0 } ∆ S t = ∆ S t , i.e. ˜ S = S . Thus, by combining this with the fact that ˜ Q is equivalent to P on ( G t -1 &gt; 0) which is due to ( Z F t -1 &gt; 0) ∩ ( G t -1 &gt; 0) = ( G t -1 &gt; 0), we easily derive for any θ ∈ L 0 (I R d , F t -1 ). Therefore, the conclusion follows immediately from these equalities.

Remark 4.4. (a) If the random time τ satisfies

$$P \left ( \bigcup _ { t = 1 } ^ { T } \{ \tilde { G } _ { t } = 0 < G _ { t - 1 } \} \right ) > 0 ,$$

then there exist models for S such that ( S, F ) satisfies AIP, while ( S τ , G ) violates it. In fact, consider and derive, by Theorem 3.2, the following

$$\Delta S _ { t } \coloneqq I _ { \{ \tilde { G } _ { t } = 0 < G _ { t - 1 } \} } - P ( \tilde { G } _ { t } = 0 < G _ { t - 1 } | \mathcal { F } _ { t - 1 } ) , \quad t = 1 , \dots , T , \\ \intertext { v e , \, b y \, T h e o r e m \ 3 . 2 , \, t h e \, f o l l o w i n g }$$

$$a n d \, d e r i v , \, b y \, \text { Theorem } \, 3 . 2 , \, \ t h e \, f o l l o w i n g \\ \quad & \quad \text {ess sup} ( \theta \overline { \Delta } \overline { S } _ { t } ) = \theta ^ { - } P ( \widetilde { G } _ { t } = 0 < G _ { t - 1 } | \mathcal { F } _ { t - 1 } ) \geq 0 , \, \text { for any } \theta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) , \\ \quad & \quad \widetilde { \mathcal { F } } _ { t - 1 } \\ \quad & \quad \widetilde { \mathcal { F } } _ { t - 1 } \\ \quad & \quad P \\ \quad & \quad \text {ess sup} ( \theta \Delta S _ { t } ^ { \tau } ) = - \theta P ( \widetilde { G } _ { t } = 0 < G _ { t - 1 } | \mathcal { F } _ { t - 1 } ) , \, \widetilde { \mathcal { Q } } \cdot a . s . , \, \text { for any } \theta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) , \\ \quad & \quad \text {Thurs, } \, \text { it is clear that, the first inequaltity above} \, \text { praves that } ( \overline { S } , \mathbb { F } ) \ f u l f l l s \ A P , \, \text { while the second and } \\ \quad & \quad \text {third equalities prove that} \, \text { both } ( \widetilde { S } , \mathbb { F } ) \ a n d \ ( S ^ { \tau } , \mathbb { G } , P ) \ v i o l a t e \ A P \, \text { respectively.}$$

Thus, it is clear that, the first inequality above proves that ( S, F ) fulfills AIP, while the second and third equalities prove that both ( ˜ S, F , ˜ Q ) and ( S τ , G , P ) violate AIP respectively. (b) Suppose that τ satisfies (4.21), and consider the model for S given by ∆ S t := I { ˜ G t =0 } , t ∈ { 1 , ..., T } and S 0 = 1 . Then ( S, F ) violates the AIP and hence violates NA also, due to

$$\underset { \mathcal { F } _ { t - 1 } } { \text {ess} } \sup _ { t - 1 } ( \theta _ { t - 1 } \Delta S _ { t } ) = \theta _ { t - 1 } ^ { + } I _ { \{ P ( \widetilde { G } _ { t } = 0 | \mathcal { F } _ { t - 1 } ) > 0 \} } - \theta _ { t - 1 } ^ { - } I _ { \{ G _ { t - 1 } = 0 \} } .$$

Furthermore, we have S τ = S 0 and hence ( S τ , G ) fulfills AIP and NA.

(c) Suppose that τ satisfies (4.21), and consider S given by ∆ S t := I { ˜ G t =0 &lt;G t -1 } , t ∈ { 1 , ..., T } and S 0 = 1 . Therefore, one can check that ( S, F ) fulfills AIP and violates NA. In fact, there exists no positive F -martingale Z such that

$$\ p o s s u t { \mathbb { I } = \mathbb { I } } { n } { a } { i } { g } { a } { y } { d } { e } { 2 } { s } { a } { t } { w } { a } { t } \\ E \left [ \Delta S _ { t } \frac { Z _ { t } } { Z _ { t - 1 } } \Big | \mathcal { F } _ { t - 1 } \right ] = 0 , \ \ a n d \ \ e s s \sup ( \theta \Delta S _ { t } ) = \theta ^ { + } I _ { \{ P ( \tilde { G } _ { t } = 0 < G _ { t - 1 } | \mathcal { F } _ { t - 1 } ) > 0 \} } \geq 0 , \\ \text {for any } \theta \in L _ { + } ^ { 0 } ( \mathcal { F } _ { t - 1 } ) . \ \ H a w e v e r , \ \text {we have } S ^ { \tau } = S _ { 0 } \ \ a n d , \ \ a s \ a \, c o n s e q u e n c e , \ ( S ^ { \tau } , \mathbb { G } ) \ f u l f i l s \ N A \ a n d \ h e n c e$$

for any θ ∈ L 0 + ( F t -1 ) . However, we have S τ = S 0 and, as a consequence, ( S τ , G ) fulfils NA and hence it satisfies AIP also.

Our second main theorem, of this subsection, describes the models of τ for which the AIP condition is unaffected after stopping.

Theorem 4.5. The following assertions are equivalent.

- (a) For any ( X, F , P ) satisfying AIP, the model ( X τ , G , P ) fulfills AIP.

(c) The probability ˜ Q defined in (3.21) coincides with P , i.e. Z F ≡ 1 . The proof of this theorem is based on the following simple but useful lemma.

- (b) For any t ∈ { 1 , ..., T } , we have { G t -1 = 0 } = { ˜ G t = 0 } P -a.s..

Lemma 4.6. Consider any model ( X, H := ( H t ) t =0 ,...,T ) . Then the following assertions hold.

- (a) Suppose that X is H -predictable. Then ( X, H ) satisfies AIP if and only if X is a constant process, i.e., X t = X 0 , P -a.s. for any t = 1 , ..., T .
- (b) ( X, H ) satisfies AIP if and only if ( ψ · X, H ) fulfills AIP, for any H -predictable and bounded process ψ , where ψ · X t := ∑ t s =1 ψ s ∆ X s , t ∈ { 1 , ..., T } .

Proof. 1) If X is a constant process, then it is clear that P ( X, H ) t (0) = L 0 + ( H t ). This implies that P ( X, H ) t (0) ∩ L 0 -( H t ) = { 0 } , or equivalently AIP holds for ( X, H ). To prove the reverse sense, we

suppose P ( X, H ) t (0) ∩ L 0 -( H t ) = { 0 } , and consider any t ∈ { 1 , .., T } and θ ∈ L 0 ( H t -1 ). Therefore, by combining all these properties, we derive

$$\underset { \mathcal { H } _ { t - 1 } } { \text {assup} } ( - \theta _ { t - 1 } \Delta X _ { t } ) = - \theta _ { t - 1 } \Delta X _ { t } \in \mathcal { P } _ { t - 1 } ^ { ( X , \mathbb { H } ) } ( 0 ) \subset L _ { + } ^ { 0 } ( \mathcal { H } _ { t - 1 } ) .$$

Thus, as θ t -1 is arbitrary, we conclude that θ t -1 ∆ X t = 0 P -a.s., for any θ t -1 ∈ L 0 ( H t -1 ). This implies that ∆ X t = 0 P -.a.s., and the proof of assertion (a) is complete.

2) If ( ψ · X, H ) fulfills AIP, for any H -predictable and bounded process ψ , then by taking ψ = 1 we deduce that ( X -X 0 , H ) satisfies AIP or equivalently ( X, H ) satisfies AIP. The reverse sense is a direct consequence from combining Proposition 2.8-(b) and the fact that, for any t ∈ { 1 , ..., T } and any H -predictable process ψ , we have

$$\underset { \theta _ { t - 1 } \in L ^ { 0 } ( \mathcal { H } _ { t - 1 } ) } { \text {issinf} } \underset { \mathcal { H } _ { t - 1 } } { \text {isssup} } ( \theta _ { t - 1 } \Delta X _ { t } ) \leq \underset { \varphi _ { t - 1 } \in L ^ { 1 0 } ( \mathcal { H } _ { t - 1 } ) } { \text {issinf} } \underset { \mathcal { H } _ { t - 1 } } { \text {isssup} } ( \varphi _ { t - 1 } \psi _ { t } \Delta X _ { t } ) , \quad P _ { \text {a.s.} } .$$

This ends the proof of the lemma.

Proof. of Theorem 4.5. The equivalence between assertion (b) and (c) can be found in Choulli/Deng [7], and for the sake of completeness we reproduce it here. Thus, remark that Z F ≡ 1 -i.e. for any t ∈ { 1 , ..., T } we have Z F t /Z F t -1 = 1 P -a.s.- is equivalent to or equivalently,

$$\frac { I _ { \{ \tilde { G } _ { t } > 0 \} } } { P ( \widetilde { G } _ { t } > 0 | \mathcal { F } _ { t = 1 } ) } = I _ { \{ G _ { t - 1 } > 0 \} } , \quad P - a . s . , \\ \\ \\ \\ \intertext { f o r } P ( \widetilde { G } _ { t } > 0 | \mathcal { F } _ { t = 1 } ) \cdot \Psi = \Psi ^ { ( \widetilde { G } _ { t } ) }$$

$$I _ { \{ \tilde { G } _ { t } > 0 \} } = & P ( \tilde { G } _ { t } > 0 | \mathcal { F } _ { t - 1 } ) I _ { \{ G _ { t - 1 } > 0 \} } = P ( \tilde { G } _ { t } > 0 | \mathcal { F } _ { t - 1 } ) , \quad P ^ { \text {a.s.} } \\ \intertext { r e , s i n g u i v a l e n t o u } & \intertext { o r e , s i n g u i v a l e n t o u } & \intertext { o r e , s i n g u i v a l e n t o u }$$

Suppose that assertion (b) holds. Then we get ( X, H , P ) = ( ˜ X, H , P ) and it satisfies AIP for any model ( X, H ) satisfying AIP, see Lemma 4.6 (b). Thus, in virtue of Theorem 4.2 , ( X τ , G ) satisfies AIP. This proves the implication (b) = ⇒ (a). To prove the reverse, assume assertion (a) holds, and consider

Therefore, this is equivalent to { P ( ˜ G t &gt; 0 |F t -1 ) = 1 } = { ˜ G t &gt; 0 } , while the latter equality holds if and only if { G t -1 &gt; 0 } = { G t &gt; 0 } . Hence, (b) ⇐⇒ (c) is proved, and the rest of this proof proves (b) ⇐⇒ (a).

$$X _ { t } \coloneqq \sum _ { s = 1 } ^ { t } \left ( I _ { \{ \tilde { G } _ { s } = 0 \} } - P ( \widetilde { G } _ { s } = 0 | \mathcal { F } _ { s - 1 } ) \right ) , \ \ t = 1 , \dots , T . \\ \intertext { a r t h a t ( X , \mathbb { F } ) s a t i s f i s e N A a n d a f o r i o r i AIP d u e t o C a rassus-Lepinett t t t }$$

$$I _ { \{ t \leq \tau \} } P ( \widetilde { G } _ { t } = 0 \Big | \mathcal { F } _ { t - 1 } ) = 0 , \quad P _ { \mu \text {a.s.} } \\ \intertext { t a t i o n w i t h e r s e p e c t o \mathcal { F } _ { t - 1 } o n b o t h s i d e s o f this e } \text {v.t.} \quad T \text {this yields } ( G _ { \mu } \searrow 0 ) \cap ( \widetilde { G } _ { \mu } ) = 0$$

Then it is clear that ( X, F ) satisfies NA and a fortiori AIP due to Carassus-Lepinette [6]. Due to assertion (a), the AIP for ( X τ , G ) follows, and due to ( τ ≥ s ) ∩ ( ˜ G s = 0) = ∅ , we get X τ = -∑ t s =1 I { s ≤ τ } P ( ˜ G s = 0 ∣ ∣ F s -1 ) and hence it is G -predictable. Thus, in virtue of Lemma 4.6-(a), we conclude that X τ is a null process, or equivalently for any t ∈ { 1 , ..., T }

By taking conditional expectation with respect to F t -1 on both sides of this equality, we get G t -1 P ( ˜ G t = 0 ∣ ∣ F t -1 ) = 0 , P -a.s., for any t ∈ { 1 , ..., T } . This yields ( G t -1 &gt; 0) ∩ ( ˜ G t = 0) = ∅ , P -a.s. for any t ∈ { 1 , ..., T } and assertion (b) follows immediately. This ends the proof of the theorem.

## 5 Pricing formulas for vulnerable claims

In this section, we elaborate super-hedging-pricing formulas for several vulnerable claims. Our pricing formulas relies on [6, Lemma 3.1], which we recall below, and which elaborates a backward equation for the super-hedging prices for an arbitrary model ( X, H , Q ).

Lemma 5.1. Let ( X, H , Q ) be an arbitrary model satisfying AIP and defined on the probability space (Ω , G , P ) , T ∈ (0 , ∞ ) is a fixed investment horizon, and ξ ∈ L 0 ( H T ) is a claim. Then the super-hedging price process for ξ , denoted by P H , is given by the following backward formula.

̂ The vulnerable claims that we address herein can be classified into two main classes. The first class consists of vulnerable claims that do not have recovery, or equivalently there is no payment at the random time. The second class of vulnerable claims is those claims which involve payment at the random time in a way or another. In virtue of Lemma 5.1, an important step in describing the G -price process for vulnerable claims lies in addressing the impact of τ on the one-step pricing operator ̂ P ( X, H ,Q ) t,t +1 ( · ), defined in (2.6), for any model ( X, H , Q ).

$$\mathfrak { L } , \mathfrak { G } , \mathfrak { P } , & T \in ( 0 , \infty ) \text { is a fixed environment horizon, and $\xi \in L^{0}(\mathcal{H}_{T})$ is a claim. } \text { Then the super-healing } \\ \text {price process for $\xi$, denoted by $\widehat{ \mathcal{P}}$} , & \text { is given by the following gain backward formula} . \\ & \widehat{ \mathcal{P}} _ { t } ^ { \mathbb { H } } = \text {ess infess sup } \left ( \widehat{ \mathcal{P}} _ { t + 1 } ^ { \mathbb { H } } - \theta \Delta x _ { t + 1 } \right ) = \widehat{ \mathcal{P}} _ { t , t + 1 } ^ { \mathbb { H } } \left ( \widehat{ \mathcal{P}} _ { t + 1 } ^ { \mathbb { H } } \right ) , \, t \leq T - 1 , \\ & \widehat{ \mathcal{P } } _ { T } ^ { \mathbb { H } } = \xi . \\ \intertext { h o w n l o r n o b l a g i m s t h o w d d r o w s h o r i n g a n b o g l a s c f i o d i n t o t w o r n o m a l l a c o s s . }$$

The remaining part of this section is divided into three subsections. In the first subsection, we outline the main results on the one-step pricing operators. The second subsection elaborates the general pricing formulas, while the last subsection proves the main theorems of the second subsection.

## 5.1 The one-step pricing formulas

In this subsection, we address the one-step-pricing operator under the random horizon τ in many aspects. Precisely, following the same spirit as in arbitrage theory, we aim to understand how the one-step-pricing under G can be described using F -observable processes and pricing-operators.

Theorem 5.2. Let t ∈ { 1 , .., T } , ξ ∈ L 0 ( G t ) , ( g s ) s =0 ,...,T and ( K s ) s =0 ,....,T be two F -adapted processes, and consider ( g, κ (0) , κ ( g ) , g ) given by (4.2).Then the following assertions hold.

̂ (a) If ξ = g t I { τ&gt;t } , then we have

(b) If ξ = g t I { τ ≥ t } , then we have

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S T ) } ( \xi ) = \underset { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } { \text {iss inf } } \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widehat { S } , \widehat { \mathbb { R } } ) } ( \widehat { g } _ { t } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \widehat { \tau } \geq t \} } = \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widehat { S } , \widehat { \mathbb { R } } ) } ( \widehat { g } _ { t } ) I _ { \{ \tau \geq t \} } . \\ ( \flat ) \ I f \xi = g _ { t } I _ { \{ \tau \geq t \} } , \text { then } w \ h a v e \\ \overline { \mathcal { I } } ( \mathcal { I } ) = \widehat { \mathcal { I } } _ { t } , \quad \overline { \mathcal { I } } ( \mathcal { I } ) = \widehat { \mathcal { I } } _ { t } , \quad \widetilde { \mathcal { I } } ( \widetilde { \mathcal { I } } ) = \widehat { \mathcal { I } } _ { t } , \quad \widetilde { \mathcal { I } } ( \widetilde { \mathcal { I } } ) .$$

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ) & = \underset { \delta ^ { \infty } ( \mathcal { F } _ { t - 1 } ) } { \text {ess} } \inf _ { \mathcal { T } _ { t - 1 , t } ^ { ( \widehat { S } , \mathbb { R } ) } ( \overline { g } _ { t } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) } I _ { \{ \epsilon \succ t \} } = \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widehat { S } , \mathbb { R } ) } ( \overline { g } _ { t } ) I _ { \{ \epsilon \succ t \} } . \\ ( c ) \ I f \xi & = K _ { \tau } I _ { \{ \tau \leq t \} } , \text { then}$$

$$( c ) \ I f \xi = K _ { \tau } I _ { \{ \tau \leq t \} } , \ t h e n$$

(d) If ξ = g t I { τ&gt;t } + K τ I { τ ≤ t } , then

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi _ { t } ) & = K _ { \tau } I _ { \{ \tau \leq t - 1 \} } + \underset { \delta _ { t } \in L ^ { 0 } ( \widetilde { \mathcal { F } } _ { t - 1 } ) } { \text {issinf} } \quad \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widetilde { S } , \mathbb { F } ) } ( \kappa _ { t } ^ { ( 0 ) } + \delta _ { t } I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } \\ & = K _ { \tau } I _ { \{ \tau \leq t - 1 \} } + \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widetilde { S } , \widetilde { F } ) } ( \kappa _ { t } ^ { ( 0 ) } ) I _ { \{ \tau \geq t \} } . \\ \ f { \xi } = g _ { t } I _ { \{ \tau > t \} } + K _ { \tau } I _ { \{ \tau \leq t \} } , \text { then} \\ \widehat { \mathcal { P } } ( S ^ { T , \mathbb { G } } ) & \in K _ { \xi } \ L _ { 1 } + \dots + \underset { \delta _ { t } \in L ^ { 0 } ( \widetilde { \mathcal { F } } _ { t } ) } { \text {issinf} } \quad \widehat { \mathcal { P } } ( \overline { S } , \mathbb { F } ) ( \kappa _ { t } ) + \delta _ { t } I _ { \{ \widetilde { G } _ { t } = 0 \} } \, I _ { \{ \tau \geq t \} } \\$$

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S T , \mathbb { G } ) } ( \xi ) & = K _ { \tau } I _ { \{ \tau \leq t - 1 \} } + \underset { \delta \in L ^ { 0 } ( \mathcal { F } _ { t } - 1 ) } { \text {ess} \inf } \quad \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \kappa _ { t } ^ { ( g ) } + \delta I _ { \{ \widetilde { \mathcal { G } } _ { i } = 0 \} } ) I _ { \{ \tau > t - 1 \} } \\ & = K _ { \tau } I _ { \{ \tau \leq t - 1 \} } + \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widetilde { S } , \widetilde { \mathbb { F } } ) } ( \kappa _ { t } ^ { ( g ) } ) I _ { \{ \tau \geq t \} } .$$

$$( 5 . 4 )$$

This theorem quantifies the one-step-pricing operator for ( S τ , G ) in terms of pricing operators for both models ( S, F , P ) and ( ˜ S, F , ˜ Q ). However, the latter model is the one that gives us full F -characterization. Assertion (a) addresses the first class of vulnerable claims, where there is no recovery at all (i.e. no payment at the random time τ when it occurs). In life insurance, this claim belongs to the class when benefit is paid upon survival only. Assertion (c) deals with vulnerable claims that have payments at the random time only, and this also has a meaning in life insurance, which consists of getting benefit when the insured dies and nothing in case of survival. Assertion (d) treats another vulnerable claim with recovery, as it combines both previous cases by paying benefits in both situation when the insured survives and when she dies. The claim addressed in assertion (b) is somehow in between the two classes. On the one hand, mathematically, it belongs to the second class by choosing K = g T I ] ] T [ [ as there is payment at τ which coincides with the payment upon survival. On the other hand, its pricing formula in (5.3) tells us that this claim falls into the first class immediately after the one-step-pricing.

Remark 5.3. (i) Assertion (a) conveys that the one-step super-hedging price of the claim having no recovery (i.e. no payment at τ at all) has literally the same form. This is due to the fact that ( τ ≥ t ) = ( τ &gt; t -1) . In other words, by denoting ̂ P one-step ( · ) the one-step super-hedging pricing operator, then P one-step ( Class1 ) ⊆ Class1, and P one-step ( claim-of-assertion-(b) ) ∈ Class1.

̂ ̂ (ii) Assertion (d) proves that the one-step super-hedging price of the claim has exactly the same form as the claim, while the one-step super-hedging pricing for the claim of assertion (c) transforms the claim into the claim of assertion (d). In other words, even though there is no payment upon the survival, after one-step super-hedging pricing the claim will have payment upon survival.

Theorem 5.2 conveys also, via the formulas, that the one-step pricing operators for ( ˜ S, F , ˜ Q ) and ( S, F , P ) might differ in general, while the resulting prices for any claim are comparable. In the following remark, we discuss these points in details.

Remark 5.4. (a) For any δ ∈ L 0 ( F t -1 ) , the following inequalities hold

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \bar { S } , \bar { F } ) } ( \xi ^ { \bar { F } } + \delta I _ { t - 1 } ) I _ { \{ P ( \tilde { G } _ { t } > 0 | \mathcal { F } _ { t - 1 } ) = 1 \} } & = \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \bar { S } , \bar { F } ) } I _ { \{ P ( \tilde { G } _ { t } > 0 | \mathcal { F } _ { t - 1 } ) = 1 \} } , \\ \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \bar { S } , \bar { F } ) } ( \xi ^ { \bar { F } } + \delta I _ { \{ \tilde { G } _ { t } = 0 \} } ) I _ { \{ P ( \tilde { G } _ { t } = 0 | \mathcal { F } _ { t - 1 } ) > 0 \} } & \geq \delta I _ { \{ P ( \tilde { G } _ { t } = 0 | \mathcal { F } _ { t - 1 } ) > 0 \} } . \\ \intertext { ) \text { For } Y \in \{ \widehat { g } , \widetilde { g } , \kappa \} , \text { and } a n y \, t \in \{ 1 , \dots , T \} , \text { we have } \\ & \widehat { \mathcal { P } } _ { t } ( \tilde { S } , \tilde { F } ) \tilde { Q } ( Y ) \, < \widehat { \mathcal { P } } ( \bar { S } , \bar { F } ) ( Y ) \quad P _ { \ } a g \, \Omega \, \Omega _ { 0 } \ \{ G _ { t } , \, > \, 0 \}$$

(b) For Y ∈ { ̂ g, ˜ g, g, κ } , and any t ∈ { 1 , ..., T } , we have

As a result, this gives us another proof for Theorem 4.2 using this inequality and Proposition 2.8-(d).

In most of applications, K represents the recovery process and it is usually nonnegative. Thus, as a particular case, we consider the case of vulnerable options where both payoff process g and recovery process K are nonnegative.

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widetilde { S } , \widetilde { Q } ) } ( Y _ { t } ) & \leq \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \overline { \mathbb { Q } } ) } ( Y _ { t } ) , \quad P _ { - a . s . } \ o n \quad \{ G _ { t - 1 } > 0 \} . \\ , \text {this gives us another proof for Theorem 4.2 using this inequality and Proposititon 2.8-(d).} \\ \text {applications.} \ K \text { represents the recovery process and it is usually nonnegative. Thus, as a }$$

Corollary 5.5. Consider the notation of Theorem 5.2, and assume that both processes g and K are nonnegative. Then the following assertions hold. (1) (2)

(a) If ξ := g t I { τ&gt;t } and ξ := g t I { τ ≥ t } , then we have

$$\begin{array} { r l } { \mathfrak { c } _ { t } ^ { ( 1 ) } \colon = g _ { t } I _ { \{ \tau > t \} } \ a n d \ \xi ^ { ( 2 ) } \colon = g _ { t } I _ { \{ \tau \geq t \} } , \, then \, w e h a v } \\ { \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ^ { ( 1 ) } ) = \, \text {essinf} \, \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( g _ { t } I _ { \{ G _ { t } > 0 \} } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } } \\ { = \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { F } ) } ( g _ { t } I _ { \{ G _ { t } > 0 \} } ) I _ { \{ \tau \geq t \} } , \, } \\ { = \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ^ { ( 2 ) } ) - \delta \infty \inf _ { \mathcal { G } ^ { L } ( \mathcal { F } _ { t - 1 } ) } \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \overline { g } _ { t } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } } \\ { = \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { F } ) } ( \overline { g } _ { t } ) I _ { \{ \tau \geq t \} } } \\ { 2 3 } \end{array}$$

(b) If ξ = K τ I { τ ≤ t } , then

(c) If ξ = g t I { τ&gt;t } + K τ I { τ ≤ t } , then

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { \tau } , \mathbb { G } ) } ( \xi _ { t } ) = K _ { \tau } I _ { \{ \tau \leq t - 1 \} } + \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { F } , \tilde { Q } ) } ( K _ { t } I _ { \{ \tilde { G } _ { t } > G _ { t } \} } ) I _ { \{ \tau \geq t \} } . \\ \tilde { \xi } = g _ { t } I _ { \{ \tau > t \} } + K _ { \tau } I _ { \{ \tau \leq t \} } , \text { then }$$

̂ P ( S τ , G ) t -1 ,t ( ξ ) = K τ I { τ ≤ t -1 } + ̂ P ( ˜ S, F , ˜ Q ) t -1 ,t ( max( g t I { G t &gt; 0 } , K t I { ˜ G t &gt;G t } ) ) I { τ ≥ t } . (5.10) The proof of the corollary follows from previous results, and will be omitted.

Proof. of Theorem 5.2. The proof of the theorem will be achieved in three parts. The first and second parts prove two roughly general claims, while the third part outlines the proof for the theorem. To this end, we fix t ∈ { 1 , ..., T } , and we consider a triplet (Ξ , ξ G , ξ F ) ∈ L 0 ( G t -1 ) × L 0 ( G t ) × L 0 ( F t ) such that ξ F I { ˜ G t =0 } = 0 P -a.s..

Part 1. Herein, we prove that the equality

$$\mathcal { P } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ^ { G } ) = \Xi + L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } + \bigcup _ { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } \mathcal { P } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \xi ^ { \mathbb { F } } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } , \\$$

always implies

To this end, we remark that (5.11) implies that

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ^ { \mathbb { G } } ) = & \equiv + \underset { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } { \text {iss inf } } \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widehat { S } , \mathbb { F } ) } ( \xi ^ { \mathbb { F } } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } . \\ \intertext { s e r m a r k t h a t $ ( 5 . 1 1 ) \text { implies that } }$$

$$& \equiv + \mathcal { P } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \xi ^ { \mathbb { F } } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } \\ & \quad \subset \equiv + L _ { + } ^ { 0 } ( \mathcal { G } _ { t - 1 } ) I _ { \{ \tau \leq t - 1 \} } + \mathcal { P } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \xi ^ { \mathbb { F } } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } \subset \mathcal { P } _ { t - 1 , t } ^ { ( S ^ { \tau } , \mathbb { G } ) } ( \xi ^ { \mathbb { G } } ) ,$$

for any δ ∈ L 0 ( F t -1 ). Hence, after taking essential infimum, this inclusion implies that

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ^ { \mathbb { G } } ) \leq & \equiv + \underset { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } { \text {iss inf } } \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { T } ) } ( \xi ^ { \mathbb { F } } + \delta I _ { \{ \tilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } . \\ \intertext { t h e r \ h a n d , \ a g a i n d u e t o \ ( 5 . 1 1 ) , \ f o r \ any x ^ { \mathbb { G } } \in \mathcal { P } _ { t - 1 , t } ^ { ( S ^ { T } , \mathbb { G } ) } ( \xi ^ { \mathbb { F } } ) , \ w e d u e c t h e x i n s e t e n c o f ( x _ { t - 1 } , \delta ) \in }$$

On the other hand, again due to (5.11), for any x G ∈ P ( S τ , G ) t -1 ,t ( ξ G ), we deduce the existence of ( x t -1 , δ ) ∈ P ( S, F ) t -1 ,t ( ξ F + δI { ˜ G t =0 } ) × L 0 ( F t -1 ), such that P -a.s.,

$$x ^ { G } & \geq \Xi + x _ { t - 1 } I _ { \{ \tau \geq t \} } \geq \Xi + \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \xi ^ { \mathbb { F } } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } \\ & \geq \Xi + \underset { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } { \text {iss inf } } \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \xi ^ { \mathbb { F } } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) I _ { \{ \tau \geq t \} } .$$

As a result, this implies that

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { \tau } , \mathbb { G } ) } ( \xi ) & \geq \Xi + I _ { \{ \tau \geq t \} } \, \underset { \delta \in L ^ { 0 } ( \mathcal { F } _ { t - 1 } ) } { \text {issinf} } \, \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \overline { S } , \mathbb { F } ) } ( \widehat { g } _ { t } + \delta I _ { \{ \widetilde { G } _ { t } = 0 \} } ) , \\ \intertext { } \text {going this with } ( 5 . 1 3 ) , \, \text { we get } ( 5 . 1 2 ) . \, \text { This ends the first part.} \\ \text {not proven} \, \ t o n \, \varphi ( \overline { \mathcal { F } } _ { t } \, \mathbb { G } \, \mathbb { F } ) & \subset I _ { 0 } ^ { 0 } ( \mathcal { C } \, ) \times I _ { 0 } ^ { 0 } ( \mathcal { C } ) \times I _ { 0 } ^ { 0 } ( \mathcal { F } ) \\$$

and by combining this with (5.13), we get (5.12). This ends the first part. Part 2. This part proves that, for (Ξ , ξ G , ξ F ) ∈ L 0 ( G t -1 ) × L 0 ( G t ) × L 0 ( F t ),

$$\text {If } \mathcal { P } _ { t - 1 , t } ^ { ( S ^ { T } , G ) } ( \xi ^ { G } ) & = \Xi + L _ { + } ^ { 0 } ( \mathcal { G } _ { t } ) + \mathcal { P } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { R } , \tilde { Q } ) } ( \xi ^ { F } ) I _ { \{ \tau \geq t \} } , \quad \text {then} \\ \text {we have } \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { T } , G ) } ( \xi ^ { G } ) & = \Xi + \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \tilde { S } , \tilde { R } , \tilde { Q } ) } ( \xi ^ { \tilde { R } } ) I _ { \{ \tau \geq t \} } .$$

To this end we suppose that the left hand side of this implication holds. On the one hand, we notice

$$\Xi + \mathcal { P } _ { t - 1 , t } ^ { ( \widetilde { S } , \widetilde { \mathbb { F } } , \widetilde { \mathbb { Q } } ) } ( \xi ^ { \mathbb { F } } ) I _ { \{ \tau \geq t \} } \subset \mathcal { P } _ { t - 1 , t } ^ { ( S ^ { \tau } , \mathbb { G } ) } ( \xi ^ { \mathbb { G } } ) ,$$

and hence by taking essential infinimum on both sides we get

$$\widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( S ^ { \tau } , \mathbb { G } ) } ( \xi ^ { G } ) & \leq \Xi + \widehat { \mathcal { P } } _ { t - 1 , t } ^ { ( \widetilde { S } , \mathbb { F } , \widetilde { Q } ) } ( \xi ^ { \mathbb { F } } ) I _ { \{ \tau \geq t \} } . \\ \intertext { f o r a n y } \text { } p ^ { \mathbb { G } } \in \mathcal { P } _ { t - 1 , t } ^ { ( S ^ { \tau } , \mathbb { G } ) } ( \xi ^ { G } ) , \text { there exists } p ^ { \mathbb { F } } \in \mathcal { P } _ { t - 1 , t } ^ { ( \widetilde { S } , \mathbb { F } , \widetilde { Q } ) } ( \xi ^ { \mathbb { F } } ) \text { such that }$$

On the other hand, for any p G ∈ P ( S τ , G ) t -1 ,t ( ξ G ), there exists p F ∈ P ( ˜ S, F , ˜ Q ) t -1 ,t ( ξ F ) such that

$$p ^ { \mathbb { G } } \geq \Xi + p ^ { \mathbb { F } } I _ { \{ \tau \geq t \} } \geq \Xi + \widehat { \mathcal { P } } ^ { ( \widetilde { S } , \mathbb { F } , \widetilde { Q } ) } _ { t - 1 , t } ( \xi ^ { \mathbb { F } } ) I _ { \{ \tau \geq t \} } , \quad P - a . s . . \\ \text {taking essential innum} \, i n the above inequality and combining the result} \\ \text {e.c.} \, \text {claim } ( 5 , 1 4 ) \text { follows immediately.} \, \text {This ends the second part.}$$

Therefore, by taking essential infimum in the above inequality and combining the resulting inequality with (5.15), the claim (5.14) follows immediately.This ends the second part.

Part 3. Hereto, we summarize the proof of the theorem. In fact, in order to prove assertion (a), we appeal to Theorem 4.1-(a). Then the proof of the first equality in (5.2) is a direct combination of the first equality in (4.3) and Part 1 applied to ( ξ G , Ξ , ξ F ) = ( g t I { τ&gt;t } , 0 , ̂ g t ). The second equality in (5.2) follows from combining the second equality in (4.3) and Part 2 applied to ( ξ G , Ξ , ξ F ) = ( g t I { τ&gt;t } , 0 , ̂ g t ). Similarly, the first (respectively the second) equality in (5.3) is a direct consequence of the first (respectively the second) equality in (4.4) and Part 1 (respectively Part 2) applied to ( ξ G , Ξ , ξ F ) = ( g t I { τ ≥ t } , 0 , g t ). Assertion (c) follows from combining (4.5) and Part 1 and 2 applied ( ξ G , Ξ , ξ F ) = ( K τ I { τ ≤ t } , K τ I { τ ≤ t -1 } , κ (0) t ). Finally, assertion (d) is direct consequence from combining (4.6) and Parts 1 and 2 applied to ( ξ G , Ξ , ξ F ) = ( g t I { τ&gt;t } + K τ I { τ ≤ t } , K τ I { τ ≤ t -1 } , κ ( g ) t ). This completes the proof of theorem.

## 5.2 The general pricing formulas and the prices' dynamics

Hereto, we fully describe the pricing formulas for the three kind of vulnerable claims, and afterwards we single out the various risks in their dynamics as well. To this end, throughout the rest of the paper, for any two processes X and Y , we denote by X · Y -the stochastic integral of X with respect to Y -, [ X,Y ], and 〈 X,Y 〉 -when it exists- the processes given by

$$& , \, \text {and} \, ( X , Y ) = \sum _ { s = 1 } ^ { t } X _ { s } \Delta Y _ { s } \div \sum _ { s = 1 } ^ { t } X _ { s } ( Y _ { s } - Y _ { s - 1 } ) , \quad ( X \cdot Y ) _ { 0 } = 0 , \\ & \quad [ X , Y ] \colon = \sum _ { s = 1 } ^ { t } \Delta X _ { s } \Delta Y _ { s } , \quad ( X , Y ) \colon = \sum _ { s = 1 } ^ { t } E \left [ \Delta X _ { s } \Delta Y _ { s } | \mathcal { F } _ { s - 1 } \right ] , \\ & \text {the convention } \sum _ { s = 0 } \emptyset \text { will be used throughout the paper. For further notation and definitions }$$

We recall the triplet ( m,N G , D o, F ) associated to τ , which plays central roles in quantifying the various risks generated by τ , as follows, where the convention ∑ ∅ = 0 will be used throughout the paper. For further notation and definitions about stochastic calculus in discrete-time, we refer the reader to [21] and [19].

$$m _ { t } \coloneqq & 1 + \sum _ { s = 1 } ^ { t } \left ( \widetilde { G } _ { s } - E [ \widetilde { G } _ { s } | \mathcal { F } _ { s - 1 } ] \right ) , \, N _ { t } ^ { G } \coloneqq I _ { \{ \tau \leq t \} } - \sum _ { s = 1 } ^ { t \wedge \tau } \frac { P ( \tau = s | \mathcal { F } _ { s } ) } { \widetilde { G } _ { s } } , \\ D _ { t } ^ { o , \mathbb { F } } \coloneqq & \sum _ { s = 0 } ^ { t } P ( \tau = s | \mathcal { F } _ { s } ) . \\ & 2 5$$

The process m is an F -martingale which is a BMO martingale, see [22] for more details about this fact. Herein, m quantifies the correlation risk resulting from the interaction of τ with the flow F , see [9, 11, 12, 13] for more about this. The process N G is a G -martingale which was introduced in [9] and called the main generator of pure-default-martingales of type 1. This process N G quantifies the main generator of the pure-default-risk borne in the random time, see [9, 10, 12, 13] for more details and related works and results. The process D o, F is an nondecreasing process F -adapted and is the F -optional dual projection of D := 1 ] ] τ, ∞ ] ] . We refer the reader to [10], for the role of D o, F in quantifying correlation risks resulting from the interplay between τ and the benefit policy in life insurance. The risks in G , which are mainly coming from F , are represented by a transform operator T , defined for any process M by

$$\mathcal { T } ( M ) \coloneqq \sum _ { u = 1 } ^ { \tau \wedge \cdots } \widehat { G } _ { \widetilde { G } _ { u } } ( M _ { u } - M _ { u - 1 } ) + \sum _ { u = 1 } ^ { \tau \wedge } E [ I _ { \widetilde { G } _ { u } = 0 } ( M _ { u } - M _ { u - 1 } ) | \mathcal { F } _ { u - 1 } ] . \\ \text {Herein, if } M \text { is an } \mathbb { F } \text {-martinga} \text {, the } \mathcal { T } ( M ) \text { is a } \mathbb { G } \text {-martinga} \text {. For more details about these and related } \\ \text {properties, we refer the readerto } [ 9 , \text {Theorem } 2 . 1 4 \text {] and the references therein. }$$

̂ by Definition 2.6, which we recall below

Herein, if M is an F -martingale, the T ( M ) is a G -martingale. For more details about these and related properties, we refer the reader to [9, Theorem 2.14] and the references therein. Throughout the remaining part of the paper, we use the one-period-pricing operator P ( ˜ S, ˜ Q ) t,t +1 ( · ), given

$$\widehat { \mathcal { P } } _ { t , t + 1 } ^ { ( \tilde { S } , \tilde { Q } ) } ( \cdot ) \coloneqq \underset { \theta \in L ^ { 0 } ( \mathcal { F } _ { t } ) } { \overset { \tilde { Q } } { \overset { \sim } { \infty } } } \tilde { Q } \sup ( \theta \Delta \widetilde { S } _ { t + 1 } + \cdot ) , \\ \intertext { a n d f o l l o w i n g n o t a n t i o n a l h e r v i a t i o n }$$

and we will use the following notational abreviation

$$\overset { \i C R i s k ( Y _ { 1 } , Y _ { 2 } ) } { = } & \text {Correlation Risk from } ( Y _ { 1 } , Y _ { 2 } ) , \\ \text {and } \overset { \i C P F R i s k } { = } & \text {Pure Financial Risk} .$$

Furthermore, for any recovery process R , we introduce an important functional f R ( t, x ) = f R ( t, ω, x ), which is B (I R) ⊗O ( F )-measurable, and is given by

$$f _ { R } ( t , x ) \coloneqq x I _ { \{ \tilde { G } _ { t } = G _ { t } > 0 \} } + R _ { t } I _ { \{ \tilde { G } _ { t } > G _ { t } = 0 \} } + \max ( x , R _ { t } ) I _ { \{ \tilde { G } _ { t } > G _ { t } > 0 \} } ,$$

for any ( t, x ) ∈ { 0 , ..., T, } × I R.

Theorem 5.6. Suppose ( ˜ S, F , ˜ Q ) fulfills AIP. Consider g T ∈ L 1 ( F T ) and the pair of vulnerable claim and its associated F -claim ( ξ ( G , 1) , ξ ( F , 1) ) which belongs to { ( g T I { τ&gt;T } , g T I { G T &gt; 0 } ) , ( g T I { τ ≥ T } , g T I { ˜ G T &gt; 0 } ) } . Let ̂ P ( G , 1) be the super-hedging price process for ξ ( G , 1) under ( S τ , G , P ) , and f 0 be the functional defined in (5.21) for the zero-recovery. Then the following assertions hold. (a) The price process ̂ P ( G , 1) is given by P ( G , 1) = P ( F , 1) I ] ]0 ,τ ] ] and

$$1 ) \text { for the zero-recovery. } \text { Then the following asserts hold.} \\ \text {process } \widehat { \mathcal { P } } ^ { ( G , 1 ) } \text { is given } b \, \widehat { \mathcal { Y } } ^ { ( G , 1 ) } = \widehat { \mathcal { P } } ^ { ( F , 1 ) } I _ { ] 0 , \tau ] \text { } a n d \\ \widehat { \mathcal { P } } _ { t } ^ { ( \widehat { S } , \widehat { Q } ) } = \begin{cases} \widehat { \mathcal { P } } _ { t + 1 } ^ { ( \widehat { S } , \widehat { Q } ) } \left ( f _ { 0 } ( t + 1 , \widehat { \mathcal { P } } _ { t + 1 } ^ { ( F , 1 ) } ) \right ) , \text { if } t < T , \\ \widehat { \mathcal { P } } _ { t } ^ { ( F , 1 ) } \end{cases} \\$$

(b) The price process P ( G , 1) can be decomposed into

Here G -:= G ·-1 and the quadruplet ( M (1) , N (1) , N (1) , ˜ V (1) ) is given by

$$0 \, \text { The price process } \widehat { \mathcal { P } } ( G , 1 ) \ c a n \, b e \, \text { decomposed into} \\ \widehat { \mathcal { P } } ^ { ( G , 1 ) } = \underbrace { \widehat { \mathcal { P } } _ { 0 } ^ { ( F , 1 ) } I _ { \{ \tau > 0 \} } + \sum _ { s = 1 } ^ { \tau } E \left [ \frac { G _ { s } } { G _ { s - 1 } } \widehat { \mathcal { P } } _ { s } ^ { ( F , 1 ) } - \widehat { \mathcal { P } } _ { s - 1 } ^ { ( F , 1 ) } | \mathcal { F } _ { s - 1 } \right ] } _ { S u r k i s k } + \underbrace { \mathcal { T } ( M ^ { ( 1 ) } ) } _ { P F R i s k } \\ \text { } - \underbrace { \sum _ { \substack { \mathcal { P } ( F ) \colon \widetilde { N } ^ { \prime } \colon \widetilde { G } ^ { - 1 } \colon \widetilde { F } ^ { ( \tau ) } \colon \widetilde { N } ^ { ( 1 ) } } } _ { P u r e D e f a u t - R i s k } \, \text { } \quad \text { } \widehat { \mathcal { C } } _ { R i s k ( \tau , \widetilde { b e n f i t } \, \text { policy} ) } \\ + \underbrace { G _ { s - 1 } ^ { - 1 } \cdot \mathcal { T } ( \overline { N } ^ { ( 1 ) } ) - G _ { s } ^ { - 2 } ( \widetilde { \mathcal { V } } ^ { ( 1 ) } + \Delta \langle M ^ { ( 1 ) } , m \rangle ) \cdot \mathcal { T } ( m ) } _ { C R R i s k ( \tau , \mathbb { F } ) } \\ \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text { } \quad \text$$

$$H e r \ G _ { - } \coloneqq & \ G _ { - 1 } \text { and the qua\ddupruite } \left ( M ^ { ( 1 ) } , N ^ { ( 1 ) } , \overline { N } ^ { ( 1 ) } , V ^ { ( 1 ) } \right ) \text { is given by } \\ & \widetilde { V } ^ { ( 1 ) } \coloneqq \sum _ { s = 1 } ^ { \cdot } E \left [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { R } , 1 ) } ( \widetilde { G } _ { s } - G _ { s } ) | \mathcal { F } _ { s - 1 } \right ] , \quad N ^ { ( 1 ) } \coloneqq \widehat { \mathcal { P } } ^ { ( \mathbb { R } , 1 ) } \cdot D ^ { o , \mathbb { F } } - \widetilde { V } ^ { ( 1 ) } , \\ & \overline { N } ^ { ( 1 ) } \coloneqq \sum _ { s = 1 } ^ { \cdot } \left ( \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { R } , 1 ) } \widetilde { G } _ { s } - E [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { R } , 1 ) } \widetilde { G } _ { s } | \mathcal { F } _ { s - 1 } ] \right ) - G _ { \cdot } \cdot M ^ { ( 1 ) } - \xi ^ { ( 1 ) } \cdot m , \\ & M ^ { ( 1 ) } \coloneqq \sum _ { s = 1 } ^ { \cdot } \left ( \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { R } , 1 ) } - E [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { R } , 1 ) } | \mathcal { F } _ { s - 1 } ] \right ) , \quad \xi ^ { ( 1 ) } \coloneqq E \left [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { R } , 1 ) } | \mathcal { F } _ { s - 1 } \right ] . \\ \intertext { t h e r o w h a c l e s t h a t h e s u p e r c r o w s c r f o w } \text { which does not involve paymee n t after the occurrence of } \tau , \text { is given by the super-hedging price process }$$

The theorem claims that the super-hedging price process of the vulnerable claim with payoff ξ ( G , 1) , which does not involve payment after the occurrence of τ , is given by the super-hedging price process under the model ( ˜ S, F , ˜ Q ) for a corresponding F -claim The theorem presents two cases of this class of vulnerable claims depending whether there is payment at τ or not. These two cases differ slightly in the vulnerable claim's payoff and hence in its corresponding F -claim and subsequently in the superhedging price process of this latter claim. However, both G -super-hedging prices of these two cases have the same structures before τ , and the same risk decomposition structure. The decomposition (5.23) of the dynamics' of the price process of the vulnerable claim is important in the securitization and/or hedging as it singles out the various risks and their origins.

Below, we discuss some particular cases of the pair ( τ, F ) which are frequently addressed in the credit risk and life insurance literatures.

Remark 5.7. (a) The case of immersion, which is equivalent to the martingale m being constant (i.e. m t = m 0 P -a.s. for any t ∈ { 1 , ..., T } ). If immersion holds, then

$$t = m _ { 0 } \ P { - a . s . } \ \text { for any } t \in \{ 1 , \dots , T \} . \ \text { If immersion holds, then} \\ \widehat { \mathcal { P } } ^ { ( G , 1 ) } = \underbrace { \widetilde { \mathcal { P } } _ { 0 } ^ { ( 1 ) } J _ { \{ \tau > 0 \} } + \sum _ { s = 1 } ^ { \tau \wedge } E \left [ \frac { G _ { s } } { G _ { s - 1 } } \widehat { \mathcal { P } } _ { s } ^ { ( F , 1 ) } - \widehat { \mathcal { P } } _ { s - 1 } ^ { ( \mathbb { R } , 1 ) } | \mathcal { F } _ { s - 1 } \right ] } _ { \quad s u p e r - h e d g i n g - p r i c e \, ^ { \, s \ t r e n d } } + \underbrace { \mathcal { T } ( M ^ { ( 1 ) } ) } _ { \quad - \quad S ( F , 1 ) } . \\ \quad - \underbrace { \mathcal { P } ^ { ( F , 1 ) } \cdot N ^ { G } } _ { \quad P r i c e - D e f a u t - R i s k } - \underbrace { \ G _ { - } ^ { - 1 } \cdot \mathcal { T } ( N ^ { ( 1 ) } ) } _ { \quad C R i s k ( \tau , b e n e f i t \, p o l i c y ) } . \\ \text {The case of independent between } \tau \text { and } \mathbb { F } . \ \text { This is a particular case of immersion, and f u r t h e r m o r e }$$

- (b) The case of independence between τ and F . This is a particular case of immersion, and furthermore

in this case we have

The statements in the remark follows directly from Theorem 5.6, and hence their proofs will be omitted herein. The second main result of this subsection deals with vulnerable claims that involve payment at τ , and these claims take two forms depending whether there is payment before the occurrence of τ or not. In life insurance, these vulnerable claims are known as policies where the beneficiaries receive payments at the moment of death, and for which there are two types of policies depending whether there is benefit upon survival or not. Below, we start with the case where there is no benefit upon survival.

$$\text {case we have} \\ \widehat { \mathcal { P } } ^ { ( G , 1 ) } & = \underbrace { \widehat { \mathcal { P } } _ { 0 } ^ { ( R , 1 ) } I _ { \{ \tau > 0 \} } + \sum _ { s = 1 } ^ { \tau \wedge } \left ( \frac { G _ { s } } { G _ { s - 1 } } E \left [ \widehat { \mathcal { P } } _ { s } ^ { ( R , 1 ) } | \mathcal { F } _ { s - 1 } \right ] - \widehat { \mathcal { P } } _ { s - 1 } ^ { ( R , 1 ) } \right ) } \\ & + \underbrace { G } _ { - \widehat { G } _ { - } } \cdot \mathcal { T } ( M ^ { ( 1 ) } ) - \underbrace { \widehat { \mathcal { P } } _ { ( R , 1 ) } ^ { ( F , 1 ) } \cdot N ^ { G } } _ { P u r e - D e f a u l t - R i s k } \cdot \\ \text {statements in the remark follows directly from Theorem 5.6, and hence their proofs will be omitted} \\ . . . \, The second main result of this subsection deals with vulnerable claims that involve payment$$

Theorem 5.8. Let the recovery process K be an F -adapted and integrable process, f K given by (5.21), ξ G , 2 = K τ I { τ ≤ T } be the vulnerable claim's payoff, and ̂ P ( ˜ S, ˜ Q ) t,t +1 ( · ) be the one-period-pricing operator given by (5.19). Suppose that ( ˜ S, F , ˜ Q ) fulfills AIP (equivalently ( S τ , G ) fulfills AIP ) and denote by ̂ P ( G , 2) the super-hedging price process for the claim ξ G , 2 . Then the following assertions hold. (a) The process ̂ P ( G , 2) is given by ̂ P ( G , 2) = K τ I ] ] τ, ∞ ] ] + ̂ P ( F , 2) I ] ]0 ,τ ] ] and

(b) The dynamics of P ( G , 2) can be decomposed

$$\widehat { \mathcal { P } } _ { t } ^ { ( E , 2 ) } = \begin{cases} \widehat { \mathcal { P } } _ { t , t + 1 } ^ { ( E , 2 ) } f _ { t , t + 1 } ^ { ( E , 2 ) } ( f _ { K } ( t + 1 , \widehat { \mathcal { P } } _ { t + 1 } ^ { ( E ) } ) ) , & \text {if } t = T \\ 0 & \text {if } t = T \\ \end{cases} . \\ \widehat { \mathcal { P } } _ { t } ^ { ( G , 2 ) } = & K _ { 0 } I _ { \{ \tau = 0 \} } + \widehat { \mathcal { P } } _ { 0 } ^ { ( R , 2 ) } I _ { \{ \tau > 0 \} } + \underbrace { \mathcal { C } \frac { ( T _ { \ } m ( 2 ) } ^ { ( R ) } } { \overline { P F R i s k } } + \underbrace { ( K _ { \ } m ( \mathcal { P } ^ { ( R , 2 ) } ) \cdot N ^ { G } } } _ { \sim } } _ { \sim } \\ & + \underbrace { \underline { G _ { - } ^ { - 1 } \cdot \mathcal { T } ( N ^ { ( 2 ) } ) } } _ { \sim } \\ & + \underbrace { \underline { G _ { R i s k , \tau } ( \widetilde { r } , b e n e f i t p o l i c y ) } } _ { \sim } \\ & + \underbrace { \underline { G _ { - } ^ { - 1 } \cdot \mathcal { T } ( \overline { N } ^ { ( 2 ) } ) - G _ { - } ^ { - 2 } ( \Delta \widetilde { V } ^ { ( 2 ) } + \Delta \langle M ^ { ( 2 ) } , m \rangle ) \cdot \mathcal { T } ( m ) } } _ { \sim } } _ { \sim } \\ & + \underbrace { \mathcal { C } R r i s k ( \tau , \mathbb { R } ^ { ( \tau , \tau ) } ) } _ { \sim } \\ & + \underbrace { \sum _ { s = 1 } ^ { \sim } E } _ { \sim } \left [ K _ { s } \frac { s } { \overline { G _ { s - 1 } } } - G _ { s } \right ] \widehat { \mathcal { P } } _ { s - 1 } ^ { ( E , 2 ) } \frac { G _ { s } } { \overline { G _ { s - 1 } } } - \widehat { \mathcal { P } } _ { s - 1 } ^ { ( E , 2 ) } | \mathcal { F } _ { s - 1 } ^ { - 1 } \right ] .$$

$$\widehat { \mathcal { P } } _ { t } ^ { ( \mathbb { F } , 2 ) } = \begin{cases} \widehat { \mathcal { P } } _ { t } ^ { ( \widehat { S } , \widehat { Q } ) } \left ( f _ { K } ( t + 1 , \widehat { \mathcal { P } } _ { t + 1 } ^ { ( \mathbb { F } , 2 ) } ) , & i f \ \ t < T \\ 0 & i f \ \ t = T \end{cases} \quad .$$

Here, the quadruplet ( M (2) , A (2) , N (2) , V (2) ) is given by

Below, we treat the case where there are both benefits, upon survival and at the moment of death τ .

$$\text {ere, the quadruplet (M^{(2)} , A^{(2)} , N^{(2)} , \widetilde{V}^{(2)} ) is given by \\ N ^ { ( 2 ) } \colon = ( K - \widehat { \mathcal { P } } ( \mathbb { F } ^ { 2 } ) \cdot D ^ { \mathbb { F } } - \widetilde { V } ^ { ( 2 ) } , \\ \widetilde { V } ^ { ( 2 ) } \colon = \sum _ { s = 1 } ^ { \cdot } \Delta \widetilde { V } _ { s } ^ { ( 2 ) } \colon = \sum _ { s = 1 } ^ { \cdot } E \left [ ( K _ { s } - \widehat { \mathcal { P } } _ { s } ^ { ( 2 ) } ) ( \widetilde { G } _ { s } - G _ { s } ) | \mathcal { F } _ { s - 1 } \right ] \\ \overline { N } ^ { ( 2 ) } \colon = \sum _ { s = 1 } ^ { \cdot } \left ( \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { F } , 2 ) } \widetilde { G } _ { s } - E [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { F } , 2 ) } \widetilde { G } _ { s } | \mathcal { F } _ { s - 1 } \right ) \right ) - G _ { - } \cdot M ^ { ( 2 ) } - \xi ^ { ( 2 ) } \cdot m , \\ M _ { t } ^ { ( 2 ) } \colon = \sum _ { s = 1 } ^ { t } \left ( \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { F } , 2 ) } - E [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { F } , 2 ) } | \mathcal { F } _ { s - 1 } ] \right ) , \quad \xi _ { t } ^ { ( 2 ) } \colon = E \left [ \widehat { \mathcal { P } } _ { t } ^ { ( \mathbb { F } , 2 ) } | \mathcal { F } _ { t - 1 } \right ] . \\ \text {below, we treat the case where there are both benefits, uponSurvival and at the moment of death $\tau$} .$$

Theorem 5.9. Let g T ∈ L 1 ( F T ) , the recovery process K be integrable and F -adapted, f K be the functional given by (5.21), ξ ( G , 3) := g T I { τ&gt;T } + K τ I { τ ≤ T } be the vulnerable claim's payoff. If ( ˜ S, F , ˜ Q ) fulfills AIP and ̂ P ( G , 3) denotes the super-hedging price for ξ ( G , 3) , then the following assertions hold. (a) The process ̂ P ( G , 3) is given by ̂ P ( G , 3) = K τ I ] ] τ, ∞ ] ] + ̂ P ( F , 3) I ] ]0 ,τ ] ] and

$$( b ) \, \text { The dynamics of } \widehat { \mathcal { P } } ( \mathcal { G } , 3 ) \, \text { can be decomposed } \\ \widehat { \mathcal { P } } ( \mathcal { G } , 3 ) = & \, K _ { 0 } I _ { \{ r = 0 \} } + \widehat { \mathcal { P } } _ { 0 } ^ { ( \mathcal { F } , 3 ) } I _ { \{ r > 0 \} } + \underbrace { ( \mathcal { K } - \widehat { \mathcal { P } } ( \mathcal { F } , 3 ) \cdot N ^ { G } } _ { \text {Pure-Delaunot-Risk} } + \underbrace { \mathcal { T } ( \mathcal { M } ^ { ( 3 ) } ) } _ { \text {PFRisk} } \\ & + \underbrace { \frac { 1 } { G _ { - } } \cdot \mathcal { T } ( N ^ { ( 3 ) } ) } _ { \text {C-Risk} } + \underbrace { \frac { 1 } { G _ { - } } \cdot \mathcal { T } ( \overline { N } ^ { ( 3 ) } ) } _ { \text {G-2} } - \frac { ( \Delta \widetilde { V } ^ { ( 3 ) } + \Delta \langle M ^ { ( 3 ) } , m \rangle ) } { G _ { - } ^ { 2 } } \cdot \mathcal { T } ( \mathcal { N } ^ { n } ) \\ & + \underbrace { \hat { \mathcal { T } } } _ { \text {E} } \left [ E \left [ K _ { \frac { S } { G _ { - } } } - G _ { s } \right ] + \widehat { \mathcal { F } } _ { s } ^ { ( \mathcal { F } , 3 ) } \frac { G _ { s } } { G _ { - 1 } } - \widehat { \mathcal { P } } _ { s - 1 } ^ { ( \mathcal { F } , 3 ) } | \mathcal { F } _ { s - 1 } \right ] } _ { \text {super-hedging price's truedend} } \\ \text {Here } ( M ^ { ( 3 ) } , N ^ { ( 3 ) } , \widetilde { V } ^ { ( 3 ) } ) \text { are given by } \\ & N ^ { ( 3 ) } \colon = ( K - \widehat { \mathcal { P } } ( \mathbf F , 3 ) \cdot D ^ { \mathbb { F } } - \widetilde { V } ^ { ( 3 ) } ) \, .$$

$$\ e s s \, \mathbb { P } ( \mathbb { G } _ { 3 } ) \, \text { is given by } \beta ( \mathbb { G } _ { 3 } ) & = K _ { \tau _ { 1 } , \tau _ { 2 } } + \beta ( \mathbb { F } _ { 3 } ) _ { T _ { 0 , 1 } , \tau _ { 1 } } \, \text { and } \\ \widehat { \mathcal { P } } _ { t } ( \mathbb { F } _ { 3 } ^ { ( \mathbb { F } _ { t } , \widehat { Q } ) } ( f _ { K } ( t + 1 , \widehat { \mathcal { P } } _ { t + 1 } ^ { ( \mathbb { F } _ { 3 } , 3 ) } ) , \text { if } t < T , \\ \widehat { \mathcal { P } } _ { t } ^ { ( \mathbb { F } _ { t } , \widehat { G } _ { T > 0 } ) } , & \quad \text {if } t = T . \\ \intertext { a n c i s  o f } \widehat { \mathcal { P } } ( \mathbb { G } _ { 3 } ) \, \text { can be decomposed }$$

$$s u p e r - h e d g i n g \ p r i c { s } { \ t r e n } { \ } \\ H e r e \left ( M ^ { ( 3 ) } , N ^ { ( 3 ) } , \overline { N } ^ { ( 3 ) } , \overline { V } ^ { ( 3 ) } \right ) \ a r e \ g i v e { n } { \ } b y \\ N ^ { ( 3 ) } \colon = ( K - \widehat { \mathcal { P } } ^ { ( F , 3 ) } ) \cdot D ^ { \mathbb { F } } \, \widetilde { V } ^ { ( 3 ) } \, , \\ \widetilde { V } ^ { ( 3 ) } \colon = \sum _ { s = 1 } ^ { \ } \Delta \widetilde { V } _ { s } ^ { ( 3 ) } \colon = \sum _ { s = 1 } ^ { \ } E \left [ ( K _ { s } - \widehat { \mathcal { P } } _ { s } ^ { ( F , 3 ) } ) ( \widetilde { G } _ { s } - G _ { s } ) | \mathcal { F } _ { s - 1 } \right ] \\ \overline { N } ^ { ( 3 ) } \colon = \sum _ { s = 1 } ^ { \ } \left ( \widehat { \mathcal { P } } _ { s } ^ { ( F , 3 ) } \widetilde { G } _ { s } - E [ \widehat { \mathcal { P } } _ { s } ^ { ( F , 3 ) } \widetilde { G } _ { s } | \mathcal { F } _ { s - 1 } ] \right ) - G _ { \cdot } \cdot M ^ { ( 3 ) } - \xi ^ { ( 3 ) } \cdot m , \\ M ^ { ( 3 ) } _ { \cdot } \colon = \sum _ { s = 1 } ^ { \ } \t t \left ( \widehat { \mathcal { P } } _ { s } ^ { ( F , 3 ) } - E [ \widehat { \mathcal { P } } _ { s } ^ { ( F , 3 ) } | \mathcal { F } _ { s - 1 } ] \right ) , \quad \xi _ { s } ^ { ( 3 ) } \colon = E \left [ \widehat { \mathcal { P } } _ { s } ^ { ( F , 3 ) } | \mathcal { F } _ { s - 1 } \right ] ,$$

$$M _ { t } ^ { ( 3 ) } \coloneqq \sum _ { s = 1 } ^ { t } \left ( \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { F } , 3 ) } - E [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { F } , 3 ) } | \mathcal { F } _ { s - 1 } ] \right ) , \quad \xi _ { s } ^ { ( 3 ) } \coloneqq E \left [ \widehat { \mathcal { P } } _ { s } ^ { ( \mathbb { F } , 3 ) } | \mathcal { F } _ { s - 1 } ] \, ,$$

We end this subsection by illustrating the main results of Theorems 5.6, 5.8 and 5.9 on the case of vulnerable options with and/or without recovery.

Theorem 5.10. Suppose that g T ∈ L 0 + ( F T ) , and the recovery process K be nonnegative integrable and F -adapted. Let ξ ( F , 1) ∈ { g T I { G T &gt; 0 } , g T I { ˜ G T &gt; 0 } } and put K := KI { ˜ G&gt;G } . Then the three processes ̂ P ( F ,i ) , i = 1 , 2 , 3 , given in Theorems 5.6, 5.8 and 5.9 respectively, satisfy the following

Proof. For any nonnegative process R and any x ∈ I R + , consider f R ( t, x ) defined in (5.21) and derive

$$i , i = 1 , 2 , 3 , \text { given in Theorems } 5 . 6 , \, 5 . 8 \, \text { and } 5 . 9 \, \text { respectively, } \text { satisfy the following} \\ \widehat { \mathcal { P } } _ { t } ^ { ( \mathbb { F } , 1 ) } = \begin{cases} \widehat { \mathcal { P } } _ { t } ^ { ( \tilde { S } , \tilde { Q } ) } \left ( \widehat { \mathcal { F } } _ { t + 1 } ^ { ( F , 1 ) } \right ) , \, \text { if } t \leq T - 1 , \\ \xi ^ { ( F , 1 ) } , \quad \text { if } t = T \\ \widehat { \mathcal { P } } _ { t } ^ { ( \tilde { F } , \tilde { S } ) } \left ( \max ( \widehat { \mathcal { F } } _ { t + 1 } ^ { ( F , 2 ) } , \overline { \mathcal { K } } _ { t + 1 } ) ) , \, \text { if } t \leq T - 1 , \\ \widehat { \mathcal { P } } _ { t } ^ { ( \tilde { F } , \tilde { R } ) } = 0 , \end{cases} \quad , \\ \text { and } \quad \widehat { \mathcal { P } } _ { t } ^ { ( \mathbb { F } , 3 ) } = \begin{cases} \widehat { \mathcal { P } } _ { t } ^ { ( \tilde { S } , \tilde { Q } ) } ( \max ( \widehat { \mathcal { P } } _ { t + 1 } ^ { ( F , 3 ) } , \overline { \mathcal { K } } _ { t + 1 } ) ) , \, \text { if } t \leq T - 1 , \\ \widehat { \mathcal { P } } _ { T } ^ { ( F , 3 ) } = g _ { T } I _ { G _ { T } > 0 } \end{cases} . \quad \text { if } t = T \\ \text { for any nonnegative process } R \text { and any } x \in \mathbb { R } ^ { + } , \text { consider } f _ { R } ( t , x ) \text { defined in } ( 5 . 2 1 ) \text { and derive} \\ \quad f _ { R } ( t , x ) \\ \colon = x I _ { \tilde { G } } \sim g _ { 0 } \colon \rangle + R _ { t } I _ { \tilde { G } } \sim g _ { 0 } = 0 \rangle + \max ( x , R _ { t } ) I _ { \tilde { G } } \sim g _ { \tilde { G } } \sim 0 \rangle$$

$$f _ { R } ( t , x ) & & f _ { R } ( t , x ) & & \\ & \vdots & x I _ { \{ \tilde { G } _ { t } = G _ { t } > 0 \} } + R _ { t } I _ { \{ \tilde { G } _ { t } > G _ { t } = 0 \} } + \max ( x , I _ { t } ) I _ { \{ \tilde { G } _ { t } > G _ { t } > 0 \} } \\ & = & \max ( x I _ { \{ G _ { t } > 0 \} } , R _ { t } I _ { \{ \tilde { G } _ { t } > G _ { t } > 0 \} } ) \left ( I _ { \{ \tilde { G } _ { t } = G _ { t } > 0 \} } + I _ { \{ \tilde { G } _ { t } > G _ { t } = 0 \} } + I _ { \{ \tilde { G } _ { t } > G _ { t } > 0 \} } \right ) \\ & = & \max ( x I _ { \{ G _ { t } > 0 \} } , R _ { t } I _ { \{ \tilde { G } _ { t } > G _ { t } \} } ) . \\ \intertext { T h o r f o r a \, \ the \, \text {proof of the } \, \ t h o r \, o u a l i t i o n s \, \ in \, \ ( 5 . 3 3 ) \, \ f o l l w o w \, i m m o d i a t i o l y \, \text { from } \, \text { combining this } \, \text { last }$$

Therefore, the proof of the three equalities in (5.33) follow immediately from combining this last equality in (5.34), and assertion (a) for Theorems 5.6, 5.8 and 5.9 respectively. This ends the proof of the theorem.

Remark 5.11. Under the assumptions of Theorem 5.10, the following hold

## 5.3 Proofs of Theorems 5.6, 5.8 and 5.9

$$\text {mark} \, 5 . 1 . \quad \text {under the assumption of} \, \text {the year} \, 5 . 1 0 , \, \text {the following is hold} \\ \widehat { \mathcal { P } } ^ { ( G , 3 ) } \geq \max \left ( \widehat { \mathcal { P } } ^ { ( G , 1 ) } , \widehat { \mathcal { P } } ^ { ( G , 2 ) } \right ) \quad a n d \quad \widehat { \mathcal { P } } ^ { ( \mathbb { F } , 3 ) } \geq \max \left ( \widehat { \mathcal { P } } ^ { ( \mathbb { F } , 1 ) } , \widehat { \mathcal { P } } ^ { ( \mathbb { F } , 2 ) } \right ) . \\ \intertext { B o r f o s $ \text {of Theorems} $ 5 . 6 , 5 . 8 $ and $ 5 . 9 $}$$

These proofs rely on the following three technical lemmas, which are interesting in themselves.

Lemma 5.12. Suppose that ( ˜ S, F , ˜ Q ) satisfies the AIP condition, and let ̂ P ( F , 3) Then

The proof of this lemma will be omitted herein.

be defined in (5.30).

$$\widehat { \mathcal { P } } ^ { ( \mathbb { F } , 3 ) } \geq 0 , \quad a n d \quad \widehat { \mathcal { P } } ^ { ( \mathbb { F } , 3 ) } I _ { \{ G = 0 \} } \equiv 0 . \\ \intertext { s u m a w l l b e o t i m t e d h e r e i n . } \quad \ a n d \quad \widehat { \mathcal { F } } _ { \ } m a r t i n g a l e \ M _ { \ } a n d \ \widehat { \mathbb { F } } _ { \ } n r e d i c t a b l e \ n o r c h e s s \ V \quad w e h a v e .$$

Lemma 5.13. For any F -martingale M and F -predictable process V , we have and

$$G _ { - } \tilde { G } ^ { - 1 } \cdot V ^ { \tau } = & ^ { p , \mathbb { F } } ( I _ { \{ \tilde { G } > 0 \} } ) \cdot V ^ { \tau } - G _ { - } ^ { - 1 } \Delta V \cdot \mathcal { T } ( m ) , \\ \\ M ^ { \tau } = & \mathcal { T } ( M ) + G _ { - } ^ { - 1 } \mathcal { T } ( [ M _ { \ } m ] - / M _ { \ } m )$$

$$M ^ { \tau } = & \mathcal { T } ( M ) + G _ { - } ^ { - 1 } \cdot \mathcal { T } ( [ M , m ] - \langle M , m \rangle ) \\ & - G _ { - } ^ { - 2 } \Delta \langle M , m \rangle \cdot \mathcal { T } ( m ) + G _ { - } ^ { - 1 } \cdot \langle M , m \rangle ^ { \tau } .$$

The proof of this lemma will be omitted herein.

Lemma 5.14. Let ( X, H , Q ) be a model defined on the probability space (Ω , G , P ) , and satisfying AIP. Consider H ∈ L 0 ( H T ) and a functional f ( t, ω, x ) such that for x ∈ R , ( f ( t, x )) t =0 ,...,T is H -adapted. Then the following backward stochastic equation has a unique solution.

$$Y _ { t } = \widehat { \mathcal { P } } _ { t , t + 1 } ^ { ( X , \mathbb { H } , Q ) } ( f ( t + 1 , Y _ { t + 1 } ) ) , \quad t \leq T - 1 , \quad Y _ { T } = H , \\ \intertext { u r e s o l u t i o n . } \widehat { Y } _ { t } = \widehat { \mathcal { P } } _ { t , t + 1 } ^ { ( X , \mathbb { H } , Q ) } ( f ( t + 1 , Y _ { t + 1 } ) ) , \quad t \leq T - 1 , \quad Y _ { T } = H ,$$

The proof of this lemma is immediate and will be omitted herein. The rest of this subsection is devoted to the proof of the three theorems. To this end, and for the sake of simplifying the notation, throughout the proofs we put

˜ P (1) := ̂ P ( F , 1) , ˜ P (2) := ̂ P ( F , 2) , ˜ P (3) := ̂ P ( F , 3) . Proof. of Theorems 5.6, 5.8 and 5.9: Throughout this proof, we assume that ( ˜ S, F , ˜ Q ) fulfills AIP and consider g T ∈ L 1 ( F T ) and an F -adapted and integrable process K . The rest of this proof is divided into three parts, where we prove Theorems 5.6, 5.8 and 5.9 respectively.

Part 1. This part addresses Theorem 5.6, and proves it using Theorem 5.9.

1) Suppose that ( ξ ( G , 1) , ξ ( F , 1) ) = ( g T I { τ&gt;T } , g T I { G T &gt; 0 } ), and put K ≡ 0 in the second equality of (5.30). Then we deduce that

$$\tilde { \xi } ^ { ( \mathcal { G } , 3 ) } & = \xi ^ { ( \mathcal { G } , 1 ) } \, \text {and} \, \tilde { \mathcal { P } } _ { t } ^ { ( 3 ) } = \begin{cases} \widehat { \mathcal { P } } _ { t + 1 } ^ { ( \widetilde { \mathcal { S } } , \widetilde { Q } ) } ( f _ { 0 } ( t + 1 , \widehat { \mathcal { P } } _ { t + 1 } ^ { ( 3 ) } ) ) , \ t \leq T - 1 , \\ \widehat { \mathcal { P } } _ { T } ^ { ( 3 ) } = g _ { T } I _ { \{ G _ { T } > 0 \} } . \end{cases} \\ \text {since, by combining this with $\emph{Lemma 5.14},$ we conclude that $\tilde { \mathcal { P } } ^ { ( 3 ) } = \widetilde { \mathcal { P } } ^ { ( 1 ) } , \text { and hence $\mathcal{ } \widehat{ \mathcal{ } P } ( \mathcal { G } , 3 ) = \widehat { \mathcal{ } P } ( \mathcal { G } , 1 ) }$$

- we derive the following

˜ Hence, by combining this with Lemma 5.14, we conclude that ˜ P (3) = ˜ P (1) , and hence ̂ P ( G , 3) = ̂ P ( G , 1) follows immediately as well. This proves assertion (a) for the pair of claims ( g T I { τ&gt;T } , g T I { G T &gt; 0 } ). 2) Suppose ( ξ ( G , 1) , ξ ( F , 1) ) = ( g T I { τ ≥ T } , g T I { ˜ G T &gt; 0 } ). By combining Lemma 5.1 and Theorem 5.2-(b),

$$\widehat { \mathcal { P } } _ { T - 1 } ^ { ( \mathbb { G } , 3 ) } & = \widehat { \mathcal { P } } _ { T - 1 , T } ^ { ( S ; \mathbb { G } ) } ( \xi ^ { ( \mathbb { G } , 1 ) } ) = \widehat { \mathcal { P } } _ { T - 1 , T } ^ { ( \widetilde { S } , \widetilde { Q } ) } ( g _ { T } I _ { \{ G _ { T } > 0 \} } ) I _ { \{ \tau \geq T \} } . \\ \intertext { i r t u e o f ( \tau > T - 1 ) = ( \tau \geq T ) \text { and } \widetilde { \mathcal { P } } _ { T } ^ { ( 1 ) } = g _ { T } I _ { \{ \widetilde { G } _ { T } > 0 \} } , \text { we obtain }$$

̂ P ( G , 3) T -1 = ̂ P ( ˜ S, F , ˜ Q ) T -1 ,T ( g T ) I { τ ≥ T } = ̂ P ( ˜ S, F , ˜ Q ) T -1 ,T ( ˜ P (1) T ) I { τ&gt;T -1 } = ˜ P (1) T -1 I { τ&gt;T -1 } . (5.42) Thus, on the one hand, this latter equality proves (5.22) for t = T -1. On the other hand, as we stated in Remark 5.3-(a), this equality tells us that after this one-step we fall into the setting of the first claim, i.e. the case of g t I { τ&gt;t } with t = T -1 and g T -1 = ˜ P (1) T -1 instead. Thus, thanks to step 1 above, we deduce that (5.22) holds for any t ∈ { 0 , ..., T -1 } , and the proof of assertion for the pair of claims ( g T I { τ ≥ T } , g T I { ˜ G T &gt; 0 } ) is complete.

Then, in virtue of ( τ &gt; T -1) = ( τ ≥ T ) and ˜ P (1) T = g T I { ˜ G T &gt; 0 } , we obtain

3) Thanks to steps 1 and 2 above, we deduce that when K ≡ 0 we get ( ̂ P ( G , 3) , ˜ P (3) ) = ( ̂ P ( G , 1) , ˜ P (1) ). Then by combining this with Theorem 5.9-(b), where we put K = 0, we conclude that the quadruplets ( M (3) , N (3) , N (3) , ˜ V (3) ) and ( M (1) , N (1) , N (1) , ˜ V (1) ) coincide, and hence assertion (b) follows immediately. This ends the proof of Theorem 5.6.

Part 2. Hereto, we prove Theorem 5.8 using again Theorem 5.9. In fact, in this case we put g T = 0 in Theorem 5.9, and get

$$\xi ^ { ( \mathbb { G } , 3 ) } = \xi ^ { ( \mathbb { G } , 2 ) } , \quad \text {and} \quad \widetilde { \mathcal { P } } _ { t } ^ { ( 3 ) } = \widehat { \mathcal { P } } _ { t , t + 1 } ^ { ( \widetilde { S } , \widetilde { Q } ) } ( f _ { I } ( K + 1 , \widetilde { \mathcal { P } } _ { t + 1 } ^ { ( 3 ) } ) ) , \quad t \leq T - 1 , \quad \widetilde { \mathcal { P } } _ { T } ^ { ( 3 ) } = 0 .$$

Hence, by combining these with Lemma 5.14, we obtain ˜ P (3) = ˜ P (2) and ̂ P ( G , 3) = ̂ P ( G , 2) . Therefore, the proof of Theorem 5.8-(a) follows immediately, and both quadruplets ( M (3) , N (3) , N (3) , ˜ V (3) ) and ( M (2) , N (2) , N (2) , ˜ V (2) ) coincide. This yields assertion (b) of Theorem 5.8 and completes its proof. Part 3. This part proves Theorem 5.9. To this end, we start proving (5.30). By applying Lemma 5.1 to ( S τ , G ) for the claim ξ G , and using Theorem 5.2-(d) afterwards, we derive

$$\mathcal { G } ( \mathbb { G } ) & \text { for the claim } \xi \sim \text {, and using } T \text { here } \mathcal { G } \text { is } 2 \cdot ( \mathbb { D } ) \text { afterwards, we always } \mathcal { W } \text { deviates} \\ & \widehat { \mathcal { P } } _ { T - 1 } ^ { ( G , 3 ) } = \mathcal { \widehat { P } } _ { T - 1 , T } ^ { ( S T , \mathbb { G } ) } ( \xi ^ { G } ) = K _ { T - 1 } I _ { \{ \tau \leq T - 1 \} } + \widehat { \mathcal { P } } _ { T - 1 , T } ^ { ( \mathcal { S } , \mathbb { F } ) } ( \kappa _ { T } ^ { ( g ) } ) I _ { \{ \tau > T - 1 \} } \\ & = K _ { T - 1 } I _ { \{ \tau \leq T - 1 \} } + \widehat { \mathcal { P } } _ { T - 1 , T } ^ { ( \widehat { S } , \widehat { F } , \widehat { Q } ) } ( f _ { K } ( T , g _ { T } \widehat { G } _ { T > 0 } ) I _ { \{ \tau > T - 1 \} } \\ & = K _ { T - 1 } I _ { \{ \tau \leq T - 1 \} } + \widehat { \mathcal { P } } _ { T - 1 , \widehat { T } } ^ { ( \widehat { S } , \widehat { F } , \widehat { Q } ) } ( f _ { K } ( T , \widehat { P } _ { T } ^ { ( 3 ) } ) I _ { \{ \tau > T - 1 \} } \\ & = K _ { T - 1 } I _ { \{ \tau \leq T - 1 \} } + \widehat { \mathcal { P } } _ { T - 1 } ^ { ( 3 ) } I _ { \{ \tau > T - 1 \} } . \\ \intertext { v o r p o s } \text {a} 5.1 and \text {theorem 5.2-(d)} \text { afterwards for the pair } ( K , \widehat { \mathcal { P } } ^ { ( 3 ) } ) \text { instead of } ( K , g ) , \text { and write }$$

This proves the equality (5.30) for t = T -1. Thus, to prove this equlity for t = T -2, we apply again Lemma 5.1 and Theorem 5.2-(d) afterwards for the pair ( K, P (3) ) instead of ( K,g ), and write

Therefore, we obtain the equality (5.30) for t = T -2. Thus, we can prove the equality (5.30) for any t by backward induction. In fact, we suppose that (5.30) holds for t +1, and we will prove it holds for t . To this end, we apply then Lemma 5.1 and Theorem 5.2-(d) afterwards and get

$$\text {proves the equality (5.30) for $t = T - 1$. Thus, to prove this equality for $t = T - 2$, we apply again} \\ \intertext { a n d . Theorem 5.2-(d) a f t e r w d s for the pair ( K , \widehat { \mathcal { P } } ^ { ( 3 ) } ) instead of ( K , g ) , and write } \widehat { \mathcal { P } } _ { T - 2 } ^ { ( G , 3 ) } = \widehat { \mathcal { P } } _ { T - 2 , T - 1 } ^ { ( S T , \mathbb { G } ) } ( \widehat { \mathcal { P } } _ { T - 1 } ^ { ( G , 3 ) } ) \\ = K _ { T - 2 } I _ { \{ \tau \leq T - 2 \} } + \widehat { \mathcal { P } } _ { T - 2 , T - 1 } ^ { ( \widehat { S } , \mathbb { F } ) } ( \kappa _ { T - 1 } ( K , \widetilde { \mathcal { P } } ^ { ( 3 ) } ) I _ { \{ \tau > T - 2 \} } \\ = K _ { T - 2 } I _ { \{ \tau \leq T - 2 \} } + \widehat { \mathcal { P } } _ { T - 2 , T - 1 } ^ { ( \widehat { S } , \mathbb { F } ) } ( f _ { K } ( T - 1 , \widetilde { \mathcal { P } } _ { T - 1 } ^ { ( 3 ) } ) I _ { \{ \tau > T - 2 \} } \\ = K _ { T - 2 } I _ { \{ \tau \leq T - 2 \} } + \widetilde { \mathcal { P } } _ { T - 2 } ^ { ( 3 ) } I _ { \{ \tau > T - 2 \} } \\ \text {for, we obtain the equality (5.30) for $t = T - 2$. Thus, we can prove the equality (5.30) for any} \\ \text {backward induction. In fact, we suppose that (5.30) holds for $t + 1$, and we will prove it holds} \\ \text {To this end, we apply then Lemma 5.1 and Theorem 5.2-(d) afterwards and get}$$

$$\widehat { \mathcal { P } } _ { t } ^ { ( G , 3 ) } & = \widehat { \mathcal { P } } _ { t } ^ { ( S , 3 ) } ( \widehat { \mathcal { G } } _ { 1 } ^ { ( G , 3 ) } ) = K _ { t } I _ { \uparrow \zeta t } + \widehat { \mathcal { P } } _ { t , 1 + 1 } ^ { ( S , F , \widehat { Q } ) } ( \kappa _ { + 1 } ( K , \widetilde { \mathcal { P } } ^ { ( 3 ) } ) ) I _ { \uparrow \zeta t } > \\ & = K _ { t } I _ { \uparrow \zeta t } \{ + \widehat { \mathcal { P } } _ { t , t + 1 } ^ { ( S , F , \widehat { Q } ) } ( f _ { K } ( t + 1 , \widehat { \mathcal { P } } _ { t + 1 } ^ { ( 3 ) } ) ) I _ { \uparrow \zeta t } \} \\ & = K _ { t } I _ { \uparrow \zeta t } \{ + \widehat { \mathcal { P } } _ { t } ^ { ( 3 ) } I _ { \uparrow \zeta t } > . \\ \intertext { i n c h e f a l u g y } \text {, the equality ( 5.30) hold for } t , \text { and hence it holds for any } t \in \{ 0 , \dots , T - 1 \} . \text { This completes } \intertext { o f proof of ( 5.30), while the re t s of this part addresses ( 5.31). By combining ( 5.30) and the two facts }$$

$$\widehat { \mathcal { P } } ^ { ( \mathbb { G } , 3 ) } & \\ = & ( K _ { 0 } - \widetilde { \mathcal { P } } _ { 0 } ^ { ( 3 ) } ) I _ { \{ \tau = 0 \} } + ( K - \widetilde { \mathcal { P } } ^ { ( 3 ) } ) \cdot D + ( \widetilde { \mathcal { P } } ^ { ( 3 ) } ) ^ { \tau } \\ = & \widehat { \mathcal { P } } _ { 0 } ^ { ( \mathbb { G } , 3 ) } + ( K - \widetilde { \mathcal { P } } ^ { ( 3 ) } ) \cdot N ^ { \mathbb { G } } + \frac { K - \widetilde { \mathcal { P } } ^ { ( 3 ) } } { \widetilde { G } } I _ { \mathbb { 0 } , \tau } \cdot D ^ { o , \mathbb { F } } + ( \widetilde { \mathcal { P } } ^ { ( 3 ) } ) ^ { \tau } - \widetilde { \mathcal { P } } _ { 0 } ^ { ( 3 ) } . \\ \intertext { t a s e l q u a l i y i s d e u t o d = N ^ { \mathbb { G } } + \widetilde { G } ^ { - 1 } I _ { [ 0 , \tau ] } \cdot D ^ { o , \mathbb { F } } , s e e ( 5 . 1 7 ) f o r d e t a l i s . $ T h a n k s t o ( 5 . 3 2 ) , }$$

Hence, the equality (5.30) holds for t , and hence it holds for any t ∈ { 0 , ..., T -1 } . This completes the proof of (5.30), while the rest of this part addresses (5.31). By combining (5.30) and the two facts XI ] ]0 ,τ ] ] = X τ -X τ D and X τ D = X τ I ] ] τ, ∞ ] ] = X 0 I { τ =0 } + X · D , which hold for any process X , we get ̂ P ( G , 3) 0 = K 0 I { τ =0 } + ˜ P (3) 0 ) I { τ&gt; 0 } and

$$( 5 . 4 6 )$$

˜ The last equality is due to D = N G + ˜ G -1 I [ [0 ,τ [ [ · D o, F , see (5.17) for details. Thanks to (5.32), it is clear that M (3) 0 = A (3) 0 = 0 and

$$\widetilde { \mathcal { P } } ^ { ( 3 ) } = \widetilde { \mathcal { P } } _ { 0 } ^ { ( 3 ) } + & - M ^ { ( 3 ) } + A ^ { ( 3 ) } , \text { where } A _ { t } ^ { ( 3 ) } \colon = \sum _ { s = 1 } ^ { t } E [ \widetilde { \mathcal { P } } _ { s } ^ { ( 3 ) } - \widetilde { \mathcal { P } } _ { s - 1 } ^ { ( 3 ) } | \mathcal { F } _ { s - 1 } ] , \quad t = 1 , \dots , T . \\$$

$$Then by inserting these in ( 5 . 6 ) and applying Lerna 5 . 1 3 to \widetilde { V } ^ { ( 3 ) } , M ^ { ( 3 ) } and N ^ { ( 3 ) } , we derive \\ \widehat { \mathcal { P } } ^ { ( G , 3 ) } = \widehat { \mathcal { P } } _ { 0 } ^ { ( G , 3 ) } + ( K - \widetilde { \mathcal { P } } ^ { ( 3 ) } ) \cdot N ^ { G } + \widehat { \mathcal { G } } _ { \overline { K } , [ 0 , \tau ] } \cdot D ^ { \circ , \overline { F } } + ( \widetilde { \mathcal { P } } ^ { ( 3 ) } ) ^ { \tau } - \widetilde { \mathcal { P } } _ { 0 } ^ { ( 3 ) } \\ = \widehat { \mathcal { P } } _ { 0 } ^ { ( G , 3 ) } + ( \widetilde { K } - \widetilde { \mathcal { P } } ^ { ( 3 ) } ) \cdot N ^ { G } + ( M ^ { ( 3 ) } ) ^ { \tau } + ( \mathcal { A } ^ { ( 3 ) } ) ^ { \tau } + \frac { 1 } { \widetilde { G } } ( N ^ { ( 3 ) } + \widetilde { V } ^ { ( 3 ) } ) ^ { \tau } \\ = \widehat { \mathcal { P } } _ { 0 } ^ { ( G , 3 ) } + ( K - \widetilde { \mathcal { P } } ^ { ( 3 ) } ) \cdot N ^ { G } + \mathcal { T } ^ { ( M ^ { ( 3 ) } ) } + \frac { 1 } { G _ { - } } \cdot \mathcal { T } ^ { ( \overline { N } ^ { ( 3 ) } ) } + G _ { - } ^ { - 1 } \cdot \mathcal { T } ^ { ( N ^ { ( 3 ) } ) } ) \\ - \frac { \Delta ( M ^ { ( 3 ) } , m ) } { G _ { - } ^ { 2 } } \cdot \mathcal { T } ( m ) + \frac { 1 } { G _ { - } } \cdot \langle M ^ { ( 3 ) } , m \rangle ^ { \tau } - \frac { \Delta \widetilde { V } ^ { ( 3 ) } } { G _ { - } ^ { 2 } } \cdot \mathcal { T } ( m ) \\ + \frac { p , \mathbb { F } } { G _ { - } } ( I _ { \widetilde { G } > 0 } ) \cdot ( \widetilde { V } ^ { ( 3 ) } ) ^ { \tau } + ( A ^ { ( 3 ) } ) ^ { \tau } - G _ { - } ^ { - 1 } I _ { 0 } | _ { \tau } \cdot | \left ( I _ { \widetilde { G } = 0 } \right ) ^ { N } \right ) ^ { p , \mathbb { F } } , \\ = \widehat { \mathcal { P } } _ { 0 } ^ { ( G , 3 ) } + ( K - \widetilde { \mathcal { P } } ^ { ( 3 ) } ) \cdot N ^ { G } + \mathcal { T } ( M ^ { ( 3 ) } ) + \frac { 1 } { G _ { - } } \cdot \mathcal { T } ( \overline { N } ^ { ( 3 ) } ) + \frac { 1 } { G _ { - } } \cdot \mathcal { T } ( N ^ { ( 3 ) } ) \\ - \left ( \Delta ( M ^ { ( 3 ) } , m ) + \Delta \widetilde { V } ^ { ( 3 ) } \right ) G _ { - } ^ { - 2 } \cdot \mathcal { T } ( m ) \\ + G _ { - } ^ { - 1 } \cdot ( \widetilde { V } ^ { ( 3 ) } ) ^ { \tau } + G _ { - } ^ { - 1 } \cdot \langle M ^ { ( 3 ) } , m \rangle ^ { \tau } + ( A ^ { ( 3 ) } ) ^ { \tau } . \\ \text {The last equality is due to I _ { \widetilde { G } = 0 } } \cdot D ^ { \circ , \mathbb { F } } = 0 . \text { Thus, direct calculations yields} \\ \quad G _ { - } ^ { - 1 } \cdot ( \widetilde { V } ^ { ( 3 ) } ) ^ { \tau } + G _ { - } ^ { - 1 } \cdot \langle M ^ { ( 3 ) } , m \rangle ^ { \tau } + ( A ^ { ( 3 ) } ) ^ { \tau }$$

˜ The last equality is due to I { ˜ G =0 } · D o, F = 0. Thus, direct calculations yields

$$G _ { - } ^ { - 1 } \cdot ( \widetilde { V } ^ { ( 3 ) } ) ^ { \tau } + G _ { - } ^ { - 1 } \cdot \langle M ^ { ( 3 ) } , m \rangle ^ { \tau } + ( A ^ { ( 3 ) } ) ^ { \tau } \\ = \sum _ { s = 1 } ^ { \tau \wedge \cdot } E \left [ K _ { s } \frac { \widetilde { G } _ { s } - G _ { s } } { G _ { s - 1 } } + \widetilde { \mathcal { P } } _ { s } ^ { ( 3 ) } \frac { G _ { s } } { G _ { s - 1 } } - \widetilde { \mathcal { P } } _ { s - 1 } ^ { ( 3 ) } \right ] \mathcal { F } _ { s - 1 } \right ] . \\ \text {of assertion } ( b ) , \text { and the proof of the theorem is complete.}$$

This ends the proof of assertion (b), and the proof of the theorem is complete.

## References

- [1] Baptiste, J., Carassus, L. and Lepinette, E. Pricing without martingale measure. To appear in 'ESAIM Proceedings MAS 2022', 2023.
- [2] Barron, E.N., Cardaliaguet, P. and Jensen, R. Conditional essential suprema with applications. Applied Mathematics and Optimization, 48, 229-253, 2003.
- [3] B´ elanger, A., Shreve, S. E. and Wong, D. A general framework for pricing credit risk. Mathematical Finance, 14(3), 317-350, 2004.
- [4] Bensaid, J.P., Lesne, H. and Pag` es, H., and Scheinkman, J. Derivatives asset pricing with transaction costs. Math. Finance, 2, 63-86, 1992.
- [5] Bielecki, T. and Rutkowski, M. Credit Risk. Modeling, Valuation and Hedging. Berlin: Springer Finance, 2002.
- [6] Carassus, L. and Lepinette, E. Pricing without no-arbitrage condition in discrete-time. Journal of Mathematical Analysis and Applications, 505, 1, 125441, 2022.
- [7] Choulli, T. and Deng, J. No-arbitrage for informational discrete-time market models. Stochastics, 89, 3-4, 628-653, 2017.

- [8] Choulli, T. and Deng, J. Structure Conditions under Progressively Added Information. Theory Probab. Appl., 65, 3, 418-453, 2020.
- [9] Choulli, T., Daveloose, C. and Vanmaele, M. A martingale representation theorem and valuation of defaultable securities. Mathematical Finance, 30, 4, 1527-1564, 2020.
- [10] Choulli, T., Daveloose, C. and Vanmaele, M. Mortality/Longevity Risk-Minimization with or without Securitization. Mathematics 2021, 9, 1629, 2021.
- [11] Choulli, T. and Yansori, S. Log-optimal portfolio without NFLVR: existence, complete characterization, and duality. Theory Probab. Appl. , 67, 2 229-245, 2022.
- [12] Choulli, T. and Yansori, S. Log-optimal and num´ eraire portfolios for market models stopped at a random time. Finance and Stochastics, 26, 535-585, 2022.
- [13] Choulli, T. and Yansori, S. Explicit description of all deflators for market models under random horizon with applications to NFLVR. Stochastic Processes and their Applications 151, 230-264, 2022.
- [14] Dellacherie, C. and Meyer, P-A. Probabilit´ e et Potentiel, Th´ eorie des martingales. Chapter V-VIII. Hermann, 1980.
- [15] El Mansour, M. and Lepinette, E. Conditional interior and conditional closure of a random sets. Journal of Optimization Theory and Applications, 187, 356-369, 2020.
- [16] Fernholz, R. and Karatzas, I. Stochastic portfolio theory: An overview. A Bensoussan, Q Zhang (Eds.), Handbook in Numerical analysis: Mathematical modelling and Numerical Methods in Finance, 89-167, 2009.
- [17] F¨ ollmer, H. and Schied, A. Stochastic Finance: An introduction in Discrete Time. 2nd ed., De Gruyter Studies in Mathematics, Walter de Gruyter, Berlin-New York, 2004.
- [18] F¨ ollmer, H. and Kramkov, D. Optional decompositions under constraints. Probab. Theory Relat. Fields, 109, 1-25, 1997.
- [19] He, S.W., Wang, J.G. and Yan, J.A. Semimartingale Theory and Stochastic Calculus. Kexue Chubanshe (Science Press)/CRC Press, Beijing/Boca Raton, 1992.
- [20] Jacod, J. Calcul stochastique et probl` emes de martingale, 714, Springer, 1979.
- [21] Jacod, J. and Shiryaev, A.N. Limit theorems for stochastic processes. Grundlehren der mathematischen Wissenschaften 288, a series of comprehensive studies in mathematics. Springer-Verlag Berlin, Heidelberg, 2003.
- [22] Jeulin, T. Semi-martingales et grossissement de filtration. Springer-Verlag, 833, Lecture Notes in Mathematics, Berlin, Heidelberg, New York, 1980.
- [23] Kabanov, Y. and Safarian, M. Markets with transaction costs. Mathematical Theory. SpringerVerlag, 2009.
- [24] Kabanov, Y. and Lepinette, E. Essential supremum with respect to a random partial order. Journal of Mathematical Economics, 49, 6, 478-487, 2013.
- [25] Karatzas, I. and Shreve, S. Method of Mathematical Finance. Springer-Verlag, 1988.

- [26] Kladivko, K. and Zervos, M. Mean-variance hedging of contingent claims with random maturity. Mathematical Finance, 33, 4, 977-1411, 2023.
- [27] Schal, M. Martingale measures and hedging in discrete time financial markets. Math. Oper. Res., 24, 509-528, 1999.
- [28] Szimayer, A. A reduced form model for ESO valuation modelling the effects of employee departure and takeovers on the value of employee share options. Mathematical Methods of Operation Research, 59, 111-128, 2004.
- [29] Sircar, R and Xiong, W. A general framework for evaluating executive stock options. Journal of Economic Dynamics and Control, 31, 2317-2349, 2007.
- [30] Carr, P. and Linetsky, V. The valuation of executive stock options in an intensity-based framework. European Finance Review, 4, 211-230, 2000.
- [31] Leung, T. and Sircar, R. Accounting for risk aversion, vesting, job termination risk and multiple exercises in valuation of employee stock options. Mathematical Finance, 19, 99-128, 2009.