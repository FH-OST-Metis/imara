# Evaluation config for running open-rag-eval on the LinearRAG pipeline

input_queries: "data/eval/queries.csv"
results_folder: "data/eval/linearrag"

# This is the CSV produced by src/app/eval_open_rag.py when --system linearrag
generated_answers: "generated_answers.csv"

# Consolidated evaluation results and metrics plot
# (relative to results_folder)
eval_results_file: "results.csv"
metrics_file: "metrics.png"

evaluator:
  - type: "TRECEvaluator"
    model:
      type: "GeminiModel"
      name: "gemini-2.5-flash"
      api_key: ${oc.env:GEMINI_API_KEY}
    options:
      k_values: [1, 3, 5]
      run_consistency: true
      metrics_to_run_consistency:
        - "mean_umbrela_score"
        - "vital_nuggetizer_score"
        - "citation_f1_score"
