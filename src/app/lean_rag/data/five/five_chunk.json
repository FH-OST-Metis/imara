[
  {
    "hash_code": "58a5220afbc19a123e028a11414c1627",
    "text": "## ScanSSD: Scanning Single Shot Detector for Mathematical Formulas in PDF Document ImagesParag Mali, Puneeth Kukkadapu, Mahshad Mahdavi, and, Richard ZanibbiDepartment of Computer Science, Rochester Institute of Technology Rochester, NY 14623, USA Email: { psm2208, pxk8301, mxm7832, rxzvcs } @rit.eduAbstract -We introduce the Scanning Single Shot Detector (ScanSSD) for locating math formulas offset from text and embedded in textlines. ScanSSD uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results. For our experiments we use the TFD-ICDAR2019v2 dataset, a modification of the GTDB scanned math article collection. ScanSSD detects characters in formulas with high accuracy, obtaining a 0.926 f-score, and detects formulas with high recall overall. Detection errors are largely minor, such as splitting formulas at large whitespace gaps (e.g., for variable constraints) and merging formulas on adjacent textlines. Formula detection f-scores of 0.796 (IOU ≥ 0 . 5 ) and 0.733 (IOU ≥ 0 . 75 ) are obtained. Our data, evaluation tools, and code are publicly available.## I. INTRODUCTIONThe PDF format is used ubiquitously for sharing and printing documents. Unfortunately, while the latest PDF specification supports embedding structural information for graphical elements (e.g., figures, tables, and footnotes 1 ), most born-digital PDF documents contain only rendering-level information such as characters, lines, and images. These low-level objects can be recovered by parsing the document [1], but graphic regions must be located using detection algorithms. For example, PDFFigures [2] extracts figures and tables from born-digital Computer Science research papers for Semantic Scholar. 2 Unfortunately, older PDF documents may contain only scanned images of document pages, providing no information about characters or other graphical objects in the page images.We present a new image-based detector for mathematical formulas in in both born-digital and scanned PDF documents. Math expressions may be displayed and offset from the main text, or appear embedded directly in text lines (see Figure 1). Displayed expressions are generally easier to detect due to indentation and vertical gaps, whereas embedded expressions are more challenging. Embedded equations may differ in font, e.g., for the italicized variable t in Figure 1, but italicized words and variations in text fonts make fonts unreliable for detecting embedded formulas in general.1 https://www.iso.org/standard/63534.html2 https://www.semanticscholar.orgFig. 1: Embedded (blue) vs. displayed (red) formulas.other![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000000_bdbe12dcf6bea0b18188daf25e54716666882858ff8be81a9991368033d74680.png)Our work makes two main contributions. First, we introduce the ScanSSD architecture for detecting formulas using only visual features. A deep neural-network Single Shot Detector (SSD [3]) locates formulas at multiple scales using a sliding window in a 600 dpi page image. Page-level formula detections are obtained by pooling SSD region detections using a simple voting and thresholding procedure. ScanSSD detects characters in formulas with high accuracy (92.6% f-score), and detects formula regions accurately enough to be used as a baseline for indexing mathematical formulas in born-digital and scanned PDF documents. The ScanSSD code is publicly available. 3Our second contribution is a new benchmark for formula detection comprised of a dataset and evaluation tools. The dataset is a modification of the GTDB database of Suzuki et al [4]. Our data and evaluation tools were developed for the ICDAR 2019 TFD competition [5], and the dataset (TFD-ICDAR2019v2) and evaluation tools are publicly available (see Section III).In the next section we provide an overview of related work, followed by our dataset (Section III), ScanSSD (Sections IV and V), our results (Section VI), and finally our conclusions and plans for future work (Section VII).## II. RELATED WORKExisting methods for formula detection in PDF documents use formatting information, such as page layout, character labels, character locations, font sizes, etc. However, PDF documents are generated by many different tools, and the quality of their character information varies. Lin et al. [6] point out math formulas may be composed of"
  },
  {
    "hash_code": "3e55ced1cae6ef2d0be08a0bd3f97a5a",
    "text": "and evaluation tools are publicly available (see Section III).In the next section we provide an overview of related work, followed by our dataset (Section III), ScanSSD (Sections IV and V), our results (Section VI), and finally our conclusions and plans for future work (Section VII).## II. RELATED WORKExisting methods for formula detection in PDF documents use formatting information, such as page layout, character labels, character locations, font sizes, etc. However, PDF documents are generated by many different tools, and the quality of their character information varies. Lin et al. [6] point out math formulas may be composed of several object3 https://github.com/MaliParag/ScanSSDtypes (e.g. text, image, graph). For example, the square root sign in a PDF generated from L A T E X contains the text object representing a radical sign and a graphical object for the horizontal line. As a result, some symbols must be identified from multiple drawing elements.Given characters and formula locations, the visual structure of each formula (i.e., spatial arrangement of symbols on writing lines) can be recovered with high accuracy using existing techniques [7]-[12]. For formula retrieval, flexible matching of sub-expressions requires that formula structure (i.e., visual syntax and/or semantics) be available -however, there has been recent work using CNN-based embeddings for purely appearance-based retrieval [13].Displayed expression detection is relatively easy, as offset formulas differ in height and width of the line, character size, and, symbol layout [14]. Embedded mathematical expressions are more challenging: Iwatsuki et al. [15] conclude that distinguishing dictionary words that appear in italics and embedded mathematical expressions is a non-trivial task as embedded formulas at times can contain complex mathematical structures such as summations or integrals. However, many embedded math expressions are very small, often just a single symbol in a definition such as 'where w is the set of words'. Some approaches have been proposed specifically for embedded math expression detection [15], [16] and others specifically for displayed math expressions [17], [18].Lin et al. classify formula detection methods into three categories based on the features used [6]. These categories are character-based, image-based, and layout-based. Character-based methods use OCR engines to identify characters, and characters not recognized by the engine are considered candidates for math expression elements. The second category of methods uses image segmentation. Most traditional methods require segmentation thresholds. Setting threshold values can be difficult, especially for the unknown documents. Layout-based methods detect math expressions using features such as line height, line spacing, alignment, etc. Many published methods use a combination of character features, layout features, and context features.## A. Traditional MethodsGarain and Chaudhari did a survey of over 10,000 document pages and found the frequency of each mathematical character in formulas [19]. They used this information found to develop a detector for embedded mathematical expressions [20]. They scan each text line and decide if the line contains one of the 25 most frequent mathematical symbols. After finding the leftmost word containing a mathematical symbol, they grow the region around the word on the left and right using rules to identify the formula region. For detection of displayed expressions they use two features: first, white space around math expressions. Second, the standard deviation of the left lowermost pixels of symbols on the text line. They base this feature on the observation that for a math expression, the leftmost pixels of each symbol are often not on the same line, while for text they often are. A disadvantage of their method for embedded formula detection is that it requires symbol recognition, which adds complexity to the system. Another approach based on locating mathematical symbols and then growing formula regions around symbols was proposed by Kacem et al., but using fuzzy logic [21].Lin et al. [6] proposed a four-step detection process. In the first step, they extract the locations, bounding boxes, baselines, fonts, etc., and use them for character and layout features in the following steps. They also process math symbols comprised of multiple objects: for example, a vertical delimiter may be made up of multiple short vertical line objects. They detect named mathematical functions such as 'sin,' 'cos,' etc., and numbers. In the next step, they distinguish text lines from non-text lines. They find displayed math expressions in nontext lines using geometric layout features (e.g., line-height), character features (e.g., is it the character part of a named math function like 'sin'), and context features (e.g., whether the preceding and the following character is a math element). In the last step, they classify characters into math and non-math characters. They find embedded math expressions by merging characters tagged as math characters. SVM classification was used for both isolated math expression detection, and character classification into math and non-math.## B. CRF and Deep Learning-Based TechniquesFor born-digital PDF papers, Iwatsuki et al"
  },
  {
    "hash_code": "fb5aced16a5f67aab6f90460e0cd53c4",
    "text": "nontext lines using geometric layout features (e.g., line-height), character features (e.g., is it the character part of a named math function like 'sin'), and context features (e.g., whether the preceding and the following character is a math element). In the last step, they classify characters into math and non-math characters. They find embedded math expressions by merging characters tagged as math characters. SVM classification was used for both isolated math expression detection, and character classification into math and non-math.## B. CRF and Deep Learning-Based TechniquesFor born-digital PDF papers, Iwatsuki et al. [15] created a manually annotated dataset and applied conditional random fields (CRF) for math-zone identification using both layout features (e.g. font types) and linguistic features (e.g. n-grams) extracted from PDF documents. For each word, they used three labels: the beginning of a math expression, inside a math expression, and at the end of a math expression. They concluded that words and fonts are important for distinguishing math from the text. This method has limitations, as it requires a specially annotated dataset that has each word annotated with either beginning, inside or end of the math expression label. Their method works only for born-digital PDF documents with layout information.Gao et al. [18] used a combination of CNN and RNN for formula detection. They first extract text, graph and image streams from the PDF document. Next, they perform top-down layout analysis based on XY-cutting [22], and bottom-up layout analysis based on connected components to generate candidate expression regions. Features are then extracted using neural networks from each candidate region, and they classify candidate regions. Finally, they adjust and refine the incomplete math expression areas. Similar to their method, we use a CNN model (VGG16 [23]) for feature extraction. In contrast to their method, we do not depend on the layout analysis of the page.Recently Ohyama et al. [4] used a U-net to detect characters in formulas. The U-net acts as a pixel-level image filter, and does not produce regions for symbols or formulas.Detection is evaluated based on pixel-level agreement between detected and ground-truth symbols; formula detection is estimated based on the number of formulas with at least half of their characters detected. In contrast, our method produces bounding boxes for mathematical expressions of one or more symbols. As we wanted to propose specific regions (bounding boxes) for formulas, we decided to explore modern object detection methods employing deep neural networks.We next we discuss different object detection methods, and our selection of SSD as the underlying detector for our model.## C. Object DetectionThe first deep learning algorithm that achieved noticably stronger results for object detection task was the R-CNN [24] (Region proposal with CNN). Unlike R-CNN, which feeds ≈ 2 k regions to a CNN for each image, in Fast R-CNN [25] only the original input image is used as input. Faster R-CNN [26] introduced a different architecture, the region-proposal network. In contrast to R-CNN, Fast R-CNN, and Faster R-CNN which use region proposals, the YOLO [27] and Single Shot MultiBox Detectors (SSD) [3] perform detection in a single-stage network. Both YOLO and SSD divide the input image into a grid, where each grid point has an associated set of 'default' bounding boxes. Unlike YOLO, SSD uses multiple grids with different scales instead of a single grid. This allows an SSD detector to divide the responsibility for detecting objects across scales. The SSD network learns to predict offsets and size modifications for each default bounding box. Just like R-CNN, SSD uses the VGG16 [23] architecture for feature extraction. SSD does not require selective search, region proposals, or multi-stage networks like R-CNN, Fast R-CNN, and, Faster R-CNN.Among the CNN-based object detectors, SSD is a simple single stage model that obtains accuracy comparable to models with region proposal steps such as Faster R-CNN [3], [28]. Liao et al. with their TextBoxes architecture have shown that a modified SSD can detect wide regions [29]. Formulas are often quite wide, and so we use an SSD modified in a manner similar to TextBoxes as the basis for our formula detector. Details for our detector are presented in Section IV.## III. CREATING THE TFD-ICDAR2019V2 DATASETFor typeset formula detection, we modified ground truth for the GTDB1 and GTDB2 datasets 4 created by Suzuki et al. [30]. TFD-ICDAR2019v2 represents 'Typeset Formula Detection task for ICDAR 2019,' version 2. The first version was used for the CROHME math recognition competition at ICDAR 2019 [5]: version two (v2) adds formulas"
  },
  {
    "hash_code": "1c47303c917ee5f26b10ed1ba9f3b8c6",
    "text": "basis for our formula detector. Details for our detector are presented in Section IV.## III. CREATING THE TFD-ICDAR2019V2 DATASETFor typeset formula detection, we modified ground truth for the GTDB1 and GTDB2 datasets 4 created by Suzuki et al. [30]. TFD-ICDAR2019v2 represents 'Typeset Formula Detection task for ICDAR 2019,' version 2. The first version was used for the CROHME math recognition competition at ICDAR 2019 [5]: version two (v2) adds formulas to ground truth that were missing in the original. The dataset is available online, and we provide scripts to compile and render the dataset PDFs at 600 dpi, and evaluation scripts with region matching using thresholded intersection-over-union (IOU) measures. 54 available from https://github.com/uchidalab/GTDB-Dataset5 https://github.com/MaliParag/TFD-ICDAR2019TABLE I: TFD-ICDAR2019v2 Collection Statistics.|          | Docs (Pages)   |   1 symbol |   Formulas > 1 symbol |   Total ||----------|----------------|------------|-----------------------|---------|| Training | 36 (569)       |       7506 |                 18947 |   26453 || Test     | 10 (236)       |       2556 |                  9350 |   11906 |The GTDB collection provides annotations for 48 PDF documents from scientific journals and textbooks using a variety of font faces and notation styles. It also provides ground truth at the character level in CSV format, including spatial relationships between math characters (e.g., subscript, superscript). Character labels, and an indication of whether a character belongs to a formula region are also provided.At the time we created our dataset in early 2019, we were unable to locate two PDFs from GTDB1, and so omitted them in TFD-ICDAR2019v2. 6 From the remaining 46 documents, 10 PDFs from GTDB2 serve as the test set (see Figure 7). We developed image processing tools for modifying the GTDB ground-truth to reflect scale and translation differences found in the publicly available versions of the PDF documents. GTDB also does not provide bounding boxes for math expressions directly: we used character bounding boxes and spatial relationships to generate math regions in our ground truth files.Metrics for TFD-ICDAR2019v2 may be found in Table I. It is worth noting that over 25% of formulas in the collection contain a single symbol (e.g., ' λ ').## IV. SCANSSD: WINDOW-LEVEL DETECTIONFigure 2 illustrates the ScanSSD architecture. First, we use a sliding window to sample overlapping sub-images from the document page image. We then pass each window to a Single-Shot Detector (SSD [3]) to locate formula regions. SSD simultaneously evaluates multiple formula region candidates laid out in a grid (see Figure 3), and then applies non-maximal suppression (NMS) to select the window-level detections. NMS is a greedy strategy that keeps one detection per group of overlapping detections. Formulas detected within each window have associated confidences, shown using colour in the 3rd stage of Figure 2.As seen with the purple boxes in Figure 2, many formulas are repeated and/or split across the sampled windows. To obtain page-level formula detections, we first stitch the window-level SSD detections together on the page. A voting-based pooling method in then used to obtain final detection results (shown as green boxes in Figure 2).Details of the ScanSSD system are provided below.## A. Sliding WindowsTo produce sub-images for use in detection, starting from a 600 dpi page image we slide a 1200 × 1200 window with a vertical and horizontal stride (shift) of 120 pixels (10% of window size). Our windows are roughly 10 text lines in height,Fig. 2: ScanSSD architecture. Heatmaps illustrate detection confidences with gray ≈ 0 , red ≈ 0 . 5 , white ≈ 1 . 0 . Purple and green bounding boxes show formula regions after stitching window-level detections and pooling, respectively.bar chartThe image is a screenshot of a computer program interface, specifically a software that appears to be used for mathematical or scientific calculations. It is divided into two main sections: \"Input Page\" and \"Detected Formulas\". On the \"Input Page\", there are several text boxes and input fields where users can input data. The text in the boxes is not fully legible, but it seems to contain mathematical expressions or variables. There are also checkboxes and dropdown menus that likely allow users to select options or parameters.On the \""
  },
  {
    "hash_code": "93a1439cdf066a87fd5f83b17b9b81e2",
    "text": ". Purple and green bounding boxes show formula regions after stitching window-level detections and pooling, respectively.bar chartThe image is a screenshot of a computer program interface, specifically a software that appears to be used for mathematical or scientific calculations. It is divided into two main sections: \"Input Page\" and \"Detected Formulas\". On the \"Input Page\", there are several text boxes and input fields where users can input data. The text in the boxes is not fully legible, but it seems to contain mathematical expressions or variables. There are also checkboxes and dropdown menus that likely allow users to select options or parameters.On the \"Detected Formulas\" side, the software has processed the input and generated a series of formulas. These formulas are displayed in a grid format, with each formula in a separate box. The formulas are color-coded, with green indicating a successful calculation, yellow for a warning, and red for an error. The formulas vary in complexity, with some being simple arithmetic expressions and others involving multiple variables and functions.The interface![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000001_570e07c36aee7ea1457af9b460ccd711df0fbffef5658d5511b4204e7257b2ef.png)which makes math formulas large enough for SSD to detect them reliably. The SSD detector is trained using ground truth math regions cropped at the boundary of each window, after scaling and translating formula bounding boxes appropriately.Advantages. There are four main advantages to using sliding windows. The first is data augmentation: only 569 page images are available in the training set, which is very small for training a deep neural network. Our sliding windows produce 656,717 sub-images. Second, converting the original page image directly to 300 × 300 or 512 × 512 loses a great deal of visual information, and when we tried to detect formulas using subsampled page images recall was extremely low. Third, as we maintain the overlap between windows, the network sees formulas multiple times, and has multiple chances to detect a formula. This helps increase recall, because formulas appear in more regions of detection windows. Finally, Liu et al. [3] mention that SSD is challenged when detecting small objects. Formulas with just one or two characters are common, but also small. Using high-resolution sub-images increases the relative size of math regions, which makes it easier for SSD to detect them.Disadvantages. There are also a few disadvantages to using sliding windows versus detection within a single page image. The first is increased computational cost; this can be mitigated through parallelization, as each window may be processed independently. Secondly, windowing cuts formulas if they do not fit in a window. This means that a large expression may be split into multiple sub-images; this makes it impossible to train the SSD network to detect large math expressions directly. To mitigate this issue, we train the network to detect formulas across windows. Furthermore, windowing requires that we stitch (combine) results from individual windows to obtain detection results at the level of the original page. We discuss how we address these problems using pooling methods in section V.## B. Region Matching and Default Boxes in SSDSSD defines a fixed space of candidate detection regions organized in a spatial grid at multiple resolutions ('default boxes'). Each default box may be resized and translated by the SSD network to fit target regions, and is associated with a confidence score. Figure 3 shows default boxes of different sizes and aspect ratios overlaid on a 512 × 512 image. In SSD, each feature map is a pixel grid, but the associated default boxes are defined in the original image coordinate space. The image is analyzed at multiple scales; here for illustration the 32 × 32 grid of default boxes is shown. In practice, if we used only the 32 × 32 default boxes, we might miss smaller objects. For the highlighted formula in Figure 3, the wider yellow box has the maximum intersection-over-union (IOU), and during training the wide yellow box will be matched with the highlighted ground truth.Our metric for matching ground truth to candidate detection regions is the same as SSD [3]. Each ground truth box is matched to a default box with the highest IOU, and also with default boxes with an IOU greater than 0.5. Matching targets to more than one default box simplifies learning by allowing the network to predict higher scores for more boxes. The matched default boxes are considered positive examples (POS) and the remaining default boxes are considered negative examples (NEG).Fig. 3: Default boxes for a 512 × 512 window. Box centers ( glyph[star] ) are in a 32 × 32 grid. Shown are six default boxes around one point with different sizes and aspect ratios (red, green, and yellow boxes) located near a"
  },
  {
    "hash_code": "6f08254dfa1b71d7a253ac0ff01e8fba",
    "text": "the highest IOU, and also with default boxes with an IOU greater than 0.5. Matching targets to more than one default box simplifies learning by allowing the network to predict higher scores for more boxes. The matched default boxes are considered positive examples (POS) and the remaining default boxes are considered negative examples (NEG).Fig. 3: Default boxes for a 512 × 512 window. Box centers ( glyph[star] ) are in a 32 × 32 grid. Shown are six default boxes around one point with different sizes and aspect ratios (red, green, and yellow boxes) located near a target formula (pink highlight).otherthe equation for the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation of the equation![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000002_5dc032d71f2cd1938e78e77e6c4a76be8b9aeb374df1b74a5d80996ac6e667a2.png)The original SSD [3] architecture uses aspect ratios (width/height) of { 1 , 2 , 3 , 1 / 2 , 1 / 3 } . However, as we see in Figure 4, there are many wide formulas with an aspect ratio greater than 3 in the dataset. As a result, wider default boxes will have a higher chance of matching wide formulas. So, in addition to the default boxes used in the original SSD, we also add the wider default boxes used in TextBoxes [29], with aspect ratios { 5 , 7 , 10 } . In our early experiments, these wider default boxes increased recall for large formulas.## C. PostprocessingFigure 5 illustrates postprocessing in ScanSSD. We expand and/or shrink initial formula detections so that are cropped around the connected components they contain and touch at their border. The goal is to capture entire characters belonging to a detection region, without additional padding. This postprocessing is done at two stages: first, before stitching, and second, after pooling regions to obtain output formula detections.## V. SCANSSD: VOTING-BASED POOLING FROM WINDOWSAt inference time we send overlapping page windows to our modified SSD detector, and obtain formula bounding boxes with associated confidences in each window. As the SSD network sees the same page region multiple times, multiple bounding boxes are often predicted for a single formula (see Figure 2). Detections within windows are stitched together on the page, and then each detection region votes at the pixel level. Pixel-level votes are thresholded, and the bounding boxes of connected components in the resulting binary image are returned in the output as formula detections. Example formula detection results are provided in Figure 6.Voting. Let B be the set of page-level bounding boxes for detected formulas, and C be set of confidences obtained for each. Let B i ∈ B be the i th bounding box with confidence C i ∈ C . Let each pixel in image I be represented by pixelbar chartThe chart shows the number of counts for each of the 38 aspects, ranging from 1 to 38.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000003_760497dcd12af95a7053ddd08b30ad38330e84675b3ccfbdf20c1d8a58a97924.png)Fig. 4: Formula aspect ratios. Most formulas in our dataset have more width than height, i.e., are oriented horizontally.(a) Initial detectionother![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000004_27a288a"
  },
  {
    "hash_code": "b2e2d9b35c13d39e69be85969a8d5348",
    "text": "05v1_artifacts\\image_000003_760497dcd12af95a7053ddd08b30ad38330e84675b3ccfbdf20c1d8a58a97924.png)Fig. 4: Formula aspect ratios. Most formulas in our dataset have more width than height, i.e., are oriented horizontally.(a) Initial detectionother![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000004_27a288a14e84eea68e325a5ffc3c43e41a23d311e8f15eefce7f1cfdc72050ec.png)(b) After croppingother![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000005_ee9dc91cd165c3112095c000c6604a9f5df3223db089cf1dc2c8e8850729b4cd.png)Fig. 5: Postprocessing crops detection regions around connected components within or touching the initial detection.P ab . We say that a pixel P ab ∈ B i if it is inside the bounding box B i . Let us define$$L _ { a b } ^ { i } = \\begin{cases} 1 & \\text {if } P _ { a b } \\in B _ { i } \\\\ 0 & \\text {if } P _ { a b } \\notin B _ { i } \\end{cases}$$It is possible that ∑ i L i ab ≥ 1 , meaning that P ab belongs to more than one bounding box. We considered different vote scoring functions S ab for each pixel P ab :$$| B |$$$$\\text {scoring functions} \\ S _ { a b } \\text { for each pixel } P _ { a b } \\colon \\\\ \\quad \\text {uniform} \\ ( c o u n t ) \\quad \\sum _ { i \\colon = 0 } ^ { | B | } L _ { a b } ^ { i } \\\\ \\quad \\max \\quad \\arg \\max _ { i \\in \\{ 0 , \\dots , | B | \\} } L _ { a b } ^ { i } C _ { i } \\\\ \\quad \\\\ \\quad \\sum _ { i = 0 } ^ { | B | } L _ { a b } ^ { i } C _ { i } \\\\ \\quad \\text {average} \\quad \\sum _ { i = 0 } ^ { | B | } L _ { a b } ^ { i } C _ { i } \\ / \\ \\sum _ { i = 0 } ^ { | B | } L _ { a b } ^ { i } \\\\ \\quad \\\\ \\text {Threshold} \\ \\ W _ { a } \\text { computes} \\ \\text {voting} \\ \\text {method} \\ \\text {and} \\ \\text {two} \\ t$$Thresholding. We compare voting methods and tune their associated thresholds using the training data. A grid search was performed to maximize detection results for each voting method (f-score for IOU ≥ 0 . 75 ). Average scoring does not perform as well as the other methods. For uniform weighting and sum scores, we tried thresholds in { 0 , 1 , . . . , 55 } , and for max scoring we tried thresholds in { 0 , 1 , . . . , 100 } . The simplest method where each pixel for the number of detections it belongs to (uniform weighting) obtained the best detection results using a threshold value of 30, and so we use this in our experiments.## VI. RESULTS AND DISCUSSION## A. TrainingWe used a validation set to tune hyper-parameters for the ScanSSD detector. The TFD-ICDAR2019v2 training dataset was further divided into training (453 pages) and validation sets (116 pages). This produces 524,718 training and 131,999 testing sub-images, respectively. In our preliminary experiments, we observed that using a larger window sizeFig. 6: Detection results. Detected formulas are shown as blue bounding boxes. Split formulas are highlighted in pink (3rd panel), and merged formulas are highlighted in green (4th panel). A small number of false negatives (red) and false positives (yellow) are produced.otherA math textbook is opened to a page with a lot of math problems.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\"
  },
  {
    "hash_code": "a1da2dedb938fa52def73622bfcbb85f",
    "text": "training and 131,999 testing sub-images, respectively. In our preliminary experiments, we observed that using a larger window sizeFig. 6: Detection results. Detected formulas are shown as blue bounding boxes. Split formulas are highlighted in pink (3rd panel), and merged formulas are highlighted in green (4th panel). A small number of false negatives (red) and false positives (yellow) are produced.otherA math textbook is opened to a page with a lot of math problems.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000006_542b34c75b111565900365c2c5a5f4e59d354390704fb9d3a281a32092c2a415.png)with SSD512 performs far better (+5% f-score) than SSD300, [31] and cross-entropy loss with hard-negative mining performs better than focal-loss [32] (with or without hard-negative mining). Focal-loss reshapes the standard cross entropy loss such that it down-weights the loss for well-classified examples.We evaluated SSD models with different parameters 7 and found that our HBOXES512 model, which introduces additional default box aspect ratios (see Section IV-B) performs better than SSD512, and MATH512 performs better than HBOXES512. For HBOXES512 we used default boxes with aspect ratios { 1 , 2 , 3 , 5 , 7 , 10 } instead of default boxes with aspect ratios { 1 , 2 , 3 , 1 / 2 , 1 / 3 } for SSD512. MATH512 uses default boxes with aspect ratios { 1 , 2 , 3 , 5 , 7 , 10 } as well as rectangular kernels of size 1 × 5 rather than the square 3 × 3 kernel used in SSD512. From our experiments on the validation set, we observed that the MATH512 model consistently obtained the best detection results for the 512 × 512 inputs (by 0.5% to 1.0% f-score). So we use MATH512 for our evaluation. We then re-trained MATH512 using all TFD-ICDAR2019v2 training data.ScanSSD was built starting from an existing PyTorch SSD implementation. 8 The VGG16 sub-network was pre-trained on ImageNet [33].## B. Quantitative ResultsWe used two evaluation methods, based on the ICDAR 2019 Typeset Formula Detection competition [5] (Table II), and the character-level detection metrics used by Ohyama et al. [4] (Table III).7 Details are available in [31]8 https://github.com/amdegroot/ssd.pytorchTABLE II: Results for TFD-ICDAR2019|            | IOU ≥ 0 . 75   | IOU ≥ 0 . 75   | IOU ≥ 0 . 75   | IOU ≥ 0 . 5   | IOU ≥ 0 . 5   | IOU ≥ 0 . 5   ||------------|----------------|----------------|----------------|---------------|---------------|---------------||            | Precision      | Recall         | F-score        | Precision     | Recall        | F-score       || ScanSSD *  | 0.781          | 0.690          | 0.733          | 0.848         | 0.749         | 0.796         || RIT 2      | 0.753          | 0.625          | 0.683          | 0.831         | 0.670         | 0.754         || RIT 1      | 0.632          | 0.582          | 0.606          | 0.744         | 0.685         | 0.713         || Mitchiking | 0.191          | 0.139          | 0.161          | 0.369         | 0.270         | 0.312         || Samsung ‡  | 0.941          | 0.927          | 0.934          | 0.944         | 0.929         | 0.936         |* Used TFD-ICDAR2019v2 dataset- † Earlier ScanSSD, placed 2 nd in TFD-ICDAR 2019 competition [5]- ‡ Used character informationFormula detection. An earlier version of ScanSSD placed second in the ICDAR 2019 competition on Typeset Formula Detection (TFD) [5]. 9 The new ScanSSD system outperforms the other"
  },
  {
    "hash_code": "0b2ef40f269d97b5865ed0a9789eb7bf",
    "text": "0.941          | 0.927          | 0.934          | 0.944         | 0.929         | 0.936         |* Used TFD-ICDAR2019v2 dataset- † Earlier ScanSSD, placed 2 nd in TFD-ICDAR 2019 competition [5]- ‡ Used character informationFormula detection. An earlier version of ScanSSD placed second in the ICDAR 2019 competition on Typeset Formula Detection (TFD) [5]. 9 The new ScanSSD system outperforms the other systems from the competition that did not use character locations and labels from ground truth.Figure 7 gives the document-level f-scores for each of the 10 testing documents, for matching constraints IOU ≥ 0 . 5 and IOU ≥ 0 . 75 . The highest and lowest f-scores for IOU ≥ 0 . 75 are 0.8518 for Erbe94, and 0.5898 for Emden76. We think this variance is due to document styles: we have more training documents with a style similar to Erbe94 than Emden76. With more diverse training data we expect better results.Examining the effect of the IOU matching threshold on results demonstrates that the detection regions found by ScanSSD are highly precise: 70.9% of the ground-truth formulas are found at their exact location (i.e., IOU threshold of 1.0). Requiring this exact matching of detected and ground truth formulas also yields a precision of 62.67%, and an f-score of 66.5%. To obtain a more complete picture, we next look at the detection of math symbols.9 The first place system used provided character information.TABLE III: Benchmarking ScanSSD at the Character Level [4]. Note differences in data sets and evaluation techniques (see main text).| System        | Math Symbol   | Math Symbol   | Math Symbol   ||---------------|---------------|---------------|---------------|| System        | Precision     | Recall        | F-score       || ScanSSD †     | 0.889         | 0.965         | 0.925         || InftyReader * | 0.971         | 0.946         | 0.958         || ME U-Net *    | 0.973         | 0.950         | 0.961         |* Used GTDB dataset- † Used TFD-ICDAR2019v2 datasetFig. 7: Document-level results, IOU ≥ 0 . 5 and IOU ≥ 0 . 75 .bar chartThe image shows a bar graph with two sets of data points, each represented by different colors.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2003.08005v1_artifacts\\image_000007_f50db89ec43d905b1456ea20df26d4b5f76f2183614dfcf7d06bc7fcc8e6aab0.png)Math symbol detection. To measure math detection at the symbol (character) level, we consider all characters located within formula detections as 'math' characters. Our method has 0.9652 recall and 0.889 precision at the character level, resulting in a 0.925 f-score. This benchmarks well against recent results on the GTDB dataset (see Table III). Note that the detection targets (formulas for ScanSSD vs. characters), datasets, and evaluation protocols are different (1000 regions per test page are randomly sampled in Ohayama et al. [4]), and so the measures are not directly comparable. The lower precision for character detection in ScanSSD may be an artifact of predicting formulas rather than individual characters.The difference betweeen ScanSSD's math symbol detection f-score and formula detection f-score is primarily due to merging and splitting formula regions, which themselves are often valid subexpressions. Merging and splitting valid formula regions often produces regions too large or too small to satisfy the IOU matching criteria, leading to lower scores. Merging occurs in part because formula detections in neighboring text lines may overlap, and splitting may occur because large formulas have features similar to separate formulas within windowed sub-images.## C. Qualitative resultsFigure 6 provides example ScanSSD detection results. ScanSSD can detect math regions of arbitrary size, from a single character to hundreds of characters. It also detects matrices and correctly rejects equation numbers, page numbers, and other numbers not belonging to formulas. Figure 6 shows some example of detection errors. When there is a large space between characters within a formula (e.g., for variable constraints shown in the third panel of Figure 6), ScanSSD may split the"
  },
  {
    "hash_code": "731567fc455e192a82f7734f524bff99",
    "text": "neighboring text lines may overlap, and splitting may occur because large formulas have features similar to separate formulas within windowed sub-images.## C. Qualitative resultsFigure 6 provides example ScanSSD detection results. ScanSSD can detect math regions of arbitrary size, from a single character to hundreds of characters. It also detects matrices and correctly rejects equation numbers, page numbers, and other numbers not belonging to formulas. Figure 6 shows some example of detection errors. When there is a large space between characters within a formula (e.g., for variable constraints shown in the third panel of Figure 6), ScanSSD may split the formula and generate multiple detections (shown with pink boxes). Second, when formulas are close to each other, our method may merge them (shown with green boxes in Figure 6). Another error not shown, was wide embedded graphs (visually similar to functions) being detected as math formulas.On examination, it turns out that most detection 'failures' are because of valid detections merged or split in the manner described, and not spurious detections or false negatives. A small number of these are seen in Figure 6 using red and yellow boxes; note that all but one false negative are isolated symbols.## VII. CONCLUSIONIn this paper we make two contributions: 1) modifying the GTDB datasets to compensate for differences in scale and translation found in the publicly available versions of PDFs in the collection, creating new bounding box annotations for math expressions, and 2) the ScanSSD architecture for detecting math expressions in document images without using page layout, font, or character information. The method is simple but effective, applying a Single-Shot Detector (SSD) using a sliding window, followed by voting-based pooling across windows and scales.Through our experiments, we observed that 1) carefully selected default boxes improves formula detection, 2) kernels of size 1 × 5 yield rectangular receptive fields that better-fit wide math expressions with larger aspect ratios, and avoid noise that square-shaped receptive fields introduce.A key difference between formula detection in typeset documents and object detection in natural scenes is that typeset documents avoid occlusion of content by design. This constraint may help us design a better algorithm for non-maximal suppression, as the original non-maximal suppression algorithm is designed to handle overlapping objects. Also, we would like to use a modified version of the pooling methods based on agglomerative clustering such as the fusion algorithm introduced by Yu et al. [34]. We believe improved pooling will reduce the number of over-merged and split detections, improving both precision and recall.In our current architecture, we use a fixed pooling method; we plan to design an architecture where we can train the model end-to-end to learn pooling parameters directly from data. ScanSSD allows the use of multiple classes, and we would also like to explore detecting multiple page objects in a single framework.Acknowledgements. This material is based upon work supported by the Alfred P. Sloan Foundation under GrantNo. G-2017-9827 and the National Science Foundation (USA) under Grant No. IIS-1717997.## REFERENCES- [1] K. Davila, R. Joshi, S. Setlur, V . Govindaraju, and R. Zanibbi, 'Tangentv: Math formula image search using line-of-sight graphs,' in ECIR , ser. LNCS, vol. 11437, pp. 681-695.- [2] C. Clark and S. Divvala, 'Pdffigures 2.0: Mining figures from research papers,' in 2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL) . IEEE, 2016, pp. 143-152.- [3] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, 'SSD: Single shot multibox detector,' in European conference on computer vision . Springer, 2016, pp. 21-37.- [4] W. Ohyama, M. Suzuki, and S. Uchida, 'Detecting mathematical expressions in scientific document images using a u-net trained on a diverse dataset,' IEEE Access , vol. 7, pp. 144 030-144 042, 2019.- [5] M. Mahdavi, R. Zanibbi, H. Mouchere, and U. Garain, 'ICDAR 2019 CROHME + TFD: Competition on recognition of handwritten mathematical expressions and typeset formula detection,' in ICDAR 2019) . IEEE.- [6] X. Lin, L. Gao, Z. Tang, X. Lin, and X. Hu, 'Mathematical formula identification in pdf documents,' in 2011 International Conference on Document Analysis"
  },
  {
    "hash_code": "1537ff68a20f8dfc014e953d2727f920",
    "text": "pp. 144 030-144 042, 2019.- [5] M. Mahdavi, R. Zanibbi, H. Mouchere, and U. Garain, 'ICDAR 2019 CROHME + TFD: Competition on recognition of handwritten mathematical expressions and typeset formula detection,' in ICDAR 2019) . IEEE.- [6] X. Lin, L. Gao, Z. Tang, X. Lin, and X. Hu, 'Mathematical formula identification in pdf documents,' in 2011 International Conference on Document Analysis and Recognition . IEEE, 2011, pp. 1419-1423.- [7] M. Mahdavi, M. Condon, K. Davila, and R. Zanibbi, 'LPGA: Line-of-sight parsing with graph-based attention for math formula recognition,' in Proc. International Conference on Document Analysis and Recognition . Sydney, Australia: IAPR, September 2019, pp. 647654.- [8] M. Condon, 'Applying hierarchical contextual parsing with visual density and geometric features to typeset formula recognition,' Master's thesis, Rochester Institute of Technology, Rochester, NY, USA, 2017.- [9] Y. Deng, A. Kanervisto, J. Ling, and A. M. Rush, 'Imageto-markup generation with coarse-to-fine attention,' arXiv preprint arXiv:1609.04938 , 2016.- [10] J. Zhang, J. Du, and L. Dai, 'Track, attend, and parse (tap): An end-to-end framework for online handwritten mathematical expression recognition,' IEEE Transactions on Multimedia , vol. 21, no. 1, pp. 221233, 2018.- [11] F. Alvaro and R. Zanibbi, 'A shape-based layout descriptor for classifying spatial relationships in handwritten math,' in Proceedings of the 2013 ACM symposium on Document engineering . ACM, 2013, pp. 123-126.- [12] J. Zhang, J. Du, S. Zhang, D. Liu, Y. Hu, J. Hu, S. Wei, and L. Dai, 'Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition,' Pattern Recognition , vol. 71, pp. 196-206, 2017.- [13] L. Pfahler, J. Schill, and K. Morik, 'The search for equations - learning to identify similarities between mathematical expressions,' in Proc. ECML-PKDD , 2019.- [14] U. Garain and B. B. Chaudhuri, 'Ocr of printed mathematical expressions,' in Digital Document Processing . Springer, 2007, pp. 235-259.- [15] K. Iwatsuki, T. Sagara, T. Hara, and A. Aizawa, 'Detecting in-line mathematical expressions in scientific documents,' in Proceedings of the 2017 ACM Symposium on Document Engineering . ACM, 2017, pp. 141-144.- [16] X. Lin, L. Gao, Z. Tang, X. Hu, and X. Lin, 'Identification of embedded mathematical formulas in pdf documents using svm,' in Document Recognition and Retrieval XIX , vol. 8297. International Society for Optics and Photonics, 2012, p. 82970D.- [17] D. M. Drake and H. S. Baird, 'Distinguishing mathematics notation from english text using computational geometry,' in Eighth International Conference on Document Analysis and Recognition (ICDAR'05) . IEEE, 2005, pp. 1270-1274.- [18] L. Gao, X. Yi, Y. Liao, Z. Jiang, Z. Yan, and Z. Tang, 'A deep learningbased formula detection method for pdf documents,' in 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , vol. 1. IEEE, 2017, pp. 553-558.- [19] B. Chaudhuri and U. Garain, 'An approach for processing mathematical expressions in printed document,' in International Workshop on Document Analysis Systems . Springer, 1998, pp. 310-321.- [20] U. Garain and B. Chaudhuri, 'A syntactic approach for processing mathematical expressions in printed documents,' in Proceedings 15th International Conference on Pattern Recognition. ICPR-2000 , vol. 4. IEEE, 2000, pp. 523-526.- [21] A. Kacem,"
  },
  {
    "hash_code": "fdc5f67c65f33aa9d02b7a009c69213f",
    "text": ". 553-558.- [19] B. Chaudhuri and U. Garain, 'An approach for processing mathematical expressions in printed document,' in International Workshop on Document Analysis Systems . Springer, 1998, pp. 310-321.- [20] U. Garain and B. Chaudhuri, 'A syntactic approach for processing mathematical expressions in printed documents,' in Proceedings 15th International Conference on Pattern Recognition. ICPR-2000 , vol. 4. IEEE, 2000, pp. 523-526.- [21] A. Kacem, A. Bela¨ ıd, and M. B. Ahmed, 'Automatic extraction of printed mathematical formulas using fuzzy logic and propagation of context,' International Journal on Document Analysis and Recognition , vol. 4, no. 2, pp. 97-108, 2001.- [22] G. Nagy and S. Seth, 'Hierarchical representation of optically scanned documents,' in Proc. Seventh Int'l Conf. Pattern Recognition , Montreal, Canada, 1984, pp. 347-349.- [23] K. Simonyan and A. Zisserman, 'Very deep convolutional networks for large-scale image recognition,' arXiv preprint arXiv:1409.1556 , 2014.- [24] R. Girshick, J. Donahue, T. Darrell, and J. Malik, 'Rich feature hierarchies for accurate object detection and semantic segmentation,' in Proceedings of the IEEE conference on computer vision and pattern recognition , 2014, pp. 580-587.- [25] R. Girshick, 'Fast r-cnn,' in Proceedings of the IEEE international conference on computer vision , 2015, pp. 1440-1448.- [26] S. Ren, K. He, R. Girshick, and J. Sun, 'Faster r-cnn: Towards real-time object detection with region proposal networks,' in Advances in neural information processing systems , 2015, pp. 91-99.- [27] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, 'You only look once: Unified, real-time object detection,' in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 779788.- [28] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama et al. , 'Speed/accuracy trade-offs for modern convolutional object detectors,' in Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp. 73107311.- [29] M. Liao, B. Shi, X. Bai, X. Wang, and W. Liu, 'Textboxes: A fast text detector with a single deep neural network,' in Thirty-First AAAI Conference on Artificial Intelligence , 2017.- [30] M. Suzuki, F. Tamari, R. Fukuda, S. Uchida, and T. Kanahori, 'Infty: an integrated ocr system for mathematical documents,' in Proceedings of the 2003 ACM symposium on Document engineering . ACM, 2003, pp. 95-104.- [31] P. Mali, 'Scanning single shot detector for math in document images,' Master's thesis, Rochester Institute of Technology, Rochester, NY, USA, August 2019.- [32] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ ar, 'Focal loss for dense object detection,' in Proceedings of the IEEE international conference on computer vision , 2017, pp. 2980-2988.- [33] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, 'Imagenet: A large-scale hierarchical image database,' in 2009 IEEE conference on computer vision and pattern recognition . IEEE, 2009, pp. 248-255.- [34] Z. Yu, S. Lyu, Y. Lu, and P. S. Wang, 'A fusion strategy for the single shot text detector,' in 2018 24th International Conference on Pattern Recognition (ICPR) . IEEE, 2018, pp. 3687-3691."
  },
  {
    "hash_code": "449f1785fab917ae851b949d575d865d",
    "text": "IEEE, 2009, pp. 248-255.- [34] Z. Yu, S. Lyu, Y. Lu, and P. S. Wang, 'A fusion strategy for the single shot text detector,' in 2018 24th International Conference on Pattern Recognition (ICPR) . IEEE, 2018, pp. 3687-3691."
  },
  {
    "hash_code": "b5a07cc188d603b1e3e04144412c5e5d",
    "text": "## Self-supervised Reflective Learning through Self-distillation and Online Clustering for Speaker Representation LearningDanwei Cai, Zexin Cai, Ze Li, and Ming Li, Senior Member, IEEEAbstract -Speaker representation learning is crucial for voice recognition systems, with recent advances in self-supervised approaches reducing dependency on labeled data. Current twostage iterative frameworks, while effective, suffer from significant computational overhead due to repeated rounds of clustering and training. They also struggle with noisy pseudo labels that can impair model learning. This paper introduces self-supervised reflective learning (SSRL), an improved framework that addresses these limitations by enabling continuous refinement of pseudo labels during training. Through a teacher-student architecture and online clustering mechanism, SSRL eliminates the need for iterative training rounds. To handle label noise, we incorporate noisy label modeling and pseudo label queues that maintain temporal consistency. Experiments on VoxCeleb show SSRL's superiority over current two-stage iterative approaches, surpassing the performance of a 5-round method in just a single training round. Ablation studies validate the contributions of key components like noisy label modeling and pseudo label queues. Moreover, consistent improvements in pseudo labeling and the convergence of cluster counts demonstrate SSRL's effectiveness in deciphering unlabeled data. This work marks an important advancement in efficient and accurate self-supervised speaker representation learning through the novel reflective learning paradigm.Index Terms -Self-supervised learning, self-labeling, knowledge distillation, noisy label modeling, speaker recognition## I. INTRODUCTIONS PEAKER representation learning is a core component of voice recognition systems that aims to extract discriminative speaker characteristics from speech signals. Recent research has demonstrated significant advances in selfsupervised learning approaches for speaker representation learning [1]-[7]. These methods enable the utilization of unlabeled data, reducing dependency on manual annotation and facilitating deployment in practical applications.Our previous work introduced a two-stage iterative framework for unsupervised speaker representation learning [8], [9]. The first stage performs self-supervised speaker representation learning, while the second stage combines clustering with discriminative training. In this framework, clustering algorithms analyze the learned representations to generate pseudo labelsDanwei Cai and Zexin Cai are with the Department of Electrical and Computer Engineering, Duke University, Durham, NC, 27705, USA, e-mail: { danwei.cai, zexin.cai } @duke.eduZe Li and Ming Li are with Suzhou Municipal Key Laboratory of Multimodal Intelligent Systems, Digital Innovation Research Center, Duke Kunshan University, Kunshan, China and School of Computer Science, Wuhan University, Wuhan China, e-mail: { ze.li, ming.li369 } @dukekunshan.edu.cn Corresponding author: Ming Li.Fig. 1: A naive solution to bypass the iterative process of clustering and discriminative training in two-stage framework.flow chart![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000000_28303df7f2bf05a09ce0b6c10e2ed3ee8a6724eab264dfaed5d4e9367aa50ac1.png)for unlabeled data. These pseudo labels, despite containing some noise, are then used to train the network through discriminative learning. The process iterates, with each training round refining the representations and improving the quality of pseudo labels, leveraging DNNs' inherent robustness to label noise.However, this two-stage iterative approach, while effective, introduces significant computational overhead. The primary limitation stems from the iterative process of generating pseudo labels through clustering followed by discriminative training. Furthermore, the initial pseudo labels derived from clustering contain substantial noise, which impairs the model's ability to learn discriminative speaker features.To streamline learning and enhance efficiency, we sought to bypass the iterative nature of the two-stage framework by updating pseudo labels at every training step rather than after each full training round. As shown in Figure 1, A naive solution is to directly generate labels from model predictions e.g., assigning the class with the highest posterior probability. However, this approach can lead to a degenerate solution, where the model suffers from confirmation bias, collapsing to a single prediction across all samples [10].The key to avoiding this degenerate solution is to decouple training from label assignment, ensuring the model does not directly train on its own predictions. In the two-stage iterative framework, this decoupling is achieved by clustering pseudo labels after model convergence. To generate pseudo label at every training step while preventing degenerate solution, we propose self-supervised reflective learning (SSRL), which achieves this decoupling via self-supervised knowledge dis-tillation [10]-[13]. The framework employs a teacher model to generate pseudo labels for training a student model. The teacher model, updated as an exponential moving average (EMA) of the student, functions as an ensemble of past model states"
  },
  {
    "hash_code": "bb183253c952b256971b5018841ba28b",
    "text": "to decouple training from label assignment, ensuring the model does not directly train on its own predictions. In the two-stage iterative framework, this decoupling is achieved by clustering pseudo labels after model convergence. To generate pseudo label at every training step while preventing degenerate solution, we propose self-supervised reflective learning (SSRL), which achieves this decoupling via self-supervised knowledge dis-tillation [10]-[13]. The framework employs a teacher model to generate pseudo labels for training a student model. The teacher model, updated as an exponential moving average (EMA) of the student, functions as an ensemble of past model states, thereby stabilizing pseudo label generation and preventing overfitting. This mechanism enables continuous knowledge integration from previous training steps while maintaining stable learning dynamics. Essentially, the student model learns from its own reflection - a feedback-driven process where insights from previous training steps guide and improve future learning.Another limitation of the two-stage iterative framework is the high noise in pseudo labels, which degrades learning. We address this with two key strategies. First, we maintain a queue of historical pseudo labels to filter out outlier predictions and ensures consistent and reliable pseudo labels. Second, we integrate a label noise modeling strategy using a twocomponent Gaussian mixture model (GMM) to capture the loss distribution of training samples, as described by [14]. This approach leverages the tendency of DNNs to prioritize correct targets, providing a clean label probability for each sample, which is then used to adjust the final loss.This paper presents several contributions to the field of selfsupervised speaker representation learning:- 1) The introduction of a new learning paradigm, 'selfsupervised reflective learning', that bridges selfsupervised representation learning with self-supervised knowledge distillation and online clustering, eliminating iterative bottlenecks of the two-stage unsupervised framework.- 2) A detailed examination of how noisy label modeling, when combined with self-supervised knowledge distillation, can handle label noise, thereby improving the robustness of the learning process.- 3) Our approach surpasses existing iterative methodologies, marking a significant step forward in efficient and accurate unsupervised speaker representation learning.## II. RELATED WORKS## A. Self-supervised knowledge distillationIn self-supervised learning, knowledge distillation involves processing two distinct views through separate encoders and mapping one to the other using a predictor. A potential pitfall is the convergence of outputs to a uniform constant.To addressed this issue, 'bootstrap your own latent' (BYOL) introducing a momentum-based teacher network to generate targets for the student network [12]. Both networks process distinct views of the same instance through data augmentation. The student network aligns its outputs with the teacher network using a predictor, while the teacher network is updated through an EMA of the student network's weights. He et al. later introduced the 'simple siamese' (SimSam) approach, which simplified BYOL by removing the momentum mechanism [10], showing that while EMA wasn't essential, it could enhance performance.Similar to BYOL, 'self-distillation with no labels' (DINO) focuses on regression from student to momentum encoder representations [13]. However, DINO differs by using crossentropy instead of Mean square error (MSE) or cosine similarity for alignment, and by centralizing the teacher's output using a running mean with temperature-scaled softmax. With its large output dimension (65536 in the original paper), DINO effectively functions as an online clustering mechanism.Our proposed SSRL approch deviates from the DINO approach in multiple ways. Firstly, SSRL builds on a two-stage iterative unsupervised framework, starting with more reliable pseudo labels rather than random initialization. Unlike DINO's continuous class probability distributions, SSRL delivers discrete class predictions thus enables discriminative training of the model. Additionally, SSRL adopts noisy student training where only the student network undergoes augmentation and noise, allowing the teacher to generate higher quality pseudo labels. Lastly, SSRL integrates a pseudo label queue and noisy label modeling handle label inaccuracies.Self-supervised knowledge distillation has also been applied to speech representation learning [15]-[17]. For instance, DinoSR [16] adapts DINO for speech representation learning with sequence modeling, combining masked language modeling (MLM), self-distillation, and online clustering. While DinoSR focuses on frame-level pretraining using an explicit codebook for pseudo-label generation, our work targets utterance-level modeling of speaker characteristics.## B. Self-supervised pseudo labelingIn self-supervised learning, many approaches use clusteringderived pseudo labels for discriminative training. A primary approach, known as deep clustering (DC), combines conventional clustering with classification loss for network training [18]. However, DC faces several challenges: Firstly, the conventional off-the-shelf clustering requires feature extraction across the full dataset for every epoch. Secondly, the clustering alters cluster indexes across epochs, necessitating a reset of the parametric classifier, leading to unstable network training. Thirdly, The combination of"
  },
  {
    "hash_code": "1fce1ab65c513e7b2f8f4153e1b7caa8",
    "text": "explicit codebook for pseudo-label generation, our work targets utterance-level modeling of speaker characteristics.## B. Self-supervised pseudo labelingIn self-supervised learning, many approaches use clusteringderived pseudo labels for discriminative training. A primary approach, known as deep clustering (DC), combines conventional clustering with classification loss for network training [18]. However, DC faces several challenges: Firstly, the conventional off-the-shelf clustering requires feature extraction across the full dataset for every epoch. Secondly, the clustering alters cluster indexes across epochs, necessitating a reset of the parametric classifier, leading to unstable network training. Thirdly, The combination of discriminative and clustering losses can lead to degenerate solutions where all samples map to the same pseudo label [19]. DC avoids the issue by optimizing only one loss and keeping the other loss fixed between training epochs.Prototypical contrastive learning (PCL) addresses the cluster index permutation by replacing the classification layer with cluster centroids [20]. It generates class probabilities by contrasting samples with centroids and incorporates instance discrimination. However, PCL still requires per-epoch feature extraction and faces challenges with the divergence between evolving representations and fixed centroids.Online deep clustering (ODC) improves upon DC using sample and centroid memories for pseudo label generation [21]. It updates sample memory through moving averages and assigns labels based on nearest centroids. To prevent degenerate solutions, ODC implements 'merge-and-split' operations and loss re-weighting for small clusters.Recently, Asano et al. introduced a self-labeling algorithm (SeLa) addressing the degenerate solutions in combined clustering and representation learning [19]. Contraryto DC's direct clustering application, SeLa determines label assignments q ( y | x i ) from the network-derived class posterior probabilities p ( y | x i ) . Here, y denotes labels and x i denotes data samples. SeLa solves the cross-entropy optimization problem min q ∑ i ∑ y q ( y | x i ) log p ( y | x i ) with equal-sized partition constraints using the Sinkhorn-Knopp algorithm. While this unifies network training and clustering objectives, SeLa's offline cluster assignment limits its scalability.SwAV (Swapping Assignments between Views) advanced SeLa by introducing online clustering through optimal transport within mini-batches [22]. Instead of processing the full dataset, SwAV enforces consistency between cluster assignments from multiple augmented views of the same image. The swapped prediction task -predicting one view's code from another's representation - enables online training while maintaining equipartition constraints via the Sinkhorn-Knopp algorithm. Chang et al. apply SwAV to self-supervised speech representation learning by processing original speech and speaker-perturbed versions through shared encoders to create two views [23].Unlike the aforementioned methods which often rely on pseudo labels generated from randomly initialized feature representations, our approach harnesses the strength of a pretrained self-supervised model. This foundational difference enables our clustering module to produce semantic pseudo labels, sidestepping the pitfalls of arbitrary cluster assignments.## C. Self-training in semi-supervised learning and unsupervised domain adaptationIn semi-supervised learning scenarios, where there exists a limited labeled dataset complemented by a larger pool of unlabeled data, the objective is to harness the intrinsic structures or patterns prevailing within the unlabeled data to enhance the learning algorithm [24]. A prevalent approach to realizing this objective is self-training [25]-[30]. This method operates iteratively: initially, a model is trained using the available labeled data, serving as a basis to generate predictions on the unlabeled dataset. Predictions made with high confidence, termed pseudo labels, are integrated into the training set, forming an augmented dataset on which the model undergoes further training. This iterative cycle continues until convergence is achieved or a set number of iterations are completed. The core intent of self-training is to exploit the inherent but latent structures within the unlabeled data, thereby augmenting the model's capacity to generalize effectively.In the context of unsupervised domain adaptation (UDA), the labeled data are derived from a specific source domain, whereas the unlabeled data are from a distinct, but related, target domain. The pivotal challenge lies in adeptly fine-tuning the model, which has been preliminarily trained on the source domain, ensuring its optimized performance when applied to the target domain by effectively utilizing the unlabeled target data. The self-training method, due to its intrinsic reliance on unlabeled data, finds substantial applicability in UDA, seamlessly aligning with its fundamental principles [31]-[34].Numerous variants of the self-training technique have been innovated for semi-supervised learning and unsupervised do- main adaptation. For instance, one variant employs the teacherstudent architecture to impose a consistency regularization [35], [36]. Here, metrics such as the MSE or Kullback-Leibler divergence are commonly used to apply prior constraint assumptions on the unlabeled data. Central to consistency regularization is that the model's output remains robust to specific perturbations. Moreover, there exist models"
  },
  {
    "hash_code": "e5dbbf192fc00615bc683b494357f340",
    "text": "its intrinsic reliance on unlabeled data, finds substantial applicability in UDA, seamlessly aligning with its fundamental principles [31]-[34].Numerous variants of the self-training technique have been innovated for semi-supervised learning and unsupervised do- main adaptation. For instance, one variant employs the teacherstudent architecture to impose a consistency regularization [35], [36]. Here, metrics such as the MSE or Kullback-Leibler divergence are commonly used to apply prior constraint assumptions on the unlabeled data. Central to consistency regularization is that the model's output remains robust to specific perturbations. Moreover, there exist models like deep co-training [37] and Tri-Net [38], based on the disagreementbased paradigm. These models foster the simultaneous training of multiple models, leveraging the disagreements among them as a critical aspect of the learning process.In contrast to self-training, our proposed SSRL approach operates within a purely unsupervised setting, devoid of any reliance on labeled data. Unlike self-training, where the set of labels is predetermined, there is no prior knowledge of class counts or explicit label information in the unsupervised setting. The proposed SSRL method uses the teacher-student framework, and the teacher provides pseudo labels based on an online clustering mechanism. This dynamic mechanism fosters the creation of evolving clusters, which are adaptable and capable of undergoing refinements throughout the learning process. Thus, the SSRL method, with its capacity for dynamic clustering, promises enhanced performance and robustness in unsupervised learning scenarios.## D. Speaker representation learningBefore the emergence of deep learning, speaker representation learning primarily relied on statistical modeling techniques. Among these, the Gaussian Mixture Model - Universal Background Model (GMM-UBM) was widely used to model speaker features probabilistically [39]-[41]. This approach was later enhanced by i-vector representations, which leveraged factor analysis to map speaker characteristics into a lowdimensional space, improving speaker recognition and verification performance [42].With the advent of deep neural networks, speaker modeling transitioned to data-driven feature extraction, enabling more robust and discriminative speaker embeddings. One of the earliest breakthroughs was the x-vector framework, which introduced Time-Delay Neural Networks (TDNNs) along with a statistics pooling layer to aggregate features at the utterance level [43], [44]. This significantly improved robustness to variable-length speech segments. Following this, ResNet-based architectures were applied to speaker modeling, incorporating convolutional layers to effectively capture local speaker features [45], [46]. Further advancements led to ECAPATDNN, which introduced channel-wise attention mechanisms and multi-scale feature learning, enhancing speaker discriminability while maintaining a compact model size [47]. More recently, Transformer-based architectures, such as Conformer, have been explored, integrating self-attention mechanisms with convolutional layers to better capture both global and local speaker characteristics [48]-[50].A major paradigm shift in speaker modeling has been the adoption of large-scale self-supervised pre-trained models. Models such as wav2vec 2.0 [51], HuBERT [52], and WavLM [53] have been utilized as feature extractors and fine-tuned forflow chartThe diagram shows the stages of a process.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000001_92c71f147869ee2c95b13307162b73fcdccd49814422f4b8d1b38f2d2ec9941f.png)- (b) Improved two-stage framework with SSRL.Fig. 2: A csomparison of the proposed SSRL method with our previously proposed two-stage method with iterative training.flow chartThe diagram shows the process of a data augmentation process.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000002_a9443bde75c0c4f3df66b73ff14a94f7e05c3386d0e162ce3efff5d3e95608f2.png)Fig. 3: The proposed self-supervised reflective learning (SSRL) method.speaker-related tasks [1], [54], [55]. These approaches leverage self-supervised learning as a pretraining method, allowing for scalable and data-efficient speaker modeling. By learning from vast amounts of unlabeled speech data, these models significantly enhance generalizability and reduce reliance on manually labeled datasets.## E. Two-stage iterative framework for unsupervised speaker representation learningMulti-stage unsupervised learning frameworks have been widely adopted in various domains, including hyperspectral image processing [56]-[58], where structured priors and iterative refinements enhance representation quality. In speaker representation learning, a two-stage iterative framework has been proposed to"
  },
  {
    "hash_code": "33538fa01a9e72c78ea3df7797e7d85e",
    "text": ".speaker-related tasks [1], [54], [55]. These approaches leverage self-supervised learning as a pretraining method, allowing for scalable and data-efficient speaker modeling. By learning from vast amounts of unlabeled speech data, these models significantly enhance generalizability and reduce reliance on manually labeled datasets.## E. Two-stage iterative framework for unsupervised speaker representation learningMulti-stage unsupervised learning frameworks have been widely adopted in various domains, including hyperspectral image processing [56]-[58], where structured priors and iterative refinements enhance representation quality. In speaker representation learning, a two-stage iterative framework has been proposed to leverage large-scale unlabeled data efficiently [8], [9], [59]. In the first stage, self-supervised methods are used to extract initial speaker embeddings and pseudo labels. The second stage involves an iterative discriminative training process to refine these embeddings.To enhance stage one, more advanced self-supervised representation learning such as DINO are proposed [60]. Zhao et al. [61] further refined DINO with the Prototype Division method, effectively mitigating speaker confusion and enhancing overall performance. Addtionally, Tao et al. [62] proposed a multimodal contrastive learning technique using diverse positive pairs by cross-referencing speech and face data, improving the robustness of speaker encoders.A significant challenge in this framework is the presence of noisy pseudo labels. To address this, several methods have been developed. Tao et al. [63] introduced a technique that extracts reliable labels based on the neural network's fitting ability during training. Han et al. [64] and Zhou et al. [5] proposed using a Gaussian Mixture Model (GMM) to dynamically model loss distribution, distinguishing between reliable and unreliable labels, and correcting the unreliable ones using model predictions. Chen et al. [65] developed a method that coordinates information between audio and visual modalities through an 'update by disagreement' strategy, improving pseudo label quality by leveraging inter-modal disagreements.While these methods improve pseudo label quality, they typically require iterative training stages. Fang et al. [66] employed a label ensemble approach to smoothly correct noisy speaker labels by the exponential moving average of model predictions at each training epoch. Similarly, the approach presented in this paper dynamically improves pseudo labels at each epoch, eliminating the need for multiple training rounds and enhancing both efficiency and accuracy in speaker representation learning.## III. METHODSThis section introduces the self-supervised reflective learning (SSRL) approach, which improves the two-stage iterative framework [8]. In the original two-stage framework, the first stage applies self-supervised representation learning and generates initial pseudo labels. The second stage consists of multiple training rounds, each comprising: (1) pseudo-label generation through clustering, and (2) discriminative training using these labels.While maintaining the first stage unchanged, the proposed SSRL method replaces the multi-round process with continuous label refinement during a single training phase. As illustrated in Figure 2, SSRL requires an initialization step to ensure stable pseudo-label generation. This can be achieved either through brief discriminative training using Stage 1 pseudo labels, or by directly employing the Stage 1 selfsupervised model as the encoder with a predictor initialized using pseudo cluster centroids. Following initialization, SSRL dynamically updates pseudo labels during training through its reflective learning mechanism.Figure 3 illustrates the proposed SSRL method, with the detailed procedure outlined in Algorithm 1.## Algorithm 1 Self-Supervised Reflective Learning (SSRL)```&Algorithm 1 Self-supervised Reflective Learning (SSRL)      &      Require:  Unlabeled  dataset  D  =  {x;|i =  1, . . , N};  Initial  pseudo  labels  )  =  {y;|i =          1, . . , N};  Teacher encoder  $, with parameters  $;;;;;;;;;;;;;;;;;;;;;;;;;;;;          &  $v;  Student encoder  $, with parameters  $;;;;;;;;;;;;;;;;;;;;;;;;;;;;      1:  procedure  REFLECTIVELEARNING(D, )}      2:      Train student network ( $, and  h$_) with dataset {D, }} for  E1,  epochs      3:      Initialize  $t,  with  $s and  h$_ with  h$_      4:      Q;  <- Queue (length = L)      5:      Pclean(E,t) ;  <- 1                        > Initialize  empty  pseudo  label  queue  for  all  samples      6:         for  epoch in 1 to  E2  do            for     batch  B = {x;i =  1, . . , B}  in  D  do               Crop a short  segment  for each  training  sample  B$_  =  {x'}               Apply data  augmentation  to  B$_               $,  $,  $ <-  softmax(h$_ o"
  },
  {
    "hash_code": "c1db9736891b6b041911a26b661d5186",
    "text": "= L)      5:      Pclean(E,t) ;  <- 1                        > Initialize  empty  pseudo  label  queue  for  all  samples      6:         for  epoch in 1 to  E2  do            for     batch  B = {x;i =  1, . . , B}  in  D  do               Crop a short  segment  for each  training  sample  B$_  =  {x'}               Apply data  augmentation  to  B$_               $,  $,  $ <-  softmax(h$_ o $ (s'))               $,  $ <-  - 1  =  1  Pclean(L,t) i)  1:               Crop a long  segment  for each  training  sample  B$_  =  {x'}               $,  $,  $ <-  softmax(h$_ o $ (t'))               $,  $ <-  clustering  (t,t,i)               $,  $ <-  to  1  =  Q               $,  $  -  mode  of  labels  in  Q,               $,  t,i  <-  -  logpt (y;i |x)               &               Update  student  parameters  using  gradients  from  L               $,  $,  $  -  +  (  1  -  )  )               $,  $  -  +  t + (  1  -  )  )               for  2:            Fit  {log ,l,i} =  1, . . , N }  with  a  GMM               Fit  (update  Pclean(L,t)  using  GMM  2:    end  for  .  2: end  procedure -----------------------------------------------------------------------A  Self-supervised  knowledge  destination           texture  to  the  teacher  -  produce  the  feature  zs  and  the  class  At  the  heart  of  our  approach  is  the  self-supervised  knowl-edge  destination  technique.  Given  an  unlabeled  dataset,  the    the  supervalence  of  the  pseudo  label  or  the  derived  from  the  teacher:```## A. Self-supervised knowledge distillationAt the heart of our approach is the self-supervised knowledge distillation technique. Given an unlabeled dataset, the teacher network generates cluster assignments which guide the training of the student network. The teacher encoder, represented as Φ t ( · ) , transforms the data sample x into a D -dimensional feature representation z t ∈ R D :$$z _ { t } = \\Phi _ { t } ( x ) \\quad \\ \\ ( 1 ) \\quad \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "b1b27bc57ce5e098527f437b7def644d",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$Subsequently, a linear predictor, h t ( · ) , is employed to compute the probability distribution over K clusters via a softmax operator. Let p t ( k | x ) denotes the posterior probability that the sample x belongs to the k th cluster, the vector p t aggregates these probabilities for all K clusters:$$p _ { t } = \\text {softmax} ( h _ { t } ( z _ { t } ) ) = \\text {softmax} ( h _ { t } \\circ \\Phi _ { t } ( \\mathbf x ) ) \\quad ( 2 ) \\quad \\text {truncation}$$where p t ( k | x ) is the k th element of p t . An online clustering mechanism then extracts cluster assignments y ∈ { 1 , 2 , · · · , K } from p t for the training sample x .Following a parallel structure, the student encoder Φ s ( · ) , coupled with the student predictor h s ( · ) - analogous in archi- tecture to the teacher - produce the feature z s and the class prediction p s from another view of the same input x ′ . The student model's training utilizes the cross-entropy loss, under the supervision of the pseudo label y derived from the teacher:$$\\mathcal { L } = - \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\log p _ { s } ( y _ { i } | x _ { i } ^ { \\prime } )$$where N represents the number of data samples in a training batch.1) Enhancing the student's model capacity: Drawing inspiration from the noisy student method in semi-supervised learning [30], our approach amplifies the student's modeling capacity by imposing noise into the training samples during the student's training. Specifically, a short segment is extracted from the training utterance, followed by data augmentation techniques introducing background noise or convolutional reverberation to this segment. Consequently, the student model processes these augmented snippets. The teacher model, on the other hand, processes a longer clip of the same utterance in its unaltered form, facilitating the generation of stable pseudo labels. Morever, other deep neural network trainingstrategies can further improve the student's model capacity. For instance, employing dropout can mitigate the risk of overfitting to the imprecise pseudo labels [67]. Another approach involves the use of angular margin-based cross entropy [68] as a loss function, fostering the student model to capture a more discerning feature space.2) Teacher model update mechanism: Traditional knowledge distillation typically employs a teacher network, trained with labeled data and possessing superior model capacity. However, under self"
  },
  {
    "hash_code": "b78655481301ec86b4ca4223483c18ad",
    "text": "longer clip of the same utterance in its unaltered form, facilitating the generation of stable pseudo labels. Morever, other deep neural network trainingstrategies can further improve the student's model capacity. For instance, employing dropout can mitigate the risk of overfitting to the imprecise pseudo labels [67]. Another approach involves the use of angular margin-based cross entropy [68] as a loss function, fostering the student model to capture a more discerning feature space.2) Teacher model update mechanism: Traditional knowledge distillation typically employs a teacher network, trained with labeled data and possessing superior model capacity. However, under self-supervised settings, acquiring such a pretrained teacher model is not feasible. We hypothesize that the student model's capacity undergoes enhancement after each training cycle, courtesy of noisy student training. As such, an advanced teacher model can be obtained by ensembling student models from previous training steps. In specific terms, we employ an EMA technique on the student's parameters to refine the teacher model [12], [13], [69] . Denoting the parameters of student encoder Φ s ( · ) as ϕ s and the parameters of student predictor h s ( · ) as ψ s , the teacher's parameters ϕ t and ψ t undergo an update as:$$\\phi _ { t } & \\leftarrow \\lambda \\phi _ { t } + ( 1 - \\lambda ) \\phi _ { s } \\\\ \\psi _ { t } & \\leftarrow \\lambda \\psi _ { t } + ( 1 - \\lambda ) \\psi _ { s }$$where λ ∈ [0 , 1) serves as a momentum coefficient. Through the EMA update mechanism, the teacher consistently outperforms the student during the training process, thereby facilitating the student's learning by providing pseudo labels of higher quality.## B. Online clusteringIn the cluster assignment task, the objective is to maximize the alignment of the cluster assignments q ( k | x i ) with the predicted class probabilities p t ( k | x i ) provided by a teacher model, ensuring that each data point is assigned to the cluster where it best fits according to these predictions.$$\\max _ { q } \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\sum _ { k = 1 } ^ { K } q ( k | x _ { i } ) p _ { t } ( k | x _ { i } ) & & \\text {strict} \\\\ \\intertext { s u c t } \\text {subject to } \\forall k \\colon q ( k | x _ { i } ) \\in \\{ 0 , 1 \\} \\text { and } \\sum _ { k = 1 } ^ { K } q ( k | x _ { i } ) = 1 & & \\text {Uc} _ { C } \\\\ & & \\text {where }$$To address this, we explore two online clustering methodologies:1) Direct maximum probability assignment: The most intuitive method generates cluster assignment based on the highest predicted probability class from the teacher:$$q ( k | \\mathbf x _ { i } ) = \\delta \\left ( k - \\arg \\max _ { j } p _ { t } ( j | \\mathbf x _ { i } ) \\right ) \\quad \\quad ( 6 ) \\quad \\text {we are}$$Here, δ ( k -arg max j p t ( j | x i )) is the Kronecker delta function, defined as 1 when k = arg max j p t ( j | x i ) and 0 otherwise. Essentially, each data sample is allocated to the cluster corresponding to the class that the teacher model is most confident in.2) Cluster assignment through optimal transport: Drawing inspiration from SeLa [19], we introduce an added constraint to the objective in Equation 5, ensuring that the N training samples are distributed evenly across the K clusters:$$\\max _ { q } \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\sum _ { k = 1 } ^ { K } q ( k | x _ { i } ) p _ { t } ( k | x _ { i } ) \\\\ \\intertext { y } \\text {subject to } \\forall k \\colon q ( k | x _ { i } ) \\in \\{ 0 , 1 \\} , \\sum _ { i = 1 } ^ { N } q ( k | x _ { i } ) = \\frac { N } { K } \\\\ \\intertext { e } \\text {such constraints ensure a distinct label for every data point and }$$Such constraints ensure a distinct label for every data point and a uniform distribution of the N samples over the K classes, preventing identical pseudo labeling"
  },
  {
    "hash_code": "5f5aedb5ddcb88750937d2e5ee3075f9",
    "text": "_ { i } ) \\\\ \\intertext { y } \\text {subject to } \\forall k \\colon q ( k | x _ { i } ) \\in \\{ 0 , 1 \\} , \\sum _ { i = 1 } ^ { N } q ( k | x _ { i } ) = \\frac { N } { K } \\\\ \\intertext { e } \\text {such constraints ensure a distinct label for every data point and }$$Such constraints ensure a distinct label for every data point and a uniform distribution of the N samples over the K classes, preventing identical pseudo labeling for all training samples.Building on the perspective of SeLa [19], the optimization problem depicted in Equation 7 can be mapped to an optimal transport problem [70]. To understand this, let's define P as the K × N matrix where P ki = 1 N p t ( k | x i ) , and Q as the K × N matrix of assigned joint probabilities between a and b with Q ki = 1 N q ( k | x i ) . Following the notation in [70], Q can be conceptualized as an element of the transportation polytope:$$U ( r , c ) \\colon = \\{ Q \\in \\mathbb { R } _ { + } ^ { K \\times N } | Q \\mathbb { 1 } = r , Q ^ { T } 1 = c \\}$$where 1 is the vector of all ones of appropriate dimension. Based on the given constraints, we get:$$r = \\frac { 1 } { K } \\cdot 1 ; \\quad c = \\frac { 1 } { N } \\cdot 1$$Given matrices P and Q , the objective function in Equation 7 can be recast as:$$\\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\sum _ { k = 1 } ^ { K } q ( k | x _ { i } ) p _ { t } ( k | x _ { i } ) = \\langle Q , P \\rangle$$where ⟨·⟩ is the Frobenius dot-product between two matrices. Consequently, Equation 7 can be translated into an optimal transport problem between r and c with a cost of -P :$$\\min _ { Q \\in U ( \\mathbf r , \\mathbf c ) } \\langle Q , - P \\rangle$$To expedite the optimal transport solver, an entropic constraint was integrated into the classical optimal transport problem as introduced by Cuturi [70]. This regularization of the problem is defined by:$$U _ { \\alpha } ( { r } , { c } ) \\coloneqq \\left \\{ Q \\in U ( { r } , { c } ) \\ | \\ K L ( Q \\| { r } { c } ^ { T } ) \\leq \\alpha \\right \\}$$where KL represents the Kullback-Leibler divergence. Given the concavity of entropy, we have U α ( r , c ) ⊂ U ( r , c ) . Consequently, the optimal transport problem (as shown in Equation 11) is reframed as:$$\\min _ { Q \\in U _ { \\alpha } ( \\mathbf r , \\mathbf c ) } \\langle Q , - P \\rangle$$Introducing a Lagrange multiplier for the entropy constraint, we arrive at the dual optimization problem:$$\\min _ { Q \\in U ( r , c ) } \\langle Q , - P \\rangle + \\frac { 1 } { \\lambda } K L ( Q \\| r c ^ { T } )$$From the Lagrangian of Equation 14, we can express the minimizer of Equation 14 as:$$Q = \\text {diag} ( \\mathbf u ) e ^ { \\lambda P } \\text {diag} ( \\mathbf v )$$Fig. 4: Histogram of the cross entropy loss between the model's prediction and the pseudo label. (a) and (b) are produced by the model trained with the initial pseudo label without applying the proposed SSRL approach. (c) and (d) are produced by the teacher model after 30 epochs of SSRL.bar chartThe image is a collection of four graphs, each representing a different scenario or model. The graphs are labeled with (a) Initial Model, Linear Scale, (b) Initial Model, Log Scale, (c) Reflective Learning, Linear Scale, and (d) Reflective Learning, Log Scale. Each graph has a title and a legend, with the legend items being \"correctly labeled\", \"mislabeled\", and \"all\". The x-axis of each graph is labeled \"Loss\" and the y-axis is labeled \"Density\". The graphs show a comparison between"
  },
  {
    "hash_code": "bc5f0ea5e575166f8a4ca6e377c9797b",
    "text": "teacher model after 30 epochs of SSRL.bar chartThe image is a collection of four graphs, each representing a different scenario or model. The graphs are labeled with (a) Initial Model, Linear Scale, (b) Initial Model, Log Scale, (c) Reflective Learning, Linear Scale, and (d) Reflective Learning, Log Scale. Each graph has a title and a legend, with the legend items being \"correctly labeled\", \"mislabeled\", and \"all\". The x-axis of each graph is labeled \"Loss\" and the y-axis is labeled \"Density\". The graphs show a comparison between the \"correctly labeled\" and \"mislabeled\" scenarios, with the \"all\" scenario being the baseline. The graphs are color-coded, with \"correctly labeled\" in blue, \"mislabeled\" in orange, and \"all\" in green. The graphs are arranged in a 2x2 grid, with the top left graph being (a) and the bottom right graph being (d).![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000003_bb508bca5c8c40b4809348e79790856a02eb91bfcbcaf7a0d56ffe0e41d6732e.png)In the equation, exponentiation is carried out element-wise. Additionally, u and v are two non-negative vectors that serve as scaling coefficients, ensuring the resulting matrix Q adheres to the probability matrix standards.The Sinkhorn-Knopp algorithm is employed to determine the optimal Q . This algorithm iteratively adjusts the rows and columns of the matrix utilizing diagonal matrices until a convergence point is reached:$$\\forall k \\colon \\mathbf u _ { k } \\leftarrow \\frac { \\mathbf r _ { k } } { [ e ^ { \\lambda P } \\mathbf v ] _ { k } } ; \\quad \\forall i \\colon \\mathbf v _ { i } \\leftarrow \\frac { \\mathbf c _ { i } } { [ \\mathbf u ^ { T } e ^ { \\lambda P } ] _ { i } } \\quad ( 1 6 ) \\quad \\text {labels} \\ \\{ \\mathbf u ^ { T } e ^ { \\lambda P } \\} _ { i }$$To generate pseudo labels using the Sinkhorn-Knopp algorithm, we employ a batched approach. Specifically, we accumulate the matrix P over M batches with batch size B , ensuring total number of training samples N = M × B is larger than the number of cluster K . Every M batches, we update the cluster assignments utilizing the SinkhornKnopp algorithm. This method provides a computationally efficient way to handle large datasets, ensuring consistent and optimized pseudo-label assignments in line with the teacher's predictions.Either with direct maximum probability assignment or optimal transport-based cluster assignment, the assigned clusters are determined based on the teacher model's predictions, without constraints ensuring that every output class will receive data samples. As training progresses, clusters with extremely low prediction confidence shrink and eventually disappear. This phenomenon is observed in our experiments, as described later in Section V-B and Figure 6, where the number of active clusters gradually decreases during training until a stable count is reached.SSRL naturally maintains stable and meaningful cluster assignments throughout training. Starting from initial pseudo labels, the online clustering continuously refines assignments as the teacher model's discrimination ability improves. When a sample's current assignment becomes suboptimal, the teacher model reassigns it based on learned representations. Online clustering works organically with EMA updates - EMA ensures smooth model evolution while preserving previous knowledge, enabling stable and consistent refinements to pseudo labels.## C. Pseudo label correctionTo further refine the pseudo label generation process, we introduce a label correction mechanism employing a pseudo label queue. This queue retains a history of pseudo labels previously generated by the teacher model for each training sample. With a predetermined fixed length L , the queue ensures consideration only of the most recent L predictions. To filter out sporadic or outlier predictions and cultivate a robust pseudo label, we employ a statistical mode evaluation of the labels within the queue. This ensures that the most frequently occurring label in the recent history is selected as the final pseudo label, thereby enhancing the reliability of the label assignment and mitigating the effects of transient erroneous predictions.## D. Noisy label modelingTo mitigate the challenges posed by noisy pseudo labels, our framework incorporates a strategy to model label noise following the approach in [14]. Prior research [14] has shown that deep neural networks (DNNs) tend to learn correctly labeled samples first before gradually fitting to mislabeled ones. As a result, mislabeled samples typically exhibit higher loss values compared to correctly labeled ones, allowing us to leverage this property for noise modeling.Figure 4 shows an"
  },
  {
    "hash_code": "2d47c981153944556540bdd2a83b0b0e",
    "text": "recent history is selected as the final pseudo label, thereby enhancing the reliability of the label assignment and mitigating the effects of transient erroneous predictions.## D. Noisy label modelingTo mitigate the challenges posed by noisy pseudo labels, our framework incorporates a strategy to model label noise following the approach in [14]. Prior research [14] has shown that deep neural networks (DNNs) tend to learn correctly labeled samples first before gradually fitting to mislabeled ones. As a result, mislabeled samples typically exhibit higher loss values compared to correctly labeled ones, allowing us to leverage this property for noise modeling.Figure 4 shows an illustration of such behavior. Since the pseudo label is estimated in the self-supervised setting, we do not have the ground truth references for the correct and incorrect labels. To estimate this noisy label information, we employ the Hungarian algorithm, mapping the pseudo labels to the ground truth labels. Figure 4 exhibit a bimodal distribution with two distinct peaks of the logarithmically scaled losses. By modeling this loss distribution, we can effectively segregate accurately labeled data from the mislabeled, which then aids in computing cleaner label probabilities for the training set.To achieve this, we use a two-component GMM to model the logarithmically scaled losses generated by the teacher model. Mathematically, the mixture model can be expressed as:$$p ( \\ell _ { t } ) = & \\, \\pi \\cdot \\mathcal { N } ( \\log ( \\ell _ { t } ) ; \\mu _ { 1 } , \\sigma _ { 1 } ^ { 2 } ) + \\\\ & ( 1 - \\pi ) \\cdot \\mathcal { N } ( \\log ( \\ell _ { t } ) ; \\mu _ { 2 } , \\sigma _ { 2 } ^ { 2 } )$$For sample x i , ℓ t,i represents the cross entropy loss between the teacher's prediction and its pseudo label. The term p ( ℓ t )represents the probability distribution of log( ℓ t ) . The coefficient π is the mixture weight, and N (log( ℓ t ); µ, σ 2 ) is the Gaussian distribution parameterized by mean µ and variance σ 2 .The GMM aids in distinguishing between the loss distributions of clean labels and those of noisy labels. After establishing this loss distribution model, a clean label probability is assigned to each training sample as:$$p _ { \\text {clean} } ( \\ell _ { t } ) = \\frac { \\pi \\cdot \\mathcal { N } ( \\log ( \\ell _ { t } ) ; \\mu _ { 1 } , \\sigma _ { 1 } ^ { 2 } ) } { p ( \\ell _ { t } ) } \\quad \\ \\ ( 1 8 ) \\quad \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "9058b97b78626f2b961713a79316a7df",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"
  },
  {
    "hash_code": "e5b2db788e721eafa9ec810952ebb510",
    "text": "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$Given that samples with clean labels yield lower losses, the Gaussian component N (log( ℓ t ); µ 1 , σ 2 1 ) associated with these samples has a smaller mean, i.e., µ 1 &lt; µ 2 . Utilizing the clean label probability, p clean ( ℓ t ) , the final loss is adjusted, directing the model to give greater emphasis to samples deemed to have accurate labels:$$\\mathcal { L } = - \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } p _ { \\text {clean} } ( \\ell _ { t , i } ) \\log p _ { s } ( y _ { i } | x _ { i } ^ { \\prime } ) \\quad ( 1 ) \\quad \\text {in}$$Combining all the methods discussed above, Algorithm 1 presents the complete procedure of the proposed SSRL method. Here, we apply the direct maximum probability assignment as the online clustering method. It can easily be extended to the optimal transport method.## IV. EXPERIMENTAL SETUPS## A. DataThe experiments are conducted on the VoxCeleb dataset [71], [72]. For model training, we use the development set of VoxCeleb 2, which contains 1,092,009 audio files from 5,994 speakers. While speaker identity labels are available, they are only used for experimental analysis and not for model training.For evaluation, we report speaker verification results using three trial lists from the V oxCeleb 1 dataset as defined in [72]:- VoxCeleb 1-O: The original trial list with 37,720 trials from 40 speakers.- VoxCeleb 1-E: An extended trial list with 581,480 trials from 1,251 speakers.- VoxCeleb 1-H: A hard trial list with 552,536 trials from 1,190 speakers, where all test pairs share the same language and gender.## B. Data AugmentationData augmentation is effective for deep speaker representation learning in both supervised learning [73] and contrastive self-supervised learning [74]-[76]. We utilized two primary strategies:- Additive noise augmentation: The MUSAN dataset [77] was used as our noise source, adding ambient noise, musical sounds, and babble noise to our audio files. Babble noise was generated by merging three to eight separate speech files from the MUSAN dataset, with signal-to-noise ratios (SNR) ranging from 0 to 20 dB.- Convolutional reverberation noise augmentation: We used 40,000 simulated room impulse responses (RIR) from small to medium-sized rooms, as described in [78].To maintain variability during training, we applied on-the-fly data augmentation. In SSRL training, the student network was trained with two-thirds of the data augmented utterances, while the teacher network used unaltered speech data.## C. Implementation detailsWe evaluate the proposed methods on two different network architectures for speaker representation learning: ResNet [79] and ECAPA-TDNN [47]. The baseline method used for comparison is the two-stage iterative framework. For each network architecture, a supervised model is trained to serve as a reference point (upper bound) for model performance, using the same training hyperparameters as those in the second stage of the two-stage iterative framework.1) ResNet - two-stage iterative framework: We first use the two-stage iterative framework trained on ResNet [79] as the"
  },
  {
    "hash_code": "082d6624572f8baf0bc2ccbbd4493d68",
    "text": "the teacher network used unaltered speech data.## C. Implementation detailsWe evaluate the proposed methods on two different network architectures for speaker representation learning: ResNet [79] and ECAPA-TDNN [47]. The baseline method used for comparison is the two-stage iterative framework. For each network architecture, a supervised model is trained to serve as a reference point (upper bound) for model performance, using the same training hyperparameters as those in the second stage of the two-stage iterative framework.1) ResNet - two-stage iterative framework: We first use the two-stage iterative framework trained on ResNet [79] as the baseline, following our previous research on the two-stage iterative framework [8], [9].In the first stage, we apply contrastive self-supervised learning (CSL) [76] to learn speaker representations. In the second stage (iterative training), initial pseudo labels for the training dataset are generated using K-means clustering on speaker embeddings from CSL. The number of clusters is set to 6,000, the same as in [9], where it was determined using the elbow method. For network architecture, hyperparameters, and other training details, readers can refer to [9].2) ResNet - improved two-stage framework with SSRL: For the improved two-stage framework with SSRL trained on ResNet, the first stage remains the same as in the twostage iterative framework. To initiate second-stage training, the number of clusters for K-means is set to 8,000, which is higher than the 6,000 clusters used in the two-stage iterative framework. This adjustment is made for two reasons: (1) The elbow method identifies a reasonable cluster count between 5,000 and 8,000 [9]. (2) As discussed in Section III-B, the cluster count naturally decreases due to the online clustering process. Setting a higher initial cluster count ensures sufficient granularity, allowing the model to refine pseudo labels without collapsing clusters too early.In the second stage, to initialize SSRL training, the ResNetbased speaker embedding network is trained for 55 epochs with initial pseudo labels. A cosine annealing scheduler adjusts the learning rate from 1e-3 to 1e-5, including a 5-epoch warmup phase. The batch size is set to 512, and the Adam optimizer is applied.During the SSRL training phase, audio waveforms are cropped to 2 seconds for the student model and 6 seconds for the teacher model. The student network is trained for 100 epochs using the Adam optimizer, with the learning rate scheduled via cosine annealing from 5e-4 to 1e-5. The loss function used is cross entropy. The pseudo label queue length is set to 5 unless stated otherwise. The EMA momentum parameter, denoted as λ in Equation 4, linearly increases from 0.999 to 0.9999 during SSRL training.3) ECAPA-TDNN -two-stage iterative framework: To compare with other studies, we also adopt the ECAPA-TDNNbased speaker embedding network [47] as an alternative backbone.TABLE I: Comparison of two self-supervised pretrained models. EER is evaluated on VoxCeleb 1-O; labeling metrics are based on k-means clustering with 8,000 clusters.| Pretrained Method   | Network Architecture   | #Parameters EER   | ↓            | NMI   | Accuracy ↑   | ↑ Purity ↑   ||---------------------|------------------------|-------------------|--------------|-------|--------------|--------------|| CSL                 | ResNet                 | 1.37M             | 8.86% 0.7744 |       | 36.87%       | 55.32%       || DINO                | ECAPA-TDNN             | 63.65M 1          | 2.94% 0.9319 |       | 65.26%       | 88.52%       |For the first stage self-supervised training, ECAPA-TDNN speaker embedding network [47] is pretrained with DINO [13]. Following the structure in [2], the ECAPA-TDNN network has channels sequenced as 1024, 1024, 1024, 1024, and 3072 across the initial TDNN layer and four TDNN blocks. After the ECAPA-TDNN encoder, we use attentive statistical pooling followed by a 512-dimensional fully connected layer for speaker embeddings. The DINO projection head includes four fully connected layers with hidden dimensions of 2048, 2048, 8192, and 256, ending with a 65536-dimensional weight-normalized fully connected layer. We employ multicrop data augmentation, giving the EMA teacher two 4-second data-augmented views and the student four 2-second dataaugmented views for each training sample.The DINO pretraining uses a stochastic gradient descent (SG"
  },
  {
    "hash_code": "6172e0a1cb4d6f1bddb46387f2da8f6f",
    "text": "NN layer and four TDNN blocks. After the ECAPA-TDNN encoder, we use attentive statistical pooling followed by a 512-dimensional fully connected layer for speaker embeddings. The DINO projection head includes four fully connected layers with hidden dimensions of 2048, 2048, 8192, and 256, ending with a 65536-dimensional weight-normalized fully connected layer. We employ multicrop data augmentation, giving the EMA teacher two 4-second data-augmented views and the student four 2-second dataaugmented views for each training sample.The DINO pretraining uses a stochastic gradient descent (SGD) optimizer over 100 epochs, with a cosine annealing scheduler modulating the learning rate from 0.2 to 1e-5, including a 10-epoch warm-up phase. The temperature hyperparameters for cross-entropy are set to 0.04 for the teacher and 0.1 for the student. For more detailed training procedures, refer to [13] and [2].The comparison of different first stage models used in this work can be found in Table I. Unlike random initialization, stage 1 provides a structured representation for clustering, enabling the first clustering round to generate more reliable pseudo labels. This improves the quality of subsequent second stage training, ensuring the model refines meaningful speaker representations rather than noise.In the second stage of iterative training, pseudo labels are generated by applying K-means clustering to the speaker embeddings from the previous training round, targeting 8,000 clusters. Each training round employs the Adam optimizer with a batch size of 480, and the learning rate is managed by a cosine annealing scheduler, transitioning from 1e-4 to 1e-5 over 40 epochs.To ensure stable training, we initialize the encoder's parameters with DINO pre-trained parameters for the first training round. In subsequent training rounds, the encoder retains the parameters from the previous training round. The predictor, i.e., the final linear layer for speaker classification, is reinitialized using K-means cluster centers.4) ECAPA-TDNN - improved two-stage framework with SSRL: For the improved two-stage framework with SSRL1 The ECAPA-TDNN encoder has a total of 22.73 million parameters. The DINO projection head contains 40.92 million parameters. The projection head is only used during DINO training; speaker embeddings are extracted from the output of the ECAPA-TDNN encoder.trained on ECAPA-TDNN, the first stage remains the same as in the two-stage iterative framework.In the second stage, we directly initialize both the student and teacher networks using DINO-pretrained parameters and apply SSRL. The predictor, a single linear layer, has its weights initialized with the 8,000 K-means cluster centers, while the biases are set to zero. The training batch size for the ECAPA-TDNN model is 480, and other training configurations for SSRL remain the same as those for the ResNet-based pipeline. For the training objective, in addition to cross-entropy loss, we train another ECAPA-TDNN with SSRL using the additive angular margin (AAM) loss [68] to further enhance the model's capacity. The AAM loss margin is set to 0.2, and the scaling factor is 32.## D. Evaluation metric1) Speaker verification evaluation: We assess the effectiveness of speaker verification systems by measuring the equal error rate (EER) and the minimum detection cost (minDCF) [80]. For the detection cost function, we configure the parameters as C Miss = 1 , C FA = 1 , and P Target = 0 . 05 .2) Clustering evaluation: To evaluate clustering quality, we use three metrics as outlined in [81] and [9]:- Normalized mutual information (NMI): This metric measures the agreement between our clustering and the true data grouping, providing a score between 0 and 1, where 0 indicates no match and 1 indicates a perfect match.- Clustering accuracy: We evaluate accuracy by comparing pseudo labels to ground truth labels, using the Hungarian algorithm [82] to establish label correspondence.- Mean maximal purity per cluster: This metric assesses the semantic purity of each pseudo cluster in comparison to the ground truth labels:$$\\ p r i t y = \\frac { 1 } { K } \\sum _ { k \\in K } \\max \\left ( p \\left ( y | \\hat { y } = k \\right ) \\right )$$where K is the number of pseudo clusters, ˆ y represents a pseudo cluster and p ( y | ˆ y = k ) is the distribution of ground-truth labels within pseudo cluster k .## V. EXPERIMENTAL RESULTSThis section evaluates the improved two-stage framework with SSRL in terms of speaker verification performance and pseudo-labeling robustness. We also investigates the contributions of different individual components in"
  },
  {
    "hash_code": "b52ee359874f9499fbed5a6b0b954ff0",
    "text": "\\frac { 1 } { K } \\sum _ { k \\in K } \\max \\left ( p \\left ( y | \\hat { y } = k \\right ) \\right )$$where K is the number of pseudo clusters, ˆ y represents a pseudo cluster and p ( y | ˆ y = k ) is the distribution of ground-truth labels within pseudo cluster k .## V. EXPERIMENTAL RESULTSThis section evaluates the improved two-stage framework with SSRL in terms of speaker verification performance and pseudo-labeling robustness. We also investigates the contributions of different individual components in the proposed SSRL method.## A. Speaker verification performance1) Comparing SSRL with iterative training in two-stage framework: The primary objective of our experiments is to compare the proposed SSRL method with the iterative training in the two-stage framework. Table II shows that the SSRLtrained ResNet model achieves an EER of 2.39% on the VoxCeleb 1-O trial in just one training round, surpassing the fifth-round model in the two-stage iterative framework (2.74%).TABLE II: ResNet results: speaker verification performance (minDCF and EER[%]) on VoxCeleb 1 test trials.| Model             |       |      |       |       |       |       ||-------------------|-------|------|-------|-------|-------|-------||                   |       |      |       |       |       |       || Supervised        | 0.097 | 8.86 | 0.570 | 10.15 | 0.710 | 16.20 || Round 1           | 0.257 | 3.64 | 0.299 | 4.11  | 0.459 | 7.68  || Round 2           | 0.214 | 2.99 | 0.234 | 3.41  | 0.362 | 6.25  || Iterative Round 3 | 0.190 | 2.93 | 0.214 | 3.23  | 0.334 | 5.85  || Round 4           | 0.184 | 2.85 | 0.202 | 3.16  | 0.314 | 5.54  || Round 5           | 0.173 | 2.74 | 0.201 | 3.08  | 0.311 | 5.48  || SSRL (one round)  | 0.163 | 2.39 | 0.183 | 2.63  | 0.285 | 4.74  |TABLE III: ECAPA-TDNN results: Speaker verification performance (minDCF and EER[%]) on VoxCeleb 1 test trials.| Model                  | VoxCeleb 1-OVoxCeleb 1-EVoxCeleb 1-H   | VoxCeleb 1-OVoxCeleb 1-EVoxCeleb 1-H   | VoxCeleb 1-OVoxCeleb 1-EVoxCeleb 1-H   | VoxCeleb 1-OVoxCeleb 1-EVoxCeleb 1-H   | VoxCeleb 1-OVoxCeleb 1-EVoxCeleb 1-H   | VoxCeleb 1-OVoxCeleb 1-EVoxCeleb 1-H   ||------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------||                        |                                        |                                        |                                        |                                        |                                        |                                        || Supervised             | 0.143                                  | 1.88                                   | 0.136                                  | 1.98                                   | 0.237                                  | 3.96                                   || Supervised + AAM       | 0.075                                  | 0.99                                   | 0.081                                  | 1.22                                   | 0.144                                  | 2.35                                   || Two-                   | 0.202                                  | 2.94                                   | 0.218                                  | 3.05                                   | 0.364                                  | 5.88                                   || Stage                  | 0.181                                  | 2.49                                   | 0.183                                  | 2.73                                   | 0.288                                  | 5.01                                   || Iterative              | 0.174                                  | 2.34                                   | 0.180                                  | 2.66                                   | 0.282                                  | 4.90                                   || Framework Round 3      | 0.177                                  | 2.28                                   | 0.184                                  | 2.70                                   | 0.288                                  | 4.95"
  },
  {
    "hash_code": "47a44aae8a0b8b7c5445867888acb4cd",
    "text": "5.88                                   || Stage                  | 0.181                                  | 2.49                                   | 0.183                                  | 2.73                                   | 0.288                                  | 5.01                                   || Iterative              | 0.174                                  | 2.34                                   | 0.180                                  | 2.66                                   | 0.282                                  | 4.90                                   || Framework Round 3      | 0.177                                  | 2.28                                   | 0.184                                  | 2.70                                   | 0.288                                  | 4.95                                   || SSRL (one round)       | 0.131                                  | 1.77                                   | 0.127                                  | 1.85                                   | 0.217                                  | 3.59                                   || SSRL (one round) + AAM | 0.101                                  | 1.25                                   | 0.098                                  | 1.47                                   | 0.174                                  | 2.86                                   |Similarly, for the ECAPA-TDNN model in Table III, the SSRL method demonstrates superior performance with an EER of 1.77% in one training round, compared to 2.28% EER from the third-round model in the two-stage iterative framework. The integration of AAM loss in SSRL suggests even more potential, with EER dropping to 1.25%.The supervised results in Tables II and III serve as upper bounds for model performance. Compared to the supervised model, both self-supervised methods do not surpass supervised performance. However, SSRL significantly reduces the performance gap. For the ResNet-based pipeline, SSRL achieves an EER of 2.39% on VoxCeleb 1-O, compared to 2.74% for the best model in two-stage iterative framework. For ECAPATDNN, SSRL achieves 1.77% EER, improving upon the twostage iterative framework's best result of 2.28%, bringing it closer to the supervised model's 1.88% EER.The superiority of the SRRL second stage over the iterative second stage can be ascribed to its robust pseudo-labeling mechanism. Unlike the two-stage iterative framework which employs static pseudo labels for a whole training round, SSRL benefits from dynamically updated labels via self-supervised knowledge distillation and online clustering. This continuous refinement ensures the student model always benefits from the latest supervision signals, eliminating the 'stale' label problem observed in the two-stage iterative framework. Furthermore, SSRL's incorporation of a pseudo label queue and noisy label modeling techniques further improve the reliability and robustness of the pseudo labels, enhancing overall model performance.2) Efficiency of the SSRL approach: Unlike the iterative second stage which requires multiple training rounds, SSRLFig. 5: Comparison of training time vs. EER for iterative training and SSRL training in the two-stage framweorkline chartThe figure shows the average of the number of days in the year, and the number of days in the year, for each month.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000004_ef3c6b8bcacdf0c32783064baaa14a1b2548c140c1cd8051b1ff8679350c3159.png)introduces a more streamlined approach. This eliminates the need for iterative training, leading to improved efficiency. This is illustrated in Figure 5, which compares the EER over training time between the iterative second stage and SSRL second stage in the two-stage framework. 2For the ResNet-based pipeline, it is apparent from the visualization that SSRL achieves quicker convergence and maintains a more stable EER than iterative training. The iterative approach exhibits fluctuations due to its clustering process, where pseudo labels are re-generated between rounds, requiring random initialization of the final linear layer. This causes temporary EER spikes before stabilization. In contrast, SSRL continuously refines pseudo labels within a single training round, enabling smoother training dynamics and improved efficiency. These advantages demonstrate SSRL's potential for applications where training time and computational resources are critical considerations.The ECAPA-TDNN-based pipeline fails to converge and experiences overfitting during each training round in the iterative second stage. The verification performance (EER) shows minimal improvement before rapidly deteriorating. This occurs because we initialize the network using parameters from the previous round and k-means centers for the final linear layer, causing rapid data fitting in early epochs. Due to2 All models are trained on two NVIDIA GeForce RTX 3090 GPUs. The estimated training time focuses solely on ideal conditions, accounting only for the forward and backward propagation time (model training time of a single batch). It excludes time allocations for data loading, preprocessing pipeline"
  },
  {
    "hash_code": "b265753329bd204ad7887d3e231247ca",
    "text": "APA-TDNN-based pipeline fails to converge and experiences overfitting during each training round in the iterative second stage. The verification performance (EER) shows minimal improvement before rapidly deteriorating. This occurs because we initialize the network using parameters from the previous round and k-means centers for the final linear layer, causing rapid data fitting in early epochs. Due to2 All models are trained on two NVIDIA GeForce RTX 3090 GPUs. The estimated training time focuses solely on ideal conditions, accounting only for the forward and backward propagation time (model training time of a single batch). It excludes time allocations for data loading, preprocessing pipeline, model validations, and procedures like k-means clustering and GMM modeling. These processes, being brief in nature, are considered negligible.Fig. 6: Evolution of pseudo labeling across training epochs during the SSRL training phase.line chartA graph with the title SSRL-trained ResNet and SSRL-trained ECAPA-TDNN.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000005_19dde789b94e29e48855aa1beac7561d981716ca3d4baa625094ae274c369c4f.png)TABLE IV: Comparison of the proposed SSRL method with two-stage iterative framework variants. EERs [%] from VoxCeleb 1-O test trial; all models use ECAPA-TDNN. 'Filter' denotes mislabeled sample filtering, 'LC' for label correction.| Method            | Loss     | Filter LC   | Filter LC   | Other        | #Rounds   | Stage 1 EER   |   EER ||-------------------|----------|-------------|-------------|--------------|-----------|---------------|-------|| Thienpondt et al. | [59] AAM | -           | -           | -            | 7         | 7.3           |  2.1  || Mun et al. [83]   | AAM      | -           | -           | score norm   | 5         | 3.65          |  1.66 || Tao et al. [63]   | AAM      | ✓           | -           | -            | 5         | 7.36          |  1.66 || Han et al. [64]   | AAM      | ✓           | ✓           | -            | 5         | 6.16          |  1.47 || Tao et al. [62]   | AAM      | -           | -           | audio-visual | ≥ 2       | 2.89          |  1.44 || Chen et al. [65]  | AAM      | -           | -           | audio-visual | 7         | 7.16          |  1.27 || Chen et al. [84]  | AAM      | -           | ✓           | WavLM        | 5         | -             |  1.25 || SSRL (proposed)   | CE       | -           | ✓           | -            | 1         | 2.94          |  1.77 || SSRL (proposed)   | AAM      | -           | ✓           | -            | 1         | 2.94          |  1.25 || SSRL (proposed)   | AAM      | -           | ✓           | WavLM        | 1         | 2.94          |  1.04 |this overfitting tendency, we terminated training after the third round. These results suggest that with a strong initialization (DINO pretrained model), the two-stage iterative framework cannot substantially enhance performance. In contrast, the proposed SSRL method dynamically adjusts clustering, leading to further performance improvements even when starting with a relatively well-pretrained model.3) Comparative analysis with other two-stage iterative framework variants: In Table IV, the performance of the proposed SSRL method is compared with various two-stage iterative framework variants, all leveraging the ECAPA-TDNN model. A remarkable observation is the efficiency and efficacy of SSRL when trained with the AAM loss: it surpasses all other methods, achieving superior performance within a single training round.Two comparisons deserve special mention. First, when contrasted with the work of Chen et al. [65] - which incorporates an additional visual modality during training -our SSRL method delivers performance on par, even though it relies exclusively on audio information. Secondly, another variant from Chen et al. [84] makes use of a subset of WavLM [53], a large self-supervised speech model trained on extensive data, for feature extraction. Our SSRL approach, devoid of any large"
  },
  {
    "hash_code": "ea9be7791052b1cbd435d201ed42b7a4",
    "text": "efficacy of SSRL when trained with the AAM loss: it surpasses all other methods, achieving superior performance within a single training round.Two comparisons deserve special mention. First, when contrasted with the work of Chen et al. [65] - which incorporates an additional visual modality during training -our SSRL method delivers performance on par, even though it relies exclusively on audio information. Secondly, another variant from Chen et al. [84] makes use of a subset of WavLM [53], a large self-supervised speech model trained on extensive data, for feature extraction. Our SSRL approach, devoid of any large-scale pre-trained model, emerges with a similar performance.To further evaluate our approach, we integrated WavLM as aTABLE V: Pseudo labeling performance on training data.| Model   | Method                 | #Clusters        | NMI           | Accuracy      | Purity        ||---------|------------------------|------------------|---------------|---------------|---------------|| ResNet  | Iterative round 5 SSRL | 6000 8000 → 5085 | 0.9230 0.9333 | 68.93% 78.12% | 83.50% 87.42% || ResNet  | SSRL                   | 8000 → 5085      | 0.9333        | 78.12%        | 87.42%        || ECAPA   | Iterative round 3 SSRL | 8000 8000 → 5328 | 0.9333 0.9651 | 64.83% 88.08% | 92.10%        |feature extractor alongside the proposed SSRL method. Specifically, we extracted features from every layers of WavLMLarge encoder and combined them using a learnable weighted sum to create composite features for input to ECAPA-TDNN. Our training strategy involved initially freezing the WavLM parameters during early epochs, followed by gradual finetuning. This integration proved highly effective: the system achieved 1.04% EER on Vox1-O, representing a significant 16.8% improvement over the SSRL model trained with Melfilterbank features (1.25% EER).## B. Pseudo labeling performanceTable V details the pseudo labeling performance of the ResNet and ECAPA models on the training data. The SSRL method consistently shows superior metrics across both model architectures. For instance, with the ResNet model, the SSRL technique achieved an accuracy of 78.12% compared to the 68.93% from the two-stage iterative framework in its fifth training round. This superior performance can be attributed to the online clustering achieved through self-supervised knowledge distillation, coupled with additional strategies to enhance the quality of pseudo labels.In Figure 6, the evolution of pseudo labeling throughout the training epochs using the SSRL method is depicted. As observed, during the SSRL training process, there's a consistent reduction in the number of clusters across epochs until a stable count is reached. For the ResNet-based SSRL, this stable number is 5084, whereas for the ECAPA-TDNNbased SSRL, it's 5328. For reference, the training data contains a total of 5994 speakers. These observations indicate that the SSRL method is adept at filtering out pseudo clusters that have lower confidence, thereby progressively optimizing the labeling performance. Moreover, metrics such as NMI,TABLE VI: Performance comparison of the SSRL approach with different component configurations. p clean represents the proposed noisy label modeling method. K represents the converged number of clusters. The actual cluster counts is 5994.| Online Clustering   | EMA   | Label Queue   | p clean   | Verification EER[%] ↓   | Verification EER[%] ↓   | Verification EER[%] ↓   | Pseudo Labeling   | Pseudo Labeling   | Pseudo Labeling   | Pseudo Labeling   ||---------------------|-------|---------------|-----------|-------------------------|-------------------------|-------------------------|-------------------|-------------------|-------------------|-------------------||                     | EMA   | Label Queue   | p clean   | Vox1-O                  | Vox1-E                  | Vox1-H                  | NMI ↑             | Acc ↑             | Purity ↑          | K                 || argmax              | ✓     | ✓             | ✓         | 2.39                    | 2.63                    | 4.74                    | 0.9333            | 78.12%            | 87.42%            | 5085              || argmax              | ✗     | ✓             | ✓         | 2.48                    | 2.97                    | 5.31                    | 0.9261            | 77.23%            | 87.76%            | 4943              || argmax"
  },
  {
    "hash_code": "f58eb83742a45d3a5d874b5daa7f423e",
    "text": "urity ↑          | K                 || argmax              | ✓     | ✓             | ✓         | 2.39                    | 2.63                    | 4.74                    | 0.9333            | 78.12%            | 87.42%            | 5085              || argmax              | ✗     | ✓             | ✓         | 2.48                    | 2.97                    | 5.31                    | 0.9261            | 77.23%            | 87.76%            | 4943              || argmax              | ✓     | ✗             | ✓         | 2.51                    | 2.68                    | 4.82                    | 0.9297            | 77.31%            | 86.96%            | 4801              || argmax              | ✓     | ✓             | ✗         | 2.76                    | 2.94                    | 5.29                    | 0.9300            | 75.55%            | 86.55%            | 6152              || argmax              | ✗     | ✗             | ✗         | 5.19                    | 6.52                    | 11.73                   | 0.8567            | 66.88%            | 90.64%            | 4306              || Sinkhorn            | ✓     | ✓             | ✓         | 2.41                    | 2.57                    | 4.61                    | 0.9402            | 79.26%            | 84.45%            | 5974              |accuracy, and maximal purity per cluster show that with each passing epoch, the SSRL-trained models fine-tune their performance, reflecting continuous improvement.## C. Ablation studyThe SSRL approach employs components designed to enhance the model's performance on the unlabeled dataset. To inspect the contributions of these components, we conduct ablation studies on the ResNet-based SSRL, shown in Table VI.1) Speaker verification performance analysis: When the EMA update for the teacher model is integrated into SSRL, we observe an improvement in speaker verification performance, with EERs of 2.39%, 2.63%, and 4.74% across the VoxCeleb test trials. In contrast, the model without the EMA update shows higher EERs of 2.48%, 2.97%, and 5.31%, respectively. This empirical evidence underscores the crucial role of EMA in SSRL for enhancing speaker verification performance.Furthermore, the pseudo label queue further improves the SSRL model. Its integration not only amplifies speaker verification capabilities but also buffers against potential pitfalls associated with pseudo labeling. From Table VI, we can see that the experiment without using the label queue results in worse EERs and pseudo-labeling performance compared to the one that includes it. Specifically, the final cluster count (4,801) is significantly smaller, indicating that many classes were removed during training. This suggests that training with noisy labels leads to bias toward certain classes, and without correction, the model reinforces this bias, causing pseudo labels to collapse into fewer clusters. The label queue serves as a buffer against these potential pitfalls by stabilizing pseudo labels and preventing the excessive merging of speaker identities.Notably, introduction of noisy label modeling with p clean provides an additional layer of refinement to the SSRL approach. By guiding predictions towards cleaner samples, this mechanism mitigates the challenges associated with noisy label updates. A degradation in speaker verification performance is observed in the absence of noisy label modeling, with EER increase to 2.76%, 2.94%, and 5.29% across the test trials.Additionally, we evaluated a simplified version of SSRL. This variant, devoid of the EMA updates, pseudo label queue, and noisy label modeling, preserves only the noisy student training strategy. As observed in Table VI, this simplified version undergoes a significant performance drop, with an EER increase of 2.8 percentage points (2.39% → 5.19%)Fig. 7: DET curves on VoxCeleb 1-H test trial: comparing SSRL methods with different component configurations.line chartThe image is a line graph that compares the performance of different algorithms in terms of false alarm rate and false reject rate.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000006_3233166f111f64e3416201f6762bacc8319c0ac95511ace6b3869be38db922bd.png)compared to the full SSRL approach on the VoxCeleb 1-O test trial. In fact, this simplified SSRL had difficulty"
  },
  {
    "hash_code": "a55ba3757c6b8827c030083a823acfd9",
    "text": "image is a line graph that compares the performance of different algorithms in terms of false alarm rate and false reject rate.![Image](C:\\Users\\ML4SE\\Desktop\\openspec_demo\\configs\\data\\OpenRAGBench\\out\\2401.01473v3_artifacts\\image_000006_3233166f111f64e3416201f6762bacc8319c0ac95511ace6b3869be38db922bd.png)compared to the full SSRL approach on the VoxCeleb 1-O test trial. In fact, this simplified SSRL had difficulty converging. These observations underscore the collective significance of the various components in achieving optimal performance with SSRL. The detection error tradeoff (DET) plots on VoxCeleb 1-H test trial is shown in Figure 7 to compare the SSRL approach with different component configurations.2) Pseudo labeling analysis: The dynamism inherent in the online clustering mechanism deserves mention. Through SSRL's online clustering, certain clusters are filtered out or merged as training progresses, thereby stabilizing the number of clusters towards the conclusion. Referring to Table VI, the full version of SSRL, equipped with the EMA update, pseudo label queue, and noisy label modeling, has a converged cluster count of 5085. However, models without either the EMA update or the pseudo label queue end with smaller cluster counts, registering at 4943 and 4801, respectively. This observation indicates that the EMA update and pseudo label queue jointly act as regularizers for pseudo cluster prediction, fostering stability throughout the training epochs, thus preventing the cluster counts shrink too quickly. Specifically, the pseudo labelTABLE VII: Performance comparison of the SSRL approach using varying numbers of clusters for the initial pseudo labels. K init denotes the initial number of clusters; K converged indicates the number of clusters upon convergence. The arrow illustrates the transition from the model trained with the fixed initial clustering for 50 epochs to the converged SSRL.| K init   | Verification EER[%] ↓   | Verification EER[%] ↓   | Verification EER[%] ↓   | Pseudo Labeling   | Pseudo Labeling   | Pseudo Labeling   | Pseudo Labeling   ||----------|-------------------------|-------------------------|-------------------------|-------------------|-------------------|-------------------|-------------------|| K init   | Vox1-O                  | Vox1-E                  | Vox1-H                  | NMI ↑             | Acc ↑             | Purity ↑          | K converged       || 1,000 3  | ↶ 5.98 6.73             | ↶ 6.51 7.70             | ↶ 11.87 13.93           | ↶ 0.6189 0.6658   | ↶ 19.84% 21.84%   | ↶ 21.58% 45.58%   | 714               || 8,000    | ↶ 4.05 2.39             | ↶ 4.61 2.63             | ↶ 8.58 4.74             | ↶ 0.7744 0.9333   | ↶ 36.87% 78.12%   | ↶ 55.32% 87.42%   | 5,085             || 20,000   | ↶ 3.94 3.03             | ↶ 4.29 2.92             | ↶ 7.88 5.21             | ↶ 0.8114 0.9356   | ↶ 27.68% 73.35%   | ↶ 68.21% 92.66%   | 9,956             |TABLE VIII: Performance comparison of the SSRL approach with different pseudo label queue length L . K represents the converged number of clusters.| L   | Verification EER[%] ↓   | Verification EER[%] ↓   | Verification EER[%] ↓   | Pseudo Labeling   | Pseudo Labeling   | Pseudo Labeling   | Pseudo Labeling   ||-----|-------------------------|-------------------------|-------------------------|-------------------|-------------------|-------------------|-------------------||     | Vox1-O                  | Vox1-E                  | Vox1-H                  | NMI ↑             | Acc ↑             | Purity ↑          | K                 || 1 4 | 2.51                    | 2.68                    | 4.82                    | 0.9297            | 77.31%            | 86.96%            | 4801              || 5   | 2.39                    | 2.63                    | 4.74                    | 0.9333            | 78.12%            |"
  },
  {
    "hash_code": "d136b1cfcf1ac0a909199716dab176ff",
    "text": "---|-------------------||     | Vox1-O                  | Vox1-E                  | Vox1-H                  | NMI ↑             | Acc ↑             | Purity ↑          | K                 || 1 4 | 2.51                    | 2.68                    | 4.82                    | 0.9297            | 77.31%            | 86.96%            | 4801              || 5   | 2.39                    | 2.63                    | 4.74                    | 0.9333            | 78.12%            | 87.42%            | 5085              || 10  | 2.45                    | 2.65                    | 4.79                    | 0.9322            | 77.58%            | 87.31%            | 5320              || 20  | 2.56                    | 2.73                    | 4.93                    | 0.9296            | 76.87%            | 86.97%            | 5485              |queue serves as a buffer against erratic predictions, enhancing stability and minimizing outliers, while the EMA component ensures that the network remains consistent in its cluster assignment predictions.Conversely, the noisy label modeling approach appears to have an opposing effect on the converged cluster count compared to the EMA update and pseudo label queue. Excluding noisy label modeling culminates in an increased cluster count of 6152. This suggests that noisy label modeling prioritizes predictions for more confident clusters, diminishing those with less confidence, which consequently reduces the overall cluster count.3) Direct maximum probability versus Sinkhorn-based online clustering: To investigate the impact of different online clustering techniques, we evaluated two approaches: direct maximum probability assignment and cluster assignment through optimal transport.In terms of speaker verification, both methods prove efficacious, achieving comparable EERs across test trials. On VoxCeleb 1-E and VoxCeleb 1-H, the Sinkhorn approach registers marginal improvements with EERs of 2.57% and 4.61% compared to 2.63% and 4.74% using direct assignment. This suggests that the two techniques are largely comparable in enhancing speaker verification capabilities.Regarding pseudo labeling, the Sinkhorn method manifests an edge, garnering superior metrics of clustering accuracy (79.26% vs 78.12%) and NMI (0.9402 vs 0.9333). This indicates an enhanced capacity for accurate pseudo label generation using the optimal transport approach. Inspecting the converged number of clusters, Sinkhorn retains more clusters3 When trained with an initial cluster count of 1,000, the model could not converge, so we stopped the training after 50 epochs of SSRL.4 Label queue method is disabled when label queue length L = 1 .upon convergence at 5974, contrasted with 5085 using direct assignment. This aligns with the constraint in the Sinkhorn algorithm to distribute samples evenly across clusters. Conversely, the direct assignment aggressively merges smaller, outlier clusters. In summary, both online clustering techniques prove effective and validate the online clustering mechanism's efficacy in SSRL.4) Interplay of initial cluster count: Table VII shows the interplay between the initial cluster count K init and the SSRL approach's performance. An overly conservative choice for K init (e.g., 1,000) seems to restrict the model's ability to capture the data's inherent diversity, leading to suboptimal results. In contrast, an overly aggressive K init (e.g., 20,000) does allow for improved pseudo labeling metrics, but doesn't necessarily translate to the best verification EER. In summary, the choice of K init is crucial. It acts as a balance between providing enough granularity for capturing data diversity and ensuring the model remains focused on meaningful clusters.5) Impact of pseudo label queue length L : Table VIII shows the impact of pseudo label queue length L on the model's performance. The pseudo label queue filters transient inconsistencies, and ensuring continuity in predicted pseudo labels across training epochs. An observation is the marginal degradation in performance as L increases beyond a certain threshold. With L = 1 , essentially indicating no pseudo label queue, the verification EER on VoxCeleb 1-O test trial is 2.51% and the converged number of clusters K stands at 4801. Increasing L to 5 yields a better EER of 2.39% and a higher K of 5084. Further increments in L to 10 and 20, however, show worse EERs and expanding K s. This trend suggests an optimal range for L where the benefits of temporal stabilization maximize. An excessively long queue might integrate older, potentially less relevant pseudo labels, causing slight deteriorations in performance. This observation aligns with the inherent trade-off: while having some history aids in stabilization, overly long histories might dilute the recent advancements"
  },
  {
    "hash_code": "2519e9040ca90a8a07dc21b945a81ee0",
    "text": ".51% and the converged number of clusters K stands at 4801. Increasing L to 5 yields a better EER of 2.39% and a higher K of 5084. Further increments in L to 10 and 20, however, show worse EERs and expanding K s. This trend suggests an optimal range for L where the benefits of temporal stabilization maximize. An excessively long queue might integrate older, potentially less relevant pseudo labels, causing slight deteriorations in performance. This observation aligns with the inherent trade-off: while having some history aids in stabilization, overly long histories might dilute the recent advancements the model has achieved.## D. Fine-tuningIn this section, the SSRL pre-trained ECAPA-TDNN speaker model is fine-tuned with small-scale labeled datasets. We use the VoxCeleb 1 development set (1,211 speakers) [71] for fine-tuning and create an additional subset of 600 randomly selected speakers to evaluate self-supervised pre-training onTABLE IX: Fine-tune the self-supervised model with different labeled data in VoxCeleb 1 development set.| Fine-tuning Data   | None   | None   | 600 Speakers   | 600 Speakers   |       | 1,211 Speakers   ||--------------------|--------|--------|----------------|----------------|-------|------------------|| Pre-trained Model  |        |        |                |                |       |                  || None               | -      | -      | 0.295          | 3.94           | 0.175 | 2.31             || SSRL               | 0.101  | 1.25   | 0.089          | 1.05           | 0.075 | 0.95             |smaller datasets. Results are reported on the VoxCeleb 1-O test trials.As shown in Table IX, fine-tuning the SSRL model with labeled data significantly improves performance: fine-tuning on only 600 speakers achieves an EER of 1.05%, compared to 3.94% without SSRL pre-training. Fine-tuning the SSRL model on all labeled speakers in VoxCeleb 1 (1,211 speakers) further reduces the EER to 0.95%, compared to 2.31% without SSRL pre-training. These results demonstrate that SSRL provides a strong self-supervised foundation, which can be further enhanced with labeled data for improved speaker verification.## VI. CONCLUSIONThis paper introduces self-supervised reflective learning (SSRL), a novel paradigm for unsupervised speaker representation learning. SSRL streamlines existing two-stage iterative frameworks by integrating self-supervised knowledge distillation with online clustering. A teacher model continually refines pseudo labels through clustering, providing dynamic supervision to train the student model. The method also employs techniques like label correction and noisy label modeling to further improve pseudo label quality.Our experiments demonstrate SSRL's superiority over current two-stage iterative approaches. On VoxCeleb 1 test trials, SSRL surpasses the performance of a 5-round iterative method in just a single training round. Ablation studies validate the contributions of key components like noisy label modeling, pseudo label queues, and EMA teacher updates. Moreover, the consistent improvement in pseudo labeling throughout the training phase, coupled with the convergence of cluster count, reaffirms SSRL's prowess in deciphering pertinent clusters within unlabeled data.This work marks a pivotal advancement in efficient and accurate speaker representation learning. By combining selfsupervised distillation and online clustering, SSRL eliminates previous iterative bottlenecks. The reflective learning paradigm introduces new horizons for developing scalable, unsupervised systems. Future work should assess SSRL on larger datasets and expand hyperparameter optimizations. Integrating SSRL into end-to-end pipelines is another research direction.## VII. ACKNOWLEDGMENTSThis research is funded in part by the National Natural Science Foundation of China (62171207), Science and Technology Program of Suzhou City (SYC2022051) and Guangdong Science and Technology Plan (2023A1111120012). Many thanks for the computational resource provided by the Advanced Computing East China Sub-Center.## REFERENCES- [1] Z. Chen, S. Chen, Y. Wu, Y. Qian, C. Wang, S. Liu, Y. Qian, and M. Zeng, 'Large-Scale Self-Supervised Speech Representation Learning for Automatic Speaker Verification,' in Proceeding of ICASSP , 2022, pp. 6147-6151.- [2] Y. Chen, S. Zheng, H. Wang, L. Cheng, and Q. Chen, 'Pushing the Limits of Self-Supervised Speaker Verification Using Regularized Distillation Framework,' in Proceeding of ICASSP , 2023, pp. 1-5.- [3] Y. Tu, M.-W. Mak, and J.-"
  },
  {
    "hash_code": "b77db12f44fb50e3fb17c12be3580c31",
    "text": "Y. Qian, and M. Zeng, 'Large-Scale Self-Supervised Speech Representation Learning for Automatic Speaker Verification,' in Proceeding of ICASSP , 2022, pp. 6147-6151.- [2] Y. Chen, S. Zheng, H. Wang, L. Cheng, and Q. Chen, 'Pushing the Limits of Self-Supervised Speaker Verification Using Regularized Distillation Framework,' in Proceeding of ICASSP , 2023, pp. 1-5.- [3] Y. Tu, M.-W. Mak, and J.-T. Chien, 'Contrastive Self-Supervised Speaker Embedding with Sequential Disentanglement,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , 2024.- [4] Y. Liu, L.-F. Wei, C.-F. Zhang, T.-H. Zhang, S.-L. Chen, and X.C. Yin, 'Self-Supervised Contrastive Speaker Verification with Nearest Neighbor Positive Instances,' Pattern Recognition Letters , vol. 173, pp. 17-22, 2023.- [5] Z. Zhou, H. Yang, and T. Shinozaki, 'Self-Supervised Speaker Verification with Adaptive Threshold and Hierarchical Training,' in Proceeding of ICASSP , 2024, pp. 12 141-12 145.- [6] A. Fathan and J. Alam, 'An Analytic Study on Clustering Driven SelfSupervised Speaker Verification,' Pattern Recognition Letters , vol. 179, pp. 80-86, 2024.- [7] S. Wang, Q. Bai, Q. Liu, J. Yu, Z. Chen, B. Han, Y. Qian, and H. Li, 'Leveraging in-the-wild data for effective self-supervised pretraining in speaker recognition,' in Proceeding of ICASSP , 2024, pp. 10 90110 905.- [8] D. Cai, W. Wang, and M. Li, 'An Iterative Framework for SelfSupervised Deep Speaker Representation Learning,' in Proceeding of ICASSP , 2021, pp. 6728-6732.- [9] D. Cai, W. Wang, and M. Li, 'Incorporating Visual Information in Audio Based Self-Supervised Speaker Recognition,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 30, pp. 1422-1435, 2022.- [10] X. Chen and K. He, 'Exploring Simple Siamese Representation Learning,' in Proceedings of CVPR , 2021, pp. 15 750-15 758.- [11] G. Hinton, O. Vinyals, and J. Dean, 'Distilling the Knowledge in a Neural Network,' in NeurIPS Deep Learning and Representation Learning Workshop , 2015.- [12] J.-B. Grill, F. Strub, F. Altch´ e, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al. , 'Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning,' NeurIPS , vol. 33, pp. 21 271-21 284, 2020.- [13] M. Caron, H. Touvron, I. Misra, H. J´ egou, J. Mairal, P. Bojanowski, and A. Joulin, 'Emerging Properties in Self-supervised Vision Transformers,' in Proceedings of ICCV , 2021, pp. 9650-9660.- [14] E. Arazo, D. Ortego, P. Albert, N. E. O'Connor, and K. McGuinness, 'Unsupervised Label Noise Modeling and Loss Correction,' in Proceedings of the International Conference on Machine Learning , 2019.- [15] G. Elbanna, N. Scheidwasser-Clow, M. Kegler, P. Beckmann, K. El Hajal, and M. Cernak, 'byol-S: Learning Self-supervised Speech Representations by Bootstrapping,' in HEAR: Holistic Evaluation of Audio Representations . PMLR, 2022, pp. 25-47.- [16] A. H. Liu, H.-J. Chang, M. Auli, W.-N. Hsu, and J. Glass, 'DinoSR: Self-distillation and Online Clustering for Self-supervised Speech Representation learning,' Advances"
  },
  {
    "hash_code": "011e4f75d712a6825a76b3b18ee83cf6",
    "text": "wasser-Clow, M. Kegler, P. Beckmann, K. El Hajal, and M. Cernak, 'byol-S: Learning Self-supervised Speech Representations by Bootstrapping,' in HEAR: Holistic Evaluation of Audio Representations . PMLR, 2022, pp. 25-47.- [16] A. H. Liu, H.-J. Chang, M. Auli, W.-N. Hsu, and J. Glass, 'DinoSR: Self-distillation and Online Clustering for Self-supervised Speech Representation learning,' Advances in Neural Information Processing Systems , vol. 36, 2024.- [17] Q.-S. Zhu, L. Zhou, J. Zhang, S.-J. Liu, Y.-C. Hu, and L.-R. Dai, 'Robust Data2VEC: Noise-Robust Speech Representation Learning for ASR by Combining Regression and Improved Contrastive Learning,' in Proceeding of ICASSP , 2023, pp. 1-5.- [18] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, 'Deep Clustering for Unsupervised Learning of Visual Features,' in Proceedings of ECCV , 2018.- [19] Y. M. Asano, C. Rupprecht, and A. Vedaldi, 'Self-Labelling Via Simultaneous Clustering and Representation Learning,' in ICLR , 2020.- [20] J. Li, P. Zhou, C. Xiong, and S. C. H. Hoi, 'Prototypical Contrastive Learning of Unsupervised Representations,' in ICLR , 2021.- [21] X. Zhan, J. Xie, Z. Liu, Y.-S. Ong, and C. C. Loy, 'Online Deep Clustering for Unsupervised Representation Learning,' in Proceedings of CVPR , 2020, pp. 6687-6696.- [22] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, 'Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,' NeurIPS , vol. 33, pp. 9912-9924, 2020.- [23] H.-J. Chang, A. H. Liu, and J. Glass, 'Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering,' in Proceeding of Interspeech , 2023, pp. 2983-2987.- [24] X. Zhu and A. B. Goldberg, Introduction to Semi-Supervised Learning . Springer Nature, 2022.- [25] X. Yang, Z. Song, I. King, and Z. Xu, 'A Survey on Deep SemiSupervised Learning,' IEEE Transactions on Knowledge and Data Engineering , vol. 35, no. 9, pp. 8934-8954, 2023.- [26] M.-R. Amini, V. Feofanov, L. Pauletto, E. Devijver, and Y. Maximov, 'Self-training: A survey,' arXiv:2202.12040 , 2022.- [27] Z. Ke, D. Wang, Q. Yan, J. Ren, and R. W. Lau, 'Dual student: Breaking the Limits of the Teacher in Semi-supervised Learning,' in Proceedings of CVPR , 2019, pp. 6728-6736.- [28] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li, 'Fixmatch: Simplifying Semi-supervised Learning with Consistency and Confidence,' NeurIPS , vol. 33, pp. 596-608, 2020.- [29] X. Chen, Y. Yuan, G. Zeng, and J. Wang, 'Semi-supervised Semantic Segmentation with Cross Pseudo Supervision,' in Proceedings of CVPR , 2021, pp. 2613-2622.- [30] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, 'Self-training with Noisy Student Improves ImageNet Classification,' in Proceedings of CVPR , 2020, pp. 10 687-10 698.- [31] P. P. Busto, A. Iqbal, and J. Gall, 'Open Set Domain Adaptation for Image and Action Recognition,'"
  },
  {
    "hash_code": "4089a23b2a2b0f6d8cc0953f571f8771",
    "text": "emi-supervised Semantic Segmentation with Cross Pseudo Supervision,' in Proceedings of CVPR , 2021, pp. 2613-2622.- [30] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, 'Self-training with Noisy Student Improves ImageNet Classification,' in Proceedings of CVPR , 2020, pp. 10 687-10 698.- [31] P. P. Busto, A. Iqbal, and J. Gall, 'Open Set Domain Adaptation for Image and Action Recognition,' IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 42, no. 2, pp. 413-429, 2018.- [32] G. French, M. Mackiewicz, and M. Fisher, 'Self-ensembling for Visual Domain Adaptation,' in ICLR , 2018.- [33] Y. Zou, Z. Yu, X. Liu, B. Kumar, and J. Wang, 'Confidence Regularized Self-training,' in Proceedings of CVPR , 2019, pp. 5982-5991.- [34] Y. Zou, Z. Yu, B. Kumar, and J. Wang, 'Unsupervised Domain Adaptation for Semantic Segmentation via Class-balanced Self-training,' in Proceedings of ECCV , 2018, pp. 289-305.- [35] A. Tarvainen and H. Valpola, 'Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-supervised Deep Learning Results,' NeurIPS , vol. 30, 2017.- [36] S. Laine and T. Aila, 'Temporal Ensembling for Semi-Supervised Learning,' in ICLR , 2016.- [37] S. Qiao, W. Shen, Z. Zhang, B. Wang, and A. Yuille, 'Deep Cotraining for Semi-supervised Image Recognition,' in Proceedings of ECCV , 2018, pp. 135-152.- [38] W. Dong-DongChen and Z. WeiGao, 'Tri-net for Semi-supervised Deep Learning,' in Proceeding of IJCAI , 2018, pp. 2014-2020.- [39] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, 'Speaker Verification using Adapted Gaussian Mixture Models,' Digital signal processing , vol. 10, no. 1-3, pp. 19-41, 2000.- [40] J.-L. Gauvain and C.-H. Lee, 'Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains,' IEEE transactions on speech and audio processing , vol. 2, no. 2, pp. 291-298, 1994.- [41] P. Kenny, G. Boulianne, P. Ouellet, and P. Dumouchel, 'Joint Factor Analysis versus Eigenchannels in Speaker Recognition,' IEEE Transactions on Audio, Speech, and Language Processing , vol. 15, no. 4, pp. 1435-1447, 2007.- [42] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, 'FrontEnd Factor Analysis for Speaker Verification,' IEEE Transactions on Audio, Speech, and Language Processing , vol. 19, no. 4, pp. 788-798, 2011.- [43] D. Snyder, D. Garcia-Romero, and D. Povey, 'Time Delay Deep Neural Network-based Universal Background Models for Speaker Recognition,' in Proceeding of IEEE Automatic Speech Recognition and Understanding Workshop , 2015, pp. 92-97.- [44] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, 'X-vectors: Robust DNN Embeddings for Speaker Recognition,' in Proceeding of ICASSP , 2018, pp. 5329-5333.- [45] W. Cai, J. Chen, and M. Li, 'Exploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System,' in Proceeding of The Speaker and Language Recognition Workshop (Odyssey) , 2018.- [46] T. Zhou, Y. Zhao, and J. Wu, 'ResNeXt and Res2Net Structures for Speaker Verification,' in Proceeding of IEEE Spoken Language Technology Workshop , 2021, pp. 301-307.- [47] B. Desplanques"
  },
  {
    "hash_code": "026211a945ffcb1a4b4fd198dfaeb4e5",
    "text": "pp. 5329-5333.- [45] W. Cai, J. Chen, and M. Li, 'Exploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System,' in Proceeding of The Speaker and Language Recognition Workshop (Odyssey) , 2018.- [46] T. Zhou, Y. Zhao, and J. Wu, 'ResNeXt and Res2Net Structures for Speaker Verification,' in Proceeding of IEEE Spoken Language Technology Workshop , 2021, pp. 301-307.- [47] B. Desplanques, J. Thienpondt, and K. Demuynck, 'ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification,' in Proceeding of Interspeech , 2020, pp. 3830-3834.- [48] Y. Zhang, Z. Lv, H. Wu, S. Zhang, P. Hu, Z. Wu, H.-y. Lee, and H. Meng, 'MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification,' in Proceeding of Interspeech , 2022, pp. 306-310.- [49] D. Liao, T. Jiang, F. Wang, L. Li, and Q. Hong, 'Towards A Unified Conformer Structure: from ASR to ASV Task,' in Proceeding of ICASSP , 2023, pp. 1-5.- [50] D. Cai and M. Li, 'Leveraging ASR Pretrained Conformers for Speaker Verification Through Transfer Learning and Knowledge Distillation,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 32, pp. 3532-3545, 2024.- [51] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, 'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,' Advances in Neural iInformation Processing Systems , vol. 33, pp. 12 449-12 460, 2020.- [52] W.-N. Hsu, Y.-H. H. Tsai, B. Bolte, R. Salakhutdinov, and A. Mohamed, 'Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?' in Proceeding of ICASSP , 2021, pp. 6533-6537.- [53] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei, 'WavLM: Large-Scale Self-Supervised PreTraining for Full Stack Speech Processing,' IEEE Journal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1505-1518, 2022.- [54] Z. Fan, M. Li, S. Zhou, and B. Xu, 'Exploring wav2vec 2.0 on Speaker Verification and Language Identification,' in Proceeding of Interspeech , 2021, pp. 1509-1513.- [55] N. Vaessen and D. A. van Leeuwen, 'Fine-Tuning wav2vec2 for Speaker Recognition,' in Proceeding of ICASSP , 2022, pp. 7967-7971.- [56] J. Li, K. Zheng, J. Yao, L. Gao, and D. Hong, 'Deep Unsupervised Blind Hyperspectral and Multispectral Data Fusion,' IEEE Geoscience and Remote Sensing Letters , vol. 19, pp. 1-5, 2022.- [57] J. Li, K. Zheng, W. Liu, Z. Li, H. Yu, and L. Ni, 'Model-Guided Coarseto-Fine Fusion Network for Unsupervised Hyperspectral Image SuperResolution,' IEEE Geoscience and Remote Sensing Letters , vol. 20, pp. 1-5, 2023.- [58] J. Li, K. Zheng, L. Gao, L. Ni, M. Huang, and J. Chanussot, 'ModelInformed Multistage Unsupervised Network for Hyperspectral Image Super-Resolution,' IEEE Transactions on Geoscience and Remote Sensing , vol. 62, pp. 1-17, 2024.- [59] J."
  },
  {
    "hash_code": "13a22902735a8e2776267d794cfa9a9e",
    "text": "seto-Fine Fusion Network for Unsupervised Hyperspectral Image SuperResolution,' IEEE Geoscience and Remote Sensing Letters , vol. 20, pp. 1-5, 2023.- [58] J. Li, K. Zheng, L. Gao, L. Ni, M. Huang, and J. Chanussot, 'ModelInformed Multistage Unsupervised Network for Hyperspectral Image Super-Resolution,' IEEE Transactions on Geoscience and Remote Sensing , vol. 62, pp. 1-17, 2024.- [59] J. Thienpondt, B. Desplanques, and K. Demuynck, 'The IDLAB VoxCeleb Speaker Recognition Challenge 2020 System Description,' in VoxSRC workshop , 2020.- [60] B. Han, Z. Chen, and Y. Qian, 'Self-Supervised Learning With Cluster-Aware-DINO for High-Performance Robust Speaker Verification,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 32, pp. 529-541, 2024.- [61] Z. Zhao, Z. Li, X. Zhang, W. Wang, and P. Zhang, 'Prototype Division for Self-Supervised Speaker Verification,' IEEE Signal Processing Letters , vol. 31, pp. 880-884, 2024.- [62] R. Tao, K. A. Lee, R. K. Das, V. Hautam¨ aki, and H. Li, 'Self-Supervised Training of Speaker Encoder with Multi-Modal Diverse Positive Pairs,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 31, pp. 1706-1719, 2023.- [63] R. Tao, K. A. Lee, R. K. Das, V. Hautam¨ aki, and H. Li, 'Self-Supervised Speaker Recognition with Loss-Gated Learning,' in Proceeding of ICASSP , 2022, pp. 6142-6146.- [64] B. Han, Z. Chen, and Y. Qian, 'Self-Supervised Speaker Verification Using Dynamic Loss-Gate and Label Correction,' in Proceeding of Interspeech , 2022, pp. 4780-4784.- [65] H. Chen, H. Zhang, L. Wang, K. A. Lee, M. Liu, and J. Dang, 'Self-Supervised Audio-Visual Speaker Representation with Co-Meta Learning,' in Proceeding of ICASSP , 2023, pp. 1-5.- [66] Z. Fang, L. He, L. Li, and Y. Hu, 'Improving Speaker Verification with Noise-Aware Label Ensembling and Sample Selection: Learning and Correcting Noisy Speaker Labels,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 32, pp. 2988-3001, 2024.- [67] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting,' Journal of Machine Learning Research , vol. 15, no. 1, pp. 1929-1958, 2014.- [68] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, 'ArcFace: Additive Angular Margin Loss for Deep Face Recognition,' in Proceedings of CVPR , 2019, pp. 4685-4694.- [69] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, 'Momentum Contrast for Unsupervised Visual Representation Learning,' in Proceedings of CVPR , 2020, pp. 9729-9738.- [70] M. Cuturi, 'Sinkhorn Distances: Lightspeed Computation of Optimal Transport,' NeurIPS , vol. 26, 2013.- [71] A. Nagrani, J. S. Chung, and A. Zisserman, 'Voxceleb: A Large-Scale Speaker Identification Dataset,' in Proceeding of Interspeech , 2017, pp. 2616-2620.- [72] J. S. Chung, A. Nagrani, and A. Zisserman, 'Voxceleb2: Deep Speaker Recognition,' in Proceeding of Interspeech , 2018, pp. 1086-1090.- [73] D. Cai, W. Cai, and M. Li, 'Within-Sample"
  },
  {
    "hash_code": "0f945c43e75565aae96798d569e4bcc8",
    "text": "ani, J. S. Chung, and A. Zisserman, 'Voxceleb: A Large-Scale Speaker Identification Dataset,' in Proceeding of Interspeech , 2017, pp. 2616-2620.- [72] J. S. Chung, A. Nagrani, and A. Zisserman, 'Voxceleb2: Deep Speaker Recognition,' in Proceeding of Interspeech , 2018, pp. 1086-1090.- [73] D. Cai, W. Cai, and M. Li, 'Within-Sample Variability-Invariant Loss for Robust Speaker Recognition Under Noisy Environments,' in Proceeding of ICASSP , 2020, pp. 6469-6473.- [74] N. Inoue and K. Goto, 'Semi-Supervised Contrastive Learning with Generalized Contrastive Loss and its Application to Speaker Recognition,' in Proceeding of APSIPA ASC , 2020, pp. 1641-1646.- [75] J. Kang, J. Huh, H. S. Heo, and J. S. Chung, 'Augmentation Adversarial Training for Self-Supervised Speaker Representation Learning,' IEEE Journal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1253-1262, 2022.- [76] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, 'A Simple Framework for Contrastive Learning of Visual Representations,' in Proceedings of the ICML , 2020, pp. 1597-1607.- [77] D. Snyder, G. Chen, and D. Povey, 'MUSAN: A Music, Speech, and Noise Corpus,' arXiv:1510.08484 , 2015.- [78] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, 'A Study on Data Augmentation of Reverberant Speech for Robust Speech Recognition,' in Proceeding of ICASSP , 2017, pp. 5220-5224.- [79] K. He, X. Zhang, S. Ren, and J. Sun, 'Deep Residual Learning for Image Recognition,' in Proceedings of CVPR , 2016, pp. 770-778.- [80] 'NIST 2016 Speaker Recognition Evaluation Plan,' 2016. [Online]. Available: https://www.nist.gov/system/files/documents/2016/ 10/07/sre16 eval plan v1.3.pdf- [81] Y. M. Asano, M. Patrick, C. Rupprecht, and A. Vedaldi, 'Labelling Unlabelled Videos from Scratch with Multi-Modal Self-Supervision,' NeurIPS , vol. 33, pp. 4660-4671, 2020.- [82] J. Munkres, 'Algorithms for the Assignment and Transportation Problems,' Journal of the society for industrial and applied mathematics , vol. 5, no. 1, pp. 32-38, 1957.- [83] S. H. Mun, M. H. Han, and N. S. Kim, 'SNU-HIL System for the VoxCeleb Speaker Recognition Challenge 2021,' in VoxSRC workshop , 2021.- [84] Z. Chen, J. Wang, W. Hu, L. Li, and Q. Hong, 'Unsupervised Speaker Verification Using Pre-Trained Model and Label Correction,' in Proceeding of ICASSP , 2023, pp. 1-5."
  },
  {
    "hash_code": "2134071db8b89d35ea7e39f4f02e5c87",
    "text": "## Facilitating the Integration of Ethical Reasoning into Quantitative Courses: Stakeholder Analysis, Ethical Practice Standards, and Case StudiesRochelle E. Tractenberg 1 , Suzanne Thornton 2 1 Collaborative for Research on Outcomes and -Metrics; and Departments of Neurology; Biostatistics, Bioinformatics &amp; Biomathematics; and Rehabilitation Medicine, Georgetown University, Washington, DC, USA 2 Department of Mathematics and Statistics, Swarthmore CollegeSwarthmore, PA, USA## Cite this chapter:Tractenberg  RE  &amp;  Thornton  S.  (2022/2024).  Facilitating  the  integration  of  ethical reasoning into quantitative courses: stakeholder analysis, ethical practice standards, and case studies. In, H. Doosti, (Ed.). Ethics in Statistics. Cambridge, UK: Ethics International Press.  pp  151-182  Originally  published  (verbatim)  in Proceedings  of  the  2022  Joint Statistical Meetings, Washington, DC . Alexandria, VA: American Statistical Association. pp. 1493-1519.Preprint available at StatArXiv: http://arxiv.org/abs/2401.01973## AbstractCase studies are typically used to teach 'ethics', but emphasize narrative. When the content of a course is focused on formulae and proofs, it can seem divergent or distracting - for both  instructor  and  learner  -  to  introduce  a  case  analysis.  Moreover,  case  analyses  are typically focused on issues relating to people: obtaining consent, dealing with research team  members,  and/or  potential  institutional  policy  violations.  While  relevant  to  some research, not all students in quantitative courses plan to become researchers, and ethical practice - of mathematics, statistics, data science, and computing - is an essential topic regardless  of  whether  or  not  the  learner  intends  to  do  research.  It  is  a  mistake  to  treat 'training in ethical practice' and 'training in responsible conduct of research' as the same thing,  just  as  incorrect  as  it  is  to  assume  that  'training  in  ethical  practice'  is  the  same irrespective of what the learner will actually be practicing. Ethical reasoning is a way of thinking that requires the individual to assess what they know about a potential ethical problem - their prerequisite knowledge, and in some cases, how behaviors they observe, are directed to perform, or have performed, diverge from what they know to be ethical behavior. Ethical reasoning is a learnable, improvable set of knowledge, skills, and abilities that enable learners to recognize what they do and do not know about what constitutes 'ethical practice' of a discipline, and in some cases, to contemplate alternative decisions about how to first recognize, and then proceed past, or respond to, such divergences. A stakeholder analysis is part of prerequisite knowledge, and can be used whether there is or is not an actual case or behavior/situation to react to. When teaching courses with primarily quantitative content, a stakeholder analysis is a useful tool for instruction and assessment of learning. It can be used to both integrate authentic ethical content and encourage careful quantitative thought. This paper discusses how to introduce ethical reasoning, stakeholder analysis, and ethical practice standards authentically in quantitative courses.Key Words: teaching  ethics,  ethical  reasoning,  ASA  Ethical  Guidelines,  stakeholder analysis, case analysis, ethical practice.## 1. Introduction' Ethics refers to standards and practices that tell us how human beings ought to act in the many situations  in  which  they  find  themselves-as  friends,  parents,  children,  citizens, businesspeople, professionals, and so on. Ethics is also concerned with our character. It requires knowledge, skills, and habits.' This definition of ethics comes from the Markkula Center for Applied ethics at Santa Clara University, where a wide variety of tools and resources  for  promoting  ethics  (and  applied  ethics)  are  created  and  housed.  However, 'teaching ethics' is too vague of a task for any individual instructor teaching statistics or data science in higher education. Recently, encouragement to incorporate 'ethics' into the curricula for statistics (ASA, 2014) and for data science (National Academies, 2018) has been published for undergraduate degree programs. The ASA Curriculum Guidelines for Undergraduate  Programs  in  Statistical  Science state  that,  'ethical  issues  should  be incorporated throughout a program' (p. 13), i.e., the program remains focused on statistical science  and  'ethical  issues'  should  be  encountered  'throughout'  that  program.  The"
  },
  {
    "hash_code": "85752325500e738ec8fb08b93cdf23f9",
    "text": "instructor teaching statistics or data science in higher education. Recently, encouragement to incorporate 'ethics' into the curricula for statistics (ASA, 2014) and for data science (National Academies, 2018) has been published for undergraduate degree programs. The ASA Curriculum Guidelines for Undergraduate  Programs  in  Statistical  Science state  that,  'ethical  issues  should  be incorporated throughout a program' (p. 13), i.e., the program remains focused on statistical science  and  'ethical  issues'  should  be  encountered  'throughout'  that  program.  The Guidelines for Assessment and Instruction in Statistics Education (GAISE) in Statistics Education (GAISE) College Report (GAISE College Report Revision Committee, 2016) also includes a goal, among the 'desired result of all introductory statistics courses', that specifically  targets  'an  awareness  of ethical  issues associated  with  sound  statistical practice' (emphasis in original; p. 8). In a nuanced difference, the National Academies of Sciences, Engineering, and Medicine (National Academies 2018) seems to go further by integrating ethics into the definition of data science: 'data science spans a broad(er) array of  activities  that  involve  applying  principles  for  data  collection,  storage,  integration, analysis, inference, communication, and ethics '. (emphasis added, p. 1) This inclusion of ethics  into  the  definition  of  the  domain  is  intentional;  the  authors  of  this  report,  the Committee on Envisioning the Data Science Discipline, note that their emphasis (on ethics in the definition of data science):'… underscores the centrality of studying the many ethical considerations that arise as workers engage in data science . These considerations include deciding what data to collect, obtaining permissions to use data, crediting the sources of data properly, validating the data's  accuracy,  taking  steps  to  minimize  bias,  safeguarding  the  privacy  of  individuals referenced in the data, and using the data correctly and without alteration. It is important that  students  learn  to  recognize  ethical  issues  and  to  apply  a  high  ethical  standard.' (emphasis added, p. 2).Two of the 2018 National Academies report action items (p.3) follow directly from the Committee's focus on the centrality of ethical professional practice in data science:Recommendation 2.4: Ethics  is  a  topic  that,  given  the  nature  of  data  science,  students should learn and practice throughout their education. Academic institutions should ensure that ethics is woven into the data science curriculum from the beginning and throughout.Recommendation 2.5: The data science community should adopt a code of ethics; such a code should be affirmed by members of professional societies, included in professional development programs and curricula, and conveyed through educational programs. The code should be reevaluated often in light of new developments.As has been argued elsewhere (Tractenberg 2022-A, 2022-B; Tractenberg, 2020; Hogan et al. 2017), the ASA Ethical Guidelines for Statistical Practice (ASA 2022) represent ethical practice  standards  that  comprise  both  statistics  and  data  science.  In  particular,  the concordance of the professional practitioner communities for statistics (i.e., the ASA) and computing (Association of Computing Machinery, ACM, 2018) have longstanding ethical practice standards that are highly aligned (2018 versions discussed in Tractenberg 2020; 2018 ACM and 2022 ASA versions discussed in Tractenberg 2022-A, 2022-B). To the extent that the ASA and ACM describe the ethical practice standards of professionals in computing, statistics, and data science, 'the data science community' already has access to  the  ASA  Guidelines  to  utilize  as  its  'code  of  ethics'.  Clearly,  the  professional  and scientific communities are placing increasing value and importance on the integration of 'ethics' or ethical content into the curriculum for degrees in statistics and data science. This content exists. A remaining challenge is how to integrate both the content and how to reason  with  it  into  the  entire  curriculum  (in  statistics  and/or  data  science).  Whenever courses  in  this  curriculum  are  primarily  quantitative,  featuring  formulae,  theories,  and proofs rather than applications, the opportunities to integrate ethical practice content appear to be limited.## 2. Options for 'teaching ethics'The ASA (2014), GAISE (2016), and National Academies (2018) recommendations are not  specific  about  what,  or  how,  ethical  content"
  },
  {
    "hash_code": "4669992e46296d93cd76d30e52eaebc6",
    "text": "to integrate both the content and how to reason  with  it  into  the  entire  curriculum  (in  statistics  and/or  data  science).  Whenever courses  in  this  curriculum  are  primarily  quantitative,  featuring  formulae,  theories,  and proofs rather than applications, the opportunities to integrate ethical practice content appear to be limited.## 2. Options for 'teaching ethics'The ASA (2014), GAISE (2016), and National Academies (2018) recommendations are not  specific  about  what,  or  how,  ethical  content  or  'ethical  issues'  can  effectively  be integrated into either a single course (e.g., a target for an introductory statistics course; GAISE 2016) or the  curriculum  (ASA  2014;  National  Academies  2018).  As  has  been discussed previously (Tractenberg, 2016?), simply attaching the ethical practice standards (ASA,  ACM)  to  the  syllabus  will  not  be  effective.  Limiting  instruction  to  in-class discussion  of  examples  of  unethical  application(s)  of  statistics  (e.g.,  unsubstantiable analyses supporting claims of voter fraud in 2020 election; deploying algorithms to steal Facebook data from unwilling or unknowing data donors in 2016), and how they violate laws,  rules,  or  ethical  practice  standards  definitely  engages  student  attention.  These examples also highlight the importance of learning statistics and data science - so, they don't simply present a social or cultural 'good' that is both abstract and possibly unrelated to  quantitative  content  (e.g.,  beneficence  and  nonmaleficence;  Beauchamp  &amp;  Childress 1984). In order to inculcate students with a sense of responsibility for ethical statistical and data science practice, a coherent approach to teaching, and to assessing learning, is needed throughout a semester or term.## 2.1 Case studiesCase studies are typically  used -  and  recommended - to teach 'ethics'. Cases that are available (the development and sharing of many cases have been supported by the National Science Foundation and National Institutes of Health) are universally contextualized well beyond formulae and proofs of theorems, lemmas, or corollaries. The National Institutes of Health Office of Intramural Research describes case studies for teaching/learning ethics this way: ' Research Ethics Cases are a tool for discussing scientific integrity. Cases are designed to confront the readers with a specific problem that does not lend itself to easy answers. By providing a focus for discussion, cases help staff involved in research to define or  refine  their  own  standards,  to  appreciate  alternative  approaches  to  identifying  and resolving ethical problems, and to develop skills for dealing with hard problems on their own .' 1 (emphasis added.)1 https://oir.nih.gov/sourcebook/ethical-conduct/responsible-conduct-research-training/annualreview-ethics-case-studiesIn addition to the fact that all those who teach statistical and data science practices have an obligation  to  teach  ethical  statistical  and  data  science  practice  (i.e.,  the  topic  of  ASA Principle Elements A12, G, and G1; ASA, 2022), the use of case studies as defined by the NIH can be generally challenging across statistical instruction for several reasons:1. Unless  the  instructor  is  willing  to  add  instruction/dedicate  time  to  providing feedback on narrative responses from students (e.g., essays, open-ended responses), there is no way to effectively assess if/how students are learning or progressing.2. Without  written  work  reflecting  the  students'  engagement  with  the  case,  the instructor has no way to assess learning.3. Cases may present interesting ethical cases without relating in an authentic way to applying the ethical content.4. If a course does not utilize narrative - in content or in assessment - then teaching and  assessing  learning  with  case  studies  might  be  very  different  from  what instructors and students are familiar with. This can create challenges for students as well as for the instructors.Moreover, it is essential to point out that the National Academies and  ASA recommendations are not about research ethics but about ethical practice . This means that materials currently available to teach 'research ethics' - none of which have been created to support quantitative instructors or courses with primarily quantitative content - will be useful  for  teaching  'ethical  practice'.  That  is,  special  consideration  for  how  to  teach practitioners  to  be  ethical  when  they  use  mathematics  or  statistics  is  fundamentally different from"
  },
  {
    "hash_code": "ce2804c0ed49e4ce834e8dd906445b0c",
    "text": "familiar with. This can create challenges for students as well as for the instructors.Moreover, it is essential to point out that the National Academies and  ASA recommendations are not about research ethics but about ethical practice . This means that materials currently available to teach 'research ethics' - none of which have been created to support quantitative instructors or courses with primarily quantitative content - will be useful  for  teaching  'ethical  practice'.  That  is,  special  consideration  for  how  to  teach practitioners  to  be  ethical  when  they  use  mathematics  or  statistics  is  fundamentally different from training researchers in 'responsible conduct of research'.When the content of a course is focused on formulae and proofs, it can seem divergent or distracting to introduce a case analysis. Moreover, case analyses are typically focused on issues relating to people: obtaining consent, dealing with research team members, and/or potential institutional policy violations. Finally, cases describe actions taken by multiple parties with different roles. This is certainly one way to help students learn about ethical guidelines (or applicable rules, policies, and laws) - as they determine which of these were violated (or might have been threatened) by actors in the case. However, this approach has no way to help students appreciate how ethical practice standards -such as the ASA Ethical Guidelines  for  Statistical  Practice  (or  other  rules)  can  be  used  to  practice  ethically  in general. The case approach emphasizes identification of 'what went wrong' -i.e., require the learner to understand the context of scientific application of what they are in the course to begin learning, and then typically, the articulation of a response. The structure of cases (as defined by the National Institutes of Health) is to be intentionally ambiguous and not include objectively right or wrong responses. Thus, 'what to do' when faced with similar situations may not be made clear to learners, or, multiple options may be presented and a clear way to make a decision about what to do in response might again be unclear.In addition to the fact that beginners or those who are starting to integrate their quantitative knowledge might not appreciate the multitude of complexities involved in participating in a scientific project or team, teaching 'responsible conduct of research' with cases that are ill-defined makes it difficult to consider how to identify similar - also ill-defined - situations in the future. Moreover, cases that present completed events and ask students to determine 'what went wrong' helps to promote the idea that avoiding bad behavior, or avoiding being caught in bad behavior, is what constitutes ethical research. A final problem with typical case-based teaching of ethics is that the majority of materials that exist were developed for academic and research situations specifically. Newer resources for 'ethical AI', such as those from the Princeton Dialogues on AI and Ethics (https://aiethics.princeton.edu/casestudies/case-study-pdfs/), are not specific to research, but they do feature 'ethical issues' that are general. Unfortunately, like older cases, these do not relate to 'ethical practice'. They also tend to be self-contained, and unrelated/not relatable to quantitative material.While  quantitative  courses  are  undoubtedly  relevant  to  students  in  research  or  science majors, not all students in quantitative courses plan to become researchers, and ethical practice - of mathematics, statistics, data science, and computing - is an essential topic regardless  of  whether  or  not  the  learner  intends  to  do  research.  In  addition  to  the  four classroom-based,  instructor-specific difficulties that arise  when  quantitative  course instructors want to integrate ethical content into their course, the available cases will often be quite far from being both useful and accessible to these instructors.## 2.2 Teaching Ethical Reasoning instead of 'ethics'' Ethics is the effort to guide one's conduct with careful reasoning . One cannot simply claim 'X is wrong.'; Rather, one needs to claim 'X is wrong because (fill in the blank)'.' (Briggle &amp; Mitcham, 2012, p. 38). (emphasis added)Ethical  reasoning is  a  process  that  can  be  learned  and  improved.  It  can  be  taught and assessed. There are six elements of knowledge, skills, and abilities (KSAs) that are required for ethical reasoning (Tractenberg &amp; FitzGerald, 2012; Tractenberg 2022-A):1. Identify and 'quantify' prerequisite knowledge2. Identify decision-making frameworks.3. Identify or recognize the ethical issue.4. Identify and evaluate alternative actions (on the ethical issue).5. Make and justify a decision.6. Reflect on the decision.While it has been argued that ethical reasoning (rather than ethics) is what statistics and data science curricula should be teaching and emphasizing, it can"
  },
  {
    "hash_code": "e9c470cca47765a93a31c0cffa608ef5",
    "text": "There are six elements of knowledge, skills, and abilities (KSAs) that are required for ethical reasoning (Tractenberg &amp; FitzGerald, 2012; Tractenberg 2022-A):1. Identify and 'quantify' prerequisite knowledge2. Identify decision-making frameworks.3. Identify or recognize the ethical issue.4. Identify and evaluate alternative actions (on the ethical issue).5. Make and justify a decision.6. Reflect on the decision.While it has been argued that ethical reasoning (rather than ethics) is what statistics and data science curricula should be teaching and emphasizing, it can be challenging to get to, or beyond, KSA #3 (identify or recognize the ethical issue) when the course is featuring formulae,  proofs,  and  theorems.  Critically, proofs  and  formulae  typically  stipulate assumptions and approximations. Whenever these assumptions and approximations do not hold, cannot be met, or cannot be verified, it creates an opportunity to consider what the impacts of these failures might be . Even in a superficially abstract and quantitative context like  a  proof,  the  importance  of  assumptions  and  approximations  can  be  leveraged  to introduce ethical practice content.As noted, ethical reasoning is a way of thinking that requires the individual to assess what they know about a potential ethical problem - their prerequisite knowledge, and in some cases to contemplate alternative decisions for responding to that problem. However, when learning  quantitative  content,  reasoning,  and  skills  (e.g.,  in  mathematical  statistics, mathematics, calculus, etc.), it can be distracting to students as well as instructors to have to divert attention to person-based considerations like behavior. There are six elements of knowledge, skills, and abilities (KSAs) that are required for ethical reasoning. These are described in the context of mathematical content (i.e., formulae, proofs, theorems):1. Identify  and  'quantify'  prerequisite  knowledge :  a  practitioner  should  have sufficient  familiarity  with  the  ASA  Ethical  Guidelines  for  Statistical  Practice (GLs) to identify at least one of the 72 elements that might be relevant to a given proof or theorem. This prerequisite knowledge is essential for ethical practice, as well as for deciding what to do when faced with requests to practice unethically,2. so ensuring learners encounter them with respect to the mathematical foundations (as well as in applications later in the learning path) is important.2. Identify decision-making frameworks . The ASA GLs represent a ' virtue ethics ' decision-making framework, which can be summarized as, 'what would the (ideal) ethical practitioner do in this case?' Alternatively, a utilitarian framework can be summarized as, 'how can benefits be maximized while harms are minimized?' Either of these approaches can be useful when considering what would happen if assumptions and approximations do not hold.3. Identify or recognize the ethical issue : in quantitative courses, an ethical issue can arise from failing to considering the stakeholders - and impacts on them when assumptions and approximations do not hold.4. Identify and evaluate alternative actions (on the ethical issue). There are always a set of decisions that can be made in any circumstance: either a) do nothing, b) consult or confer with a peer or a supervisor - using the professional guidelines or other resources, or c) report violations of policy, procedure, ethical guidelines, or law. When assumptions and approximations do not hold, the ethical practitioner is transparent, but 'do nothing' would mean  'not communicating that the theorem/proof is not applicable'-i.e., not being transparent. That is clearly always an option, but never an ethical one.5. Make and justify a decision . A decision about what to do in the face of the ethical challenge that has been identified will be based on the alternatives. Justification would compare and contrast the alternatives from either the utilitarian or the virtue perspective; this is especially important if these perspectives ever differ on the course of action either supports best.6. Reflect on the decision . For a course with mostly quantitative content, reflection may need to be targeted so as not to add too much narrative work (for the students to write and the instructor to grade). Examples of streamlined reflections could be: 1)  ask  for  the  most  problematic  assumptions/approximations  of  the  course  or section in the student's opinion, with a brief explanation of why that one is the most  problematic;  2)  ask  students  to  rank  order  most  to  least  problematic approximations  (NB:  reflection  cannot  be  graded  'right  or  wrong',  but  'do nothing'  is  always  unethical,  so  questions  about  'how  often  do  you  think mathematics practitioners decide to 'do nothing'"
  },
  {
    "hash_code": "5accdc5e6033f84541af36ae7d9d977c",
    "text": ")  ask  for  the  most  problematic  assumptions/approximations  of  the  course  or section in the student's opinion, with a brief explanation of why that one is the most  problematic;  2)  ask  students  to  rank  order  most  to  least  problematic approximations  (NB:  reflection  cannot  be  graded  'right  or  wrong',  but  'do nothing'  is  always  unethical,  so  questions  about  'how  often  do  you  think mathematics practitioners decide to 'do nothing' will generate narrative that will be interesting, but might be hard to grade consistently).Importantly, ethical reasoning can be integrated into a course, so that students can learn and practice all six aspects. However, unless the instructor also has cases or vignettes, ethical reasoning does not go beyond KSA #3, Identify or recognize the ethical issue . Moreover, in addition to learning the KSAs of ethical reasoning, and the ASA GLs (or similar ethical practice standard), the integration of ethical reasoning into a course requires additional material against which students can compare options in order to identify the ethical issue. For example, if the ASA Ethical Guidelines are used, then given a case or vignette, students can identify which ASA Ethical Guideline Principle(s) or elements are relevant - or have been violated - leading to the ethical issue under consideration. This can be done with a matching task, check boxes, or asking students to order a set of GL elements in terms of their relevance or importance to a violation of assumption or approximation. For eliciting narrative responses, instructors can choose a formula, proof or theorem and ask,  'What  decisions  are  made/have  been  made  in  (or  in  this  problem's  use  of)  this (proof/theorem)? Are assumptions, definitions, or approximations decisions? Can these affect stakeholders? If so, how; if not, why  not?'## 2.3 Stakeholder analysis: a relevant subset of Ethical Reasoning' Those who fund, contribute to, use, or are affected by statistical practices are considered stakeholders. The ethical statistical practitioner respects the interests of stakeholders while practicing in compliance with these Guidelines .' (ASA 2022; Principle C). As such, the impact of violations of assumptions or failures of approximations on stakeholders can be easily leveraged to integrate content about ethical statistics and data science practice into a quantitative course. Stakeholder analysis is part of Ethical Reasoning KSA #1, prerequisite knowledge (Tractenberg 2022-A, 2022-B). Importantly, stakeholder analysis can be used whether  there  is  or  is  not  an  actual  case  or  behavior/situation  to  react  to,  and  is  also independent  of  the  use  of  an  ethical  practice  standard  (Tractenberg,  2022-C).  Thus,  a stakeholder  analysis  can  be  useful  to  facilitate  ethical  practice  simply  by  alerting  the practitioner to harms and benefits - and to which stakeholders those accrue - of any step in their workflow or analysis. Even without a specific set of ethical practice guidelines in mind, the choice that minimizes harms can be made using the stakeholder analysis. It can also be used to augment the prerequisite knowledge of a formal ethical reasoning-based case analysis (Tractenberg 2022-B).In  order  to  integrate  a  stakeholder  analysis  into  a  quantitative  course,  whenever  an approximation or assumption is made, attention can be brought to what happens if the assumption doesn't hold, at the limits of the approximation, or the inappropriate (doesn't fit the situation, isn't right for the data, assumes more than is known/knowable) use of the definition. Table 1 (adapted from Tractenberg 2019) focuses attention on harms/benefits to  create  a  simplified  'case  analysis'  that  can  be  utilized  with  any  formula,  proof,  or theorem if approximations and/or assumptions are included.Table 1: Stakeholder Analysis of Harms and Benefits that accrue to seven stakeholder typesStakeholder 1 :HARM 4,5BENEFIT 4,5YOU 2,3Your boss/clientUnknown individuals 2EmployerColleaguesProfessionPublic/public trust## Notes on stakeholder analysis table:1. Knowing to whom harms may accrue can guide learners to where the ASA GLs or other ethical practice standards can assist in decision making.2. Articulating the harms that may accrue to YOU (the individual) is essential for following advice to 'treat others' data as you would your"
  },
  {
    "hash_code": "209bd572d8122c33d7578f4be8b294a9",
    "text": "ue to seven stakeholder typesStakeholder 1 :HARM 4,5BENEFIT 4,5YOU 2,3Your boss/clientUnknown individuals 2EmployerColleaguesProfessionPublic/public trust## Notes on stakeholder analysis table:1. Knowing to whom harms may accrue can guide learners to where the ASA GLs or other ethical practice standards can assist in decision making.2. Articulating the harms that may accrue to YOU (the individual) is essential for following advice to 'treat others' data as you would your own' (Loukides, Mason &amp; Patil, 2018: Chapter 3). Individuals need to recognize the harms that can accrue to  them before  they  can  compare  harms  to  themselves  and  harms  to  others. Moreover, 'others' data' could relate to data belonging to your boss/client, your employer, unknowable others, the scientific community, or the public. Recognizing whether or not benefits or harms accrue to these different types of 'others' is the only way to make a decision about how one wants other people to treat one's data: in someone else's table, you are the client, an unknowable other, or part of the public. Are some harms 'worse' or some benefits 'better'? Any3. interested instructor can augment  the  stakeholder  analysis with additional discussion questions like these.3. If there are no recognizable harms, and plausibly no 'unknowable' harms &lt;that would be caused by a failure of an assumption or approximation&gt;, then there can be  no  conflict.  It  is  really  important  to  recognize  whether  something  truly  is unknowable or if it is actually something that can be known - but we/the learner just don't/doesn't know it. The key words here are 'recognizable' and 'plausible' - the failure to recognize something doesn't mean it does not exist. And, beware of straw man 2 or red herring 3 harms!4. If  there  are  plausible  harms  (or  benefits)  that  cannot  be  identified,  but  are believed/suspected  to  exist,  then  there  is  insufficient  information  to  make  a decision and more information is needed. Recognizing this - instead of making an uninformed decision - is currently not part of the norm. Learning how to use this table  and  complete  a  case  analysis  is  essential  for  enabling informed decisions about ethical challenges for current and future practitioners.5. All harms are not the same; all the benefits are not the same; and harms and benefits are not exchangeable.Whenever  assumptions  and/or  approximations  are  not  met,  there  may  be  impacts  on stakeholders. For example, if assumptions are not met and (but) the function/algorithm works as the employer desires, a harm accrues to the employer - because they're not getting accurate information or applicable/reproducible results but a benefit may seem to accrue to the individual ('you') because the work gets completed. If the timeline is tight, the boss may also seem to benefit from an approximation being used (because it looks like the boss got the work  done  on  time). Augmenting  the  discussion  of assumptions  and/or approximations for one construct (proof, theorem, definition, etc) can be limited to one per chapter, one per homework problem set or quiz, or one per week, depending on the ease of doing  this  given  the  content/topic.  Repeating  the  same  exercise  for  different  proofs  or assumptions/approximations  will  ensure  that  students  have  multiple  opportunities  to recognize and become competent at recognizing the impact of shortcuts and rules of thumb on stakeholders - and, offer opportunities to connect ethical practice ideas to real harms and benefits. Stakeholder analysis tables can be created with check boxes, matching, or other  forms  of  tasks  that  limit  the  need  for  narrative  responses  from  students,  but  still generate evaluable work products that can be graded. For eliciting narrative responses, instructors can choose one problem from each homework problem set and ask, 'Who could be  a  stakeholder  when  solving  a  problem  like  (one  chosen  from  the  homework assignment)?'## 2.4  Using  the  ASA  Ethical  Guidelines  explicitly:  a  relevant  subset  of  Ethical ReasoningCombining authentic ethical content with content that is more quantitative (i.e., featuring formulae, theorems, and proofs) is difficult, but directly incorporating the ASA Eth"
  },
  {
    "hash_code": "61248ee36b853e1e70186b78c33aebca",
    "text": "uable work products that can be graded. For eliciting narrative responses, instructors can choose one problem from each homework problem set and ask, 'Who could be  a  stakeholder  when  solving  a  problem  like  (one  chosen  from  the  homework assignment)?'## 2.4  Using  the  ASA  Ethical  Guidelines  explicitly:  a  relevant  subset  of  Ethical ReasoningCombining authentic ethical content with content that is more quantitative (i.e., featuring formulae, theorems, and proofs) is difficult, but directly incorporating the ASA Ethical Guidelines (included in the Appendix) can be done without a focus on ethical reasoning or stakeholder  analysis.  For  any  given  homework  problem  -  particularly  those  where applications of the formula or theorem are highlighted, questions can be asked that require2 'Straw Man': defined as 'an argument, claim or opponent that is invented in order to win or create an argument', Cambridge English Dictionary.3 'Red Herring': defined as 'something that takes attention away from a more important subject', Cambridge English Dictionary.a matching or check-box answer (multiple choice), short answer, or narrative response. Questions can be structured in the following form:Choose a problem from (a homework set) and consider what would happen if, instead of following ASA Ethical Guideline Principle A, the individual didn't use appropriate methodology? Is there an application of (what you're learning) where failing to follow Principle A could negatively impact any stakeholder - including yourself?This same question can be (re)formulated for all of the ASA GL Principles (A-H). Since the Appendix is focused on organizations and institutions, those items are simple to exclude from this type of question; similarly since Principle G is about leadership type roles, those might also be omitted. (Principle G and the Appendix could be leveraged to encourage students  to  consider  attributes  of  the  workplaces  they  would  like  to  join  once  they graduate.) Instructors would ideally mention the GL Principles repeatedly throughout the course and other materials, and it is essential that the method by which students respond (narrative,  short  answer,  or  multiple  choice)  is  consistent  with  the  instructor's  learning objectives for the ethical content in the course (Tractenberg 2020; Tractenberg et al. 2020).Other  GL  Principle  specific  questions,  that  require  the  same  attention  to  response generation and consistency with course learning objectives include:Choose a problem from (a homework set) and consider what would happen if, instead  of  following  Principle  D,  someone  didn't  follow  applicable  rules  or guidelines? Is there an application of (what you're learning) where failing to follow Principle D could negatively impact any stakeholder - including yourself? (e.g., dual use of innovation, military and &lt;non-military&gt;)Consider what would happen if, instead of following Principle E, someone on your working team wanted you to violate the ASA Ethical Guidelines because their profession  used  different  Guidelines?  Is  there  an  application  of  (what  you're learning)  where  failing  to  follow  the  ASA  Ethical  Guidelines  could  negatively impact any stakeholder - including yourself?Do activities that could negatively impact the profession as a stakeholder have the potential  to  also  negatively  impact  yourself?  Consider  the  alternative  also:  do activities  that  positively  impact  the  profession  as  a  stakeholder  also  positively impact yourself?It requires effort to introduce the idea of a stakeholder into a course in an authentic way. Also there have to be actions, or reactions, that the practitioner must make, for there to be plausible stakes for people to hold (given that the ' those who fund, contribute to, use, or are affected by statistical practices are considered stakeholders ' ). This is why we focus on the effects of failures of assumptions or approximations to hold as the point at which purely quantitative content (formula, proof, theorem) can begin to bear the integration of ethical content. These failures or violations require attention to both the formula, proof, or theorem (i.e., the actual target content of the course or lesson) and also to the possible harms or benefits that the otherwise purely quantitative content can have on stakeholders.## 3. Integrating ethical content in a Mathematical Statistics courseAn  undergraduate  course  in  Mathematical  Statistics  -  typically  the  first  exposure  for students in the major to the 'underpinnings' of statistical inference - has been revised for the 2022-23 Fall term to integrate ethical practice content. This was undertaken by one of us (ST) with input from the other"
  },
  {
    "hash_code": "47f9ff3f8999a3fb5f69cfdf4a85fc56",
    "text": "to both the formula, proof, or theorem (i.e., the actual target content of the course or lesson) and also to the possible harms or benefits that the otherwise purely quantitative content can have on stakeholders.## 3. Integrating ethical content in a Mathematical Statistics courseAn  undergraduate  course  in  Mathematical  Statistics  -  typically  the  first  exposure  for students in the major to the 'underpinnings' of statistical inference - has been revised for the 2022-23 Fall term to integrate ethical practice content. This was undertaken by one of us (ST) with input from the other (RET). This section presents the outline of the syllabus (without  timings  so  the  reader  can  map  to  their  own  term  (summer  session  or quarter/semester). Subsequent sections discuss the features and decisions represented in the syllabus, a fuller version of which is incliuded in the Appendix.## 3.1 Syllabus outlineLearning objectives (referred to throughout the syllabus)1. Understand  frequentist  and  Bayesian  methods  for  parameter  estimation  and common approaches to evaluate and compare estimators.2. Work with asymptotic theorems to characterize the behavior of common types of estimators.3. Understand how to analytically derive a Bayesian posterior distribution and how to interpret a Bayesian credible interval and demonstrate familiarity with different types of priors.4. Understand the important role of likelihood functions for hypothesis testing in both Bayesian and frequentist frameworks5. Understand the relationship between frequentist confidence interval estimation and hypothesis testing.6. Familiarity with common types of optimal testing strategies.7. Construct and interpret frequentist p-values for hypothesis tests and interpret the error rates.8. Identify/define model parameters and state statistical inferential questions in terms of these parameters for various common, realistic study settings.9. Contextualize statistical methods and theory in science and policy at large and develop a habit of mind informed by how the application of such methods can affect stakeholders.10. Understand what it means to be a steward of statistics, how to be stewardly, and why that is important. 411. 1.1 Review of probability12. 1.2 Methods to derive estimators13. 1.3 Intro to large sample theory of estimation14. 1.4 Interval estimationCourse Content and Order of Topics Introduced:Unit One -Thoughtful use of 5 estimation techniquesRelevant Learning Objectives: 1, 2, 3, 94 Two learning outcomes reflect the instructor's intention to prioritize not only the contextualization of statistical methods and theory in real world applications, but also the facts that a) statistical methods and theories have potential impacts on stakeholders; b) these impacts and stakeholders are important enough to feature at the learning outcome level (not as an aside); and c) there is more to learning the mathematical underpinnings of statistics than 'just math' stewardship, and responsibilities to the discipline, are also learnable and important to internalize.5 The addition of ' Thoughtful use of ' to the Unit title signals to students that the focus moves beyond 'use' and learners will be expected to be thoughtful (not simply memorizing) in their selection and application of estimation techniques.## Example activities for Unit One:- -Read Cargo cult statistics and the scientific crisis (Stark and Saltelli, 2016) before class and in-class discussion on what students found surprising and why.- -Groups  instructed  to  list  as  many  stakeholders  as  students  can  identify  in  the classroom and, hypothetically, in the workplace where they may use statistics.- -Instructor led stakeholder analysis regarding the application of an estimator with desirable large sample properties in the development of a drug.Unit Two - Statistical inference and stewardship 6 Relevant Learning Objectives: 4, 5, 6, 7, 10- 2.1 What's in a likelihood- 2.2 Neyman-Pearson paradigm for optimal testing- 2.3 Trustworthy testing## Example activities for Unit Two:- -Read  blurb  from The  preparation  of  stewards  with  the  Mastery  Rubric  for Stewardship: Re-envisioning the formation of scholars and practitioners (Rios, Golde  &amp;  Tractenberg;  2019)  before  class  followed  by  in-class,  small  group discussion about the meaning of the phrase \"steward of statistics\".- -Individual  reflection  on  what  it  means  to  be  a  steward  of  statistical  practices handed in as an ungraded assignment that will be revisited later in the semester.- -Student groups complete an in-class worksheet identifying potential stakeholders in the application of hypothesis testing"
  },
  {
    "hash_code": "4f0edabaa9bbba60fa04a16349373b81",
    "text": "ric  for Stewardship: Re-envisioning the formation of scholars and practitioners (Rios, Golde  &amp;  Tractenberg;  2019)  before  class  followed  by  in-class,  small  group discussion about the meaning of the phrase \"steward of statistics\".- -Individual  reflection  on  what  it  means  to  be  a  steward  of  statistical  practices handed in as an ungraded assignment that will be revisited later in the semester.- -Student groups complete an in-class worksheet identifying potential stakeholders in the application of hypothesis testing.- -Individual  assessment  item  that  tasks  students  with  making  a  decision  from  a hypothesis test that is justified with a stakeholder analysis.Unit Three Disciplinary best practices 7 for common study designs and analyses Relevant Learning Objectives: 8, 9, 10- 3.1 One and two sample means- 3.2 ANOVA- 3.3 Categorical data- 3.4 Linear regression## Example activities for Unit 3:- -Student groups assigned various principles from the American  Statistical Association's Guidelines for Ethical Statistical Practice (Ie. Principle A through H) complete a worksheet where they identify which principles specifically they think are  most  relevant  for  statistical  estimation  and  which  are  most  relevant  for inference. Followed by an instructor-led discussion where the considerations for estimation and inference are compared and contrasted.6 The addition of ' and stewardship ' to the Unit title signals to students that the focus on inference includes stewardly consideration.7 The addition of ' disciplinary best practices ' for common designs and analyses informs students that respect for the discipline (i.e., stewardship) is equally important for them to learn as the designs and analytic methods. The Unit title transmits the instructor's intention that students develop a stewardly, discipline- rather than results-centered, mindset.- -In-class group activity to identify which Ethical Principles are most relevant for each  statistical  method  (i.e.two  sample  inference,  ANOVA  models,  inference about categorical data, and inference with linear regression models).- -Individual  reflection  on  what  the  phrase  \"steward  of  statistics\"  means  to  each student.- -Individual  reflection  on  students'  projected  growth  as  a  steward  of  statistical practice incorporated into end of course evaluation. (Students' are sent a copy of their anonymous responses.)## 3.2. Notes on syllabus elements : Unit One. Motivate and contextualizeThe first unit of the semester serves to both motivate and provide language for ethical reasoning  in  statistics.  The  content  is  focused  on  assuring  student  facilities  with  the prerequisite knowledge from their prior coursework in mathematics and statistics. In Week 1, students are asked to read Cargo cult statistics and the scientific crisis (Stark and Saltelli, 2016).  This  article  presents  an  approachable  argument  for  statistical  practitioners  to carefully consider potential consequences of their statistical choices, particularly on the scientific crisis and subsequent denigrations to the credibility of science generally. Students become aware of potential stakeholders though they may not yet have the language to identify them as such.In previous academic years the syllabus has not featured mention of ethical content (such as \"stewardship\" and \"best practice\"), but now it not only is featured in the syllabus but ethics-relevant  discussions  also  begin  in  the  earliest  lectures.  The  syllabus  above communicates an intention to inculcate a sense of professional identity, ethical obligations to  respect  stakeholders,  and  the  idea  that  'even  the  mathematical  underpinnings'  of statistics  accrue  responsibilities  (e.g.  assuring  that  approximations  and  assumptions  are plausible). To optimize the chances of meeting learning objectives 9 and 10, this integration needs to begin at the earliest point in the term and carry through.A  group  activity  over  subsequent  weeks  could  involve  asking  students  to  identify stakeholders  in  different  vignettes.  In  addition  to  recognizing  stakeholders  in  diverse contexts, students are also encouraged to identify diverse stakeholders, for example, within the classroom itself. This prompts learners to expand their perspective beyond a generic teacher-student relationship to promote the realization that their peers and maybe even the entire department are stakeholders to consider. Students can then begin to practice shifting their perspectives while they are in class. This permits exploration along many, possibly under-appreciated, lines of questioning about the statistical material which, in this unit, is primarily a mathematical introduction to estimation. In-class and homework activities"
  },
  {
    "hash_code": "f1fd53ae407326d9d10e3cda70654399",
    "text": "identify stakeholders  in  different  vignettes.  In  addition  to  recognizing  stakeholders  in  diverse contexts, students are also encouraged to identify diverse stakeholders, for example, within the classroom itself. This prompts learners to expand their perspective beyond a generic teacher-student relationship to promote the realization that their peers and maybe even the entire department are stakeholders to consider. Students can then begin to practice shifting their perspectives while they are in class. This permits exploration along many, possibly under-appreciated, lines of questioning about the statistical material which, in this unit, is primarily a mathematical introduction to estimation. In-class and homework activities are intended  to  familiarize  students  with  tools  and  their  respective  properties;  introduce considerations  of,  and  beyond,  mathematical  characteristics  in  estimation;  and  practice shifting perspectives and developing some fluency about these quantitative ideas through discussion/communication with peers.Near the end of the unit, once the statistical reasoning and mathematical mechanics behind selecting  estimators  has  been  practiced  (in  homeworks)  and  assessed  (in  quizzes),  the instructor  presents  a  specific  vignette  in  which  to  consider  an  estimation  decision.  For example, having learned about the general conditions under which the distribution of a maximum likelihood estimator converges to a Gaussian distribution, consider the use of an asymptotic confidence interval  to estimate any moment or parameter. Any vignette can be created,  with  the  overall  intention  of  encouraging  the  realization  that  statistical  and mathematical properties of estimators are theoretically helpful, but cannot mitigate poor design or weak/incorrect understanding of population/sample.## 3.3 Notes on syllabus element s: Unit Two. From examples to practiceMoving into the second unit, students have been introduced to the concept of stakeholders and have some familiarity with identifying, and strategically assessing, the pros and cons of statistical decisions through a stakeholder analysis. Now as they are introduced to the concept of statistical hypothesis testing, they will also spend some time continuing to build their fluency while considering specific ethical guidance, stewardly responsibilities, and stakeholder impacts that are related to inference. The steward is the individual to whom 'we can entrust the vigor, quality, and integrity of the field' (Golde &amp; Walker, 2006: p. 5), and is someone who 'will creatively generate new knowledge, critically conserve valuable and  useful  ideas,  and  responsibly  transform  those  understandings  through  writing, teaching,  and  application'  (Golde  &amp;  Walker,  2006:  p.  5).  Rios,  Golde  &amp;  Tractenberg (2019) articulated the knowledge, skills, and abilities of the disciplinary and professional steward, extending the original definition (from describing the doctorate holder in 2006) to describing  any  practitioner  in  a  field  or  discipline.  In  order  to  make  stewardship  into something that was learnable and improvable, like ethical reasoning, a unique stewardship KSA was developed '…to describe the responsibility to recognize when these behaviors need to be applied or modeled... We have called this KSA 'requisite knowledge/situational awareness'  to  capture  the  attention  that  would  be  given  to  standards  of  professional practice (if they exist) during education or training, or when orienting new employees in the  workplace.'  (Rios  et  al.  p.  11  of  27).  Understanding  the  construct  of  stewardship provides a frame of reference for the student, in terms of understanding that statistical practitioners  incur  obligations  to  stakeholders  because  the  stewardly  practitioner  is entrusted by these stakeholders to practice ethically. Outside of class, students can be asked to read sections of Rios, Golde &amp; Tractenberg (2019), for example, followed by in-class small group discussions about what the phrase \"steward of statistics\" means to them upon their introduction to the concept of stewardship.Having  the  ASA  Ethical  Guidelines  (2022)  as  a  reference  reinforces  to  students  that 'statistical  practice'  is  much  more  than  simply  choosing  or  obtaining  a  data  set  and  a software program, and combining those together. There are 8 Principles and an Appendix, with a total of 72 elements. These are not valuable if they are simply memorized, so the activities of reading through the Guidelines to identify all the elements that might pertain, or that would be followed by a stewardly practitioner, can help to initiate the development of a sense of professional identity. Within"
  },
  {
    "hash_code": "b52aea5a58b2cab8cd7979add776805c",
    "text": "2022)  as  a  reference  reinforces  to  students  that 'statistical  practice'  is  much  more  than  simply  choosing  or  obtaining  a  data  set  and  a software program, and combining those together. There are 8 Principles and an Appendix, with a total of 72 elements. These are not valuable if they are simply memorized, so the activities of reading through the Guidelines to identify all the elements that might pertain, or that would be followed by a stewardly practitioner, can help to initiate the development of a sense of professional identity. Within the context of hypothesis testing, students learn the extent to which automated software can/can not assist in making decisions. In class, students will practice using a stakeholder analysis to explore the intricacies in formulating a hypothesis to test, in addition to assessing the fallout of various errors. Considering a simple  analysis  with  two  stakeholders,  the  statistical  practitioner  and  their  employer, groups can be assigned to assess potential harms and benefits for, say, the choice of null and alternative, the decision to focus on power or error rates, using a p-value to make a decision, or using a Bayesian posterior to make a decision.A  group  activity  over  subsequent  weeks  is  to  engage  in  discussions,  or  include  the identification, of ASA principles that are relevant for hypothesis testing (in general), and for the selection, use, and reporting of results of hypothesis testing methods. These can be in multiple choice and short answer formats (e.g., list the Principle and specific elements that are relevant; and/or give a brief explanation about choosing one or another Principle or element). Additionally, student engagement with the stakeholder analysis can continue and become more advanced. For example, homework or in-class problems can request stakeholder  analysis  for  the  choice  of  a  hypothesis  testing  method,  or  its  use  to  make decisions or draw conclusions.In previous academic years, the statistics course syllabi had not featured the ASA Ethical Guidelines. Instead, roughly 30 minutes of class time was dedicated to an activity that introduced  students  to  the  Ethical  Guidelines  for  Principle  B:  Integrity  of  Data  and Methods. First, the instructor presented these guidelines to the students, having a different person read each principle aloud. Then, students were given time to individually reflect on a real application of statistics which, unbeknownst to them, was an application with known ethical  violations.  During  the  reflection,  students  were  prompted  to  identify  which guidelines may have been violated and to consider what additional information they may need to determine if any ethical violations had occurred. Finally, the exercise concluded with the instructor revealing the known issues in the statistical analysis and provided some historical context as to how these issues were discovered. This activity occurred later in the semester, once students had been introduced to specific statistical methods for testing and estimation. By contrast, in the syllabus presented here, there are repeated opportunities for  students  to  revisit  the  Ethical  Guidelines  and  these  are  couched  within  a  larger framework  of  stewardship.  The  intentions  are:  a)  to  focus  student  attention  on  these features of ethical practice; b) to get students to engage with various aspects of 'statistical practice' that should be done ethically; and c) to allow students sufficient time to consider and work with the Ethical Guidelines, that can support development of the desired habits of  mind  that  have  the  strongest  likelihood  of  enduring  beyond  the  end  of  the  course. Practically speaking, at the end of the second unit, students will have begun to develop an ethically informed schema for answering questions about data with statistics. At this stage in the term, students are presented with a framework through the concept of stewardship though  they  may  not  yet  understand  how  this  concept  could  be  relevant  for  them  as potential statistical practitioners in the future.## 3.4 Notes on syllabus elements : Unit Three. Independent and collaborative critical thinkingThis semester-long course focuses on the mathematical and contextual considerations that are  critical  for  effective  statistical  practice.  The  intention  throughout  the  syllabus  and course is to emphasize that effective statistical practice is more than just the application of mathematical principles, or the use of software or execution of analysis. Instead, critical thinking is required throughout statistical practice as reflected in the learning objectives of the course. During the final third period of the term, students are regularly tasked with synthesizing  what  they  have  learned.  They  will  apply  the  knowledge  they  have accumulated regarding estimation, parameter and method selection, and"
  },
  {
    "hash_code": "569e0a98c6734d4c2ba7ba0b8a737b36",
    "text": "on the mathematical and contextual considerations that are  critical  for  effective  statistical  practice.  The  intention  throughout  the  syllabus  and course is to emphasize that effective statistical practice is more than just the application of mathematical principles, or the use of software or execution of analysis. Instead, critical thinking is required throughout statistical practice as reflected in the learning objectives of the course. During the final third period of the term, students are regularly tasked with synthesizing  what  they  have  learned.  They  will  apply  the  knowledge  they  have accumulated regarding estimation, parameter and method selection, and the uses of this knowledge  in  testing  hypotheses  and  drawing  inference  while  also  keeping  in  mind stakeholders, the ethical practice standards of the ASA, and how these should be considered by the stewardly practitioner. The syllabus is designed so that the first two thirds of the term focuses on training students for the practice of regular, critical mathematical reasoning both independently and in collaboration with their peers. The new techniques and methods students are learning in the last third period of the term are supported by specific learning objectives which are consistently revisited during regular assessments and group activities. Moreover, homework and in-class problems involving the application of these methods can now require justification for decision making in the form of a stakeholder analysis for (any) decisions made along the way from study design, to data collection, analysis, and interpretation.  Assessments  can  also  prompt  students  to  identify  any  ASA  Ethical Guideline elements that are relevant along this same path (from design to interpretation).As the term concludes, students are asked again to consider the importance of the stewardly practice of statistics and given the opportunity to compare their answers to their responses from earlier in the term when they were first introduced to the concept of stewardship. Finally, students can be asked to describe how they anticipate stakeholder analysis, the ASA Ethical Guidelines, and their sense of themselves as stewards to continue to grow in future courses. This simple activity can promote a commitment to ongoing engagement with ethical practice throughout their educational training and can be readily incorporated into end of semester course evaluations, for example.In  previous  academic  years  the  introduction  of  ethical  content  was  limited  to  simply familiarizing the students with the existence of the ASA Ethical Guidelines and a single case study of an inappropriate application of statistical analysis. In the current syllabus however, the entire third unit features active integration of thinking about stakeholders, stewardship,  and  how  the  ASA  Ethical  Guidelines  can  facilitate  both.  More  than familiarization  with  disciplinary  best  practices  is  required  in  this  version  of  a  statistics course syllabus. From the first day of class when the syllabus is introduced, students are made aware that an explicit objective for their training in the course is to develop new habits of mind that are formed by consideration of stakeholders in statistical practice and a sense of professional identity based on the notion of stewardship. They are introduced to new concepts in stages that  align  with  the  presentation  of  the  typical  statistical  course material. Ethical reasoning for statistical practice is scaffolded into the course material so that it is not until after students have observed and practiced the stakeholder analysis and professional guidelines for stewardship of the discipline that they are tasked with reasoning through  any  particular  example  of  statistical  analysis  in  a  real  world  problem.  This structured  approach,  rather  than  a  one-off  detour  through  a  single  case  analysis  for example, improves the likelihood that the last two learning objectives are actually met.## 4. Discussion and Conclusions## 4.1 DiscussionWe have outlined the rationale behind three different  approaches  to  integrating  ethical content into a primarily-quantitative course. The example course, Mathematical Statistics, is described in terms of an adaptable syllabus. The syllabus includes explicit signaling from the instructor as to the importance and role of stewardship, stakeholders, and the ASA Ethical Guidelines for Statistical Practice. These three dimensions, particularly as they are integrated into the course as discussed, represent the first element of ethical reasoning: prerequisite knowledge. Just as with the mathematics and statistics content, the prerequisite knowledge  required  for  ethical  reasoning  and  ethical  statistical  practice  goes  beyond memorization. Decisions that the syllabus implicitly reflects include a focus on solely the prerequisite  knowledge  dimension  of  the  ethical  reasoning  paradigm  (Tractenberg  &amp; FitzGerald,  2012;  Tractenberg,  2022-A;  2022-B);  and  a  focus  on  the  construct  of stewardship, without moving through its set of knowledge, skills, and abilities (R"
  },
  {
    "hash_code": "5c4f9fe87909e18002435c1ccde2013d",
    "text": "with the mathematics and statistics content, the prerequisite knowledge  required  for  ethical  reasoning  and  ethical  statistical  practice  goes  beyond memorization. Decisions that the syllabus implicitly reflects include a focus on solely the prerequisite  knowledge  dimension  of  the  ethical  reasoning  paradigm  (Tractenberg  &amp; FitzGerald,  2012;  Tractenberg,  2022-A;  2022-B);  and  a  focus  on  the  construct  of stewardship, without moving through its set of knowledge, skills, and abilities (Rios et al. 2019). The ethical content included in this course, as described with the syllabus we have included, could be built upon in later courses with the same types of activities, but with more advanced problems or responses by students. Or, programs that use the integration described in this syllabus can choose ethical reasoning (Tractenberg &amp; FitzGerald, 2012; Tractenberg et al. 2017) or stewardship (Rios et al. 2019) as a paradigm to graft into the full curriculum.## 4.2 ConclusionsRecent encouragement to incorporate 'ethics' into the curricula for statistics (ASA, 2014) and for data science (National Academies, 2018) has been published for undergraduate degree programs. However, resources that exist for 'teaching ethics' are not accessible for many instructors of courses that feature primarily quantitative materials and content. When discussing the best practices for teaching mathematics, Kelton (2010) notes,'It is easy for students to have the misconception that the problems are easy when they witness you solving them with no difficulty. Giving the students a chance to see where they might stumble will help them formulate questions and motivate them  to  give  the  homework  proper  attention.  It  may  be  best  to  only  give  the students one or two problems at a time to solve on their own. That is generally sufficient  for  the  students  to  ascertain  whether  they  are  having  significant difficulty. After you have demonstrated the skill, give the class a basic problem or two to try on their own.'Not only is this sentiment as true for reasoning and understanding ethical obligations as it is  for  teaching  mathematics, it is also appropriate for instructors who want to integrate ethics content into more mathematically-oriented courses. In their Recommendation 2.5, The Committee on Envisioning the Data Science Discipline pointed out that 'Ethics is a topic that, given the nature of data science, students should learn and practice throughout their  education.  Academic  institutions  should  ensure  that  ethics  is  woven  into  the  data science curriculum from the beginning and throughout.' (National Academies, 2018). This paper has outlined three different ways to overcome the difficulties that might prevent instructors  in  primarily  quantitative  courses  in  these  curricula  from  integrating  ethical content, ethical reasoning, or the consideration of how fundamental attributes of formulae, theorems,  and  proofs  can  support  this  integration  'from  the  beginning'  of  quantitative programs or courses. A syllabus and organizational structure is presented that can be used, adapted, or utilized in - or to seed - a sequence of courses where students are asked to generate increasingly sophisticated responses to similar questions.## AcknowledgementsSuzanne  Thornton  acknowledges  support  from  Swarthmore  College  Faculty  Research Support Grants.## References- American Statistical Association (ASA). (2014). Curriculum Guidelines for Undergraduate Programs in Statistical Science. Downloaded from https://www.amstat.org/docs/default-source/amstat-documents/edu-guidelines201411-15.pdf on 10 January 2016.- American Statistical Association (ASA) ASA Ethical Guidelines for Statistical Practicerevised  (2022)  downloaded from https://www.amstat.org/ASA/Your-Career/EthicalGuidelines-for-Statistical-Practice.aspx on 20 January 2022.- Association for Computing Machinery (ACM). Code of Ethics (2018) downloaded from https://www.acm.org/about-acm/code-of-ethics on 12 October 2018.- Briggle A &amp; Mitcham C. (2012). Ethics and science: An introduction. Cambridge, UK: Cambridge University Press.- DeVeaux RD, Agarwal M, Averett M, Baumer BS, Bray A, Bressoud TC, Bryant L, Cheng LZ, Francis A, Gould R, Kim AY, Kretchmar M, Lu Q, Moskol A, Nolan D, Pelayo R, Raleigh S, Sethi RJ, Sondjaja M, Tiruviluamala N, Uhlig PX,"
  },
  {
    "hash_code": "b329c39612c1cdfa9409f700c129f054",
    "text": "ics on 12 October 2018.- Briggle A &amp; Mitcham C. (2012). Ethics and science: An introduction. Cambridge, UK: Cambridge University Press.- DeVeaux RD, Agarwal M, Averett M, Baumer BS, Bray A, Bressoud TC, Bryant L, Cheng LZ, Francis A, Gould R, Kim AY, Kretchmar M, Lu Q, Moskol A, Nolan D, Pelayo R, Raleigh S, Sethi RJ, Sondjaja M, Tiruviluamala N, Uhlig PX, Washington TM,- Wesley  CL,  White  D,  Ye  P.  (2017).  Curriculum  Guidelines  for  Undergraduate Programs in Data Science. Annual Review of Statistics and Its Application 4:1, 15-30- Dow MJ, Boettcher CA, Diego JF, Karch ME, Todd-Diaz A, Woods KM. (2015). Casebased learning as pedagogy for teaching information ethics based on the Dervin sensemaking methodology. Journal of Education for Library and Information Science 56(2Spring): 141-157.- GAISE College Report ASA Revision Committee (2016). 'Guidelines for Assessment and Instruction in Statistics Education College Report 2016' http://www.amstat.org/education/gaise.- Golde CM, &amp; Walker GE. (Eds.). (2006). Envisioning the future of doctoral education: Preparing stewards of the discipline-Carnegie essays on the doctorate. San Francisco: Jossey-Bass.- Hogan H, Tractenberg RE, Elliot AC. (2017, July). Ethics and Big Data: Perspective of the American Statistical Association Committee on Professional Ethics. Presented at the 61st International Statistics Institute World Statistics Congress, Marrakesh, Morocco.- Kelton  S. (2010). An  introduction  to teaching  mathematics  at  the  college level. Downloaded from https://www.ams.org/profession/career-info/grad-school/KeltonTEACH.pdf on 10 July 2022- Loukides M, Mason H, Patil DJ. (10 July 2018). Doing good data science. Downloaded from https://www.oreilly.com/ideas/doing-good-data-science on 15 July 2018- National Academies of Sciences, Engineering, and Medicine. (2018). Data Science for Undergraduates: Opportunities and Options. Washington, DC: The National Academies Press. https://doi.org/10.17226/25104.- Rios  CR,  Golde  CM,  Tractenberg  RE.  (2019).  The  preparation  of  stewards  with  the Mastery  Rubric  for  Stewardship:  Re-envisioning  the  formation  of  scholars  and practitioners.  Education Sciences 9(4), 292; https://doi.org/10.3390/educsci9040292- Stark PB &amp; Saltelli A. (2018). Cargo cult statistics and the scientific crisis. Downloaded from https://www.significancemagazine.com/2-uncategorised/593-cargo-cultstatistics-and-scientific-crisis on 18 July 2018.- Tractenberg, RE. (2019). Preprint. Teaching and Learning about Ethical Practice: The Case Analysis.  Published  in  the  Open  Archive  of  the  Social  Sciences  (SocArXiv), https://doi.org/10.31235/osf.io/58umw- Tractenberg  RE.  (2020,  February  19).  Preprint.  Concordance  of  professional  ethical practice standards for the domain of Data Science: A white paper. Published in the Open Archive of the Social Sciences (SocArXiv), 10.31235/osf.io/p7rj2- Tractenberg,  RE.  (2022-A).  Ethical  Reasoning  for  a  Data  Centered  World.  Ethics International Press.- Tractenberg,  RE.  (2022-B).  Ethical  Practice  of  Statistics  and  Data  Science.  Ethics International Press.- Tractenberg RE. (2022-C). Stewardship of Mathematics: essential training for contributors to, and users of, the practice of mathematics. Journal of Humanistic Mathematics, 12(2) (July 2022), pages 221-253. Available at: https://scholarship.claremont.edu/jhm/vol12/iss2/13- Tractenberg RE &amp; FitzGerald KT. (2012). A Mastery Rubric"
  },
  {
    "hash_code": "e1e11ee93d79e0c945dd3768851e9d41",
    "text": "RE.  (2022-B).  Ethical  Practice  of  Statistics  and  Data  Science.  Ethics International Press.- Tractenberg RE. (2022-C). Stewardship of Mathematics: essential training for contributors to, and users of, the practice of mathematics. Journal of Humanistic Mathematics, 12(2) (July 2022), pages 221-253. Available at: https://scholarship.claremont.edu/jhm/vol12/iss2/13- Tractenberg RE &amp; FitzGerald KT. (2012). A Mastery Rubric for the design and evaluation of an institutional curriculum in the responsible conduct of research. Assessment and Evaluation in Higher Education. 37(7-8): 1003-21. DOI 10.1080/02602938.2011.596923- Tractenberg RE, FitzGerald KT, &amp; Collmann J. (2017).  Evidence of sustainable learning with the Mastery Rubric for Ethical Reasoning. Education Sciences. Special Issue: Consequential Assessment of Student Learning: 7(1), 2; doi:10.3390/educsci7010002 http://www.mdpi.com/2227-7102/7/1/2Tractenberg RE, Lindvall JM, Attwood TK, Via A. (2020, April 2) Preprint. Guidelines for curriculum and course development in higher education and training. Published in the Open Archive of the Social Sciences (SocArXiv), doi 10.31235/osf.io/7qeht.## APPENDIX: Full Syllabus and ASA Ethical Guidelines for Statistical Practice## Syllabus for a First Course in Mathematical Statistics## Learning objectives1. Understand frequentist and Bayesian methods for parameter estimation and common approaches to evaluate and compare estimators.2. Work  with  asymptotic  theorems  to  characterize  the  behavior  of  common  types  of estimators.3. Understand how to analytically derive a Bayesian posterior distribution and how to interpret a Bayesian credible interval and demonstrate familiarity with different types of priors.4. Understand the important role of likelihood functions for hypothesis testing in both Bayesian and frequentist frameworks5. Understand the  relationship  between  frequentist  confidence  interval  estimation  and hypothesis testing.6. Familiarity with common types of optimal testing strategies.7. Construct and interpret frequentist p-values for hypothesis tests and interpret the error rates.8. Identify/define model parameters and state statistical inferential questions in terms of these parameters for various common, realistic study settings.9. Contextualize statistical methods and theory in science and policy at large and develop a habit of mind informed by how the application of such methods can affect stakeholders.10. Understand what it means to be a steward of statistics, how to be stewardly, and why that is important 8 .## Topics outline for the semesterUnit One - Thoughtful use of 9estimation techniques Relevant Learning Objectives: 1, 2, 3, 98 Two learning outcomes reflect the instructor's intention to prioritize not only the contextualization of statistical methods and theory in real world applications, but also the facts that a) statistical methods and theories have potential impacts on stakeholders; b) these impacts and stakeholders are important enough to feature at the learning outcome level (not as an aside); and c) there is more to learning the mathematical underpinnings of statistics than 'just math' stewardship, and responsibilities to the discipline, are also learnable and important to internalize.9 The addition of ' Thoughtful use of ' to the Unit title signals to students that the focus moves beyond 'use' and learners will be expected to be thoughtful (not simply memorizing) in their selection and application of estimation techniques.## 1.1 Review of probabilityThroughout the review of probability,  the instructor will introduce a reasoning tool called a \"stakeholder analysis\". Students will practice naming stakeholders in various contexts and begin to understand that although assumptions and approximations always work in the theoretical  realm,  they  have  real  implications  for  practice  and  may  not  work  for  every stakeholder.## 1.2 Methods to derive estimatorsThroughout the introduction of estimation methods, the instructor will continue to feature the stakeholder analysis to explore how selection and application of estimation techniques can potentially have effects beyond simple numeric solutions.## 1.3 Intro to large sample theory of estimationAfter introducing the students to fundamental large sample results concerning maximum likelihood  estimation,  the  instructor  will  lead  an  in-class  discussion  of  a  stakeholder analysis regarding the application of an estimator with desirable large sample properties.## 1.4 Interval estimationStudents are introduced to Bayesian and frequentist interval estimation techniques. The instructor will lead"
  },
  {
    "hash_code": "08e46c5b84c19027079fb532f02ef242",
    "text": "Methods to derive estimatorsThroughout the introduction of estimation methods, the instructor will continue to feature the stakeholder analysis to explore how selection and application of estimation techniques can potentially have effects beyond simple numeric solutions.## 1.3 Intro to large sample theory of estimationAfter introducing the students to fundamental large sample results concerning maximum likelihood  estimation,  the  instructor  will  lead  an  in-class  discussion  of  a  stakeholder analysis regarding the application of an estimator with desirable large sample properties.## 1.4 Interval estimationStudents are introduced to Bayesian and frequentist interval estimation techniques. The instructor will lead a class discussion of a stakeholder analysis focused on the implications of various choices that are made in interval estimation.## Unit Two - Statistical inference and stewardship 10Relevant Learning Objectives: 4, 5, 6, 7, 10## 2.1 What's in a likelihoodAs  students'  work  through  understanding  the  concept  of  a  likelihood  function  and dimension reduction through sufficient statistics, the instructor will present the notion of a steward of statistics.## 2.2 Neyman-Pearson paradigm for optimal testingAs students' wrestle with the logic behind optimal testing, they are also becoming aware of the  restrictiveness  of  ideal  mathematical  reasoning  in  the  applied  world.  They  will  be encouraged to reference an explicit definition of stewardship as they consider how to use statistical tests in less-than-ideal conditions.## 2.3 Trustworthy testingAs students start to understand what it means to be a steward of statistical practice, the instructor  will  reference  hypothesis  tests  as  a  concrete  example  of  why  stewardship  is important.  The  instructor  will  lead  students  to  examine  the  role  of  various  potential stakeholders in stewardly statistical inference.## Unit Three - Disciplinary best practices for 11 common study designs and analysesRelevant Learning Objectives: 8, 9, 1010 The addition of ' and stewardship ' to the Unit title signals to students that the focus on inference includes stewardly consideration.11 The addition of ' Disciplinary best practices for ' common designs and analyses informs students that respect for the discipline (i.e., stewardship) is equally important for them to learn as the designs and analytic methods. The Unit title transmits the instructor's intention that students develop a stewardly, discipline- rather than results-centered, mindset .## 3.1 One and two sample meansThe instructor will introduce the American Statistical Association's Guidelines for Ethical Statistical  Practice.  Students  will  work  in  small  discussion  groups  to  consider  why particular statistical methods are 'common' from a perspective that names and considers potential stakeholders.## 3.2 ANOVAThe instructor  will  continue  to  reference  the  ASA's  Ethical  Guidelines  with  respect  to ANOVAmethods.  Groups will continue to attempt to characterize \"common\" statistical methods and will be encouraged to also consider the role of software and the level of engagement with ASA Ethical Guidelines that software encourages.## 3.3 Categorical dataStudents will be prompted to discuss the differences for stakeholders between methods that use continuous or categorical data. The instructor will emphasize what the ASA's Ethical Guidelines suggest a stewardly practitioner should prioritize when given a choice between categorical or continuous variable-based analysis.## 3.4 Linear regressionAs the semester concludes, students revisit the concept of linear regression from a more mathematically and stewardly informed perspective. In groups and as a class, students and the  instructor  will  discuss  variable  choice  and  model  selection  referencing  the  ASA's Ethical Guidelines and centering the notion of statistical stewardship.Ethical Guidelines for Statistical Practice Prepared by the Committee on Professional Ethics of the American Statistical Association 31 January 2022## PURPOSE OF THE GUIDELINES:The  American  Statistical  Association's  Ethical  Guidelines  for  Statistical  Practice  are intended  to  help  statistical  practitioners  make  decisions  ethically.  In  these  Guidelines, 'statistical practice' includes activities such as: designing the collection of, summarizing, processing,  analyzing,  interpreting,  or  presenting,  data;  as  well  as  model  or  algorithm development and deployment. Throughout these Guidelines, the term \"statistical practitioner\" includes all those who engage in statistical practice, regardless of job title, profession, level, or field of degree. The Guidelines are intended for individuals, but these principles are also relevant to organizations that engage in statistical practice.The Ethical Guidelines aim to promote accountability by informing those who rely on any aspects of statistical practice of the standards that they should expect. Society benefits from"
  },
  {
    "hash_code": "0ab90f10d92652e9ba91aaaacabf20b1",
    "text": "such as: designing the collection of, summarizing, processing,  analyzing,  interpreting,  or  presenting,  data;  as  well  as  model  or  algorithm development and deployment. Throughout these Guidelines, the term \"statistical practitioner\" includes all those who engage in statistical practice, regardless of job title, profession, level, or field of degree. The Guidelines are intended for individuals, but these principles are also relevant to organizations that engage in statistical practice.The Ethical Guidelines aim to promote accountability by informing those who rely on any aspects of statistical practice of the standards that they should expect. Society benefits from informed judgments supported by ethical statistical practice. All statistical practitioners are expected to follow these Guidelines and to encourage others to do the same.In some situations, Guideline principles may require balancing of competing interests. If an  unexpected  ethical  challenge  arises,  the  ethical  practitioner  seeks  guidance,  not exceptions,  in  the  Guidelines.  To  justify  unethical  behaviors,  or  to  exploit  gaps  in  the Guidelines, is unprofessional, and inconsistent with these Guidelines.## PRINCIPLE A: Professional Integrity and AccountabilityProfessional  integrity  and  accountability  require  taking  responsibility  for  one's  work. Ethical statistical practice supports valid and prudent decision making with appropriate methodology. The ethical statistical practitioner represents their capabilities and activities honestly, and treats others with respect.## The ethical statistical practitioner:1. Takes responsibility for evaluating potential tasks, assessing whether they have (or can attain) sufficient competence to execute each task, and that the work and timeline are feasible. Does not solicit or deliver work for which they are not qualified, or that they would not be willing to have peer reviewed.2. Uses methodology and data that are valid, relevant, and appropriate, without favoritism or prejudice, and in a manner intended to produce valid, interpretable, and reproducible results.3. Does not knowingly conduct statistical practices that exploit vulnerable populations or create or perpetuate unfair outcomes.4. Opposes efforts to predetermine or influence the results of statistical practices, and resists pressure to selectively interpret data.5. Accepts full responsibility for their own work; does not take credit for the work of others;  and  gives  credit  to  those  who  contribute.  Respects  and  acknowledges  the intellectual property of others.6. Strives to follow, and encourages all collaborators to follow, an established protocol for authorship. Advocates for recognition commensurate with each person's contribution to the work. Recognizes that inclusion as an author does imply, while acknowledgement may imply, endorsement of the work.7. Discloses conflicts of interest, financial and otherwise, and manages or resolves them according to established policies, regulations, and laws.8. Promotes the dignity and fair treatment of all people. Neither engages in nor condones discrimination  based  on  personal  characteristics.  Respects  personal  boundaries  in interactions and avoids harassment including sexual harassment, bullying, and other abuses of power or authority.9. Takes appropriate action when aware of deviations from these Guidelines by others.10. Acquires and maintains competence through upgrading of skills as needed to maintain a high standard of practice.11. Follows applicable policies, regulations, and laws relating to their professional work, unless there is a compelling ethical justification to do otherwise.12. Upholds, respects, and promotes these Guidelines. Those who teach, train, or mentor in statistical practice have a special obligation to promote behavior that is consistent with these Guidelines.PRINCIPLE B: Integrity of Data and MethodsThe ethical statistical practitioner seeks to understand and mitigate known or suspected limitations, defects, or biases in the data or methods and communicates potential impacts on  the  interpretation,  conclusions,  recommendations,  decisions,  or  other  results  of statistical practices.## The ethical statistical practitioner:1. Communicates  data  sources  and  fitness  for  use,  including  data  generation  and collection processes and known biases. Discloses and manages any conflicts of interest relating  to  the  data  sources.  Communicates  data  processing  and  transformation procedures, including missing data handling.2. Is transparent about assumptions made in the execution and interpretation of statistical practices including methods  used, limitations, possible sources of error, and algorithmic biases. Conveys results or applications of statistical practices in ways that are honest and meaningful.3. Communicates  the  stated  purpose  and  the  intended  use  of  statistical  practices.  Is transparent regarding a priori versus post hoc objectives and planned versus unplanned statistical  practices.  Discloses  when  multiple  comparisons  are  conducted,  and  any relevant adjustments.4. Meets obligations to share the data used in the statistical practices, for example, for peer review and replication, as allowable. Respects expectations of data contributors when  using"
  },
  {
    "hash_code": "5796ca3792761854f03f8b4fe2387d36",
    "text": "possible sources of error, and algorithmic biases. Conveys results or applications of statistical practices in ways that are honest and meaningful.3. Communicates  the  stated  purpose  and  the  intended  use  of  statistical  practices.  Is transparent regarding a priori versus post hoc objectives and planned versus unplanned statistical  practices.  Discloses  when  multiple  comparisons  are  conducted,  and  any relevant adjustments.4. Meets obligations to share the data used in the statistical practices, for example, for peer review and replication, as allowable. Respects expectations of data contributors when  using  or  sharing  data.  Exercises  due  caution  to  protect  proprietary  and confidential data, including all data that might inappropriately harm data subjects.5. Strives to promptly  correct substantive errors discovered after publication or implementation. As appropriate, disseminates the correction publicly and/or to others relying on the results.6. For  models  and  algorithms  designed  to  inform  or  implement  decisions  repeatedly, develops  and/or  implements  plans  to  validate  assumptions  and  assess  performance over time, as needed. Considers criteria and mitigation plans for model or algorithm failure and retirement.7. Explores and describes the effect of variation in human characteristics and groups on statistical practice when feasible and relevant.## PRINCIPLE C:  Responsibilities to StakeholdersThose who fund, contribute to, use, or are affected by statistical practices are considered stakeholders. The ethical statistical practitioner respects the interests of stakeholders while practicing in compliance with these Guidelines.## The ethical statistical practitioner:1. Seeks to establish what stakeholders hope to obtain from any specific project. Strives to  obtain  sufficient  subject-matter  knowledge  to  conduct  meaningful  and  relevant statistical practice.2. Regardless  of  personal  or  institutional  interests  or  external  pressures,  does  not  use statistical practices to mislead any stakeholder.3. Uses  practices  appropriate  to  exploratory  and  confirmatory  phases  of  a  project, differentiating findings from each so the stakeholders can understand and apply the results.4. Informs  stakeholders  of  the  potential  limitations  on  use  and  re-use  of  statistical practices in different contexts and offers guidance and alternatives, where appropriate, about scope, cost, and precision considerations that affect the utility of the statistical practice.5. Explains  any  expected  adverse  consequences  from  failing  to  follow  through  on  an agreed-upon sampling or analytic plan.6. Strives to make new methodological knowledge widely available to provide benefits to  society  at  large.  Presents  relevant  findings,  when  possible,  to  advance  public knowledge.7. Understands and conforms to confidentiality requirements for data collection, release, and dissemination and any restrictions on its use established by the data provider (to the  extent  legally  required).  Protects  the  use  and  disclosure  of  data  accordingly. Safeguards privileged information of the employer, client, or funder.8. Prioritizes both scientific integrity and the principles outlined in these Guidelines when interests are in conflict.## PRINCIPLE D:  Responsibilities to Research Subjects, Data Subjects, or those directly affected by statistical practicesThe ethical statistical practitioner does not misuse or condone the misuse of data. They protect  and  respect  the  rights  and  interests  of  human  and  animal  subjects.  These responsibilities extend to those who will be directly affected by statistical practices.## The ethical statistical practitioner:1. Keeps informed about and adheres to applicable rules, approvals, and guidelines for the protection and welfare of human and animal subjects. Knows when work requires ethical review and oversight. 122. Makes informed recommendations for sample size and statistical practice methodology in order to avoid the use of excessive or inadequate numbers of subjects and excessive risk to subjects3. For  animal  studies,  seeks  to  leverage  statistical  practice  to  reduce  the  number  of animals used, refine experiments to increase the humane treatment of animals, and replace animal use where possible.4. Protects  people's  privacy  and  the  confidentiality  of  data  concerning  them,  whether obtained from the individuals directly, other persons, or existing records. Knows and adheres to applicable rules, consents, and guidelines to protect private information.5. Uses data only as permitted by data subjects' consent when applicable or considering their interests and welfare when consent is not required. This includes primary and secondary uses, use of repurposed data, sharing data, and linking data with additional data sets.6. Considers  the  impact  of  statistical  practice  on  society,  groups,  and  individuals. Recogn"
  },
  {
    "hash_code": "cdd7124705b652f852e6543e44e92d95",
    "text": "and  the  confidentiality  of  data  concerning  them,  whether obtained from the individuals directly, other persons, or existing records. Knows and adheres to applicable rules, consents, and guidelines to protect private information.5. Uses data only as permitted by data subjects' consent when applicable or considering their interests and welfare when consent is not required. This includes primary and secondary uses, use of repurposed data, sharing data, and linking data with additional data sets.6. Considers  the  impact  of  statistical  practice  on  society,  groups,  and  individuals. Recognizes  that  statistical  practice  could  adversely  affect  groups  or  the  public perception  of  groups,  including  marginalized  groups.  Considers  approaches  to minimize negative impacts in applications or in framing results in reporting.7. Refrains  from  collecting  or  using  more  data  than  is  necessary.  Uses  confidential information only when permitted and only to the extent necessary. Seeks to minimize12 Examples of ethical review and oversight include an Institutional Review Board (IRB), an Institutional Animal Care and Use Committee (IACUC), or a compliance assessment.- the risk of re-identification when sharing de-identified data or results where there is an expectation of confidentiality. Explains any impact of de-identification on accuracy of results.8. To maximize contributions of data subjects, considers how best to use available data sources for exploration, training, testing, validation, or replication as needed for the application. The ethical statistical practitioner appropriately discloses how the data are used for these purposes and any limitations.9. Knows the legal limitations on privacy and confidentiality assurances and does not over-promise or assume legal privacy and confidentiality protections where they may not apply.10. Understands  the  provenance  of  the  data,  including  origins,  revisions,  and  any restrictions on usage, and fitness for use prior to conducting statistical practices.11. Does not conduct statistical practice that could reasonably be interpreted by subjects as sanctioning a violation of their rights. Seeks to use statistical practices to promote the just and impartial treatment of all individuals.## PRINCIPLE E: Responsibilities to members of multidisciplinary teamsStatistical  practice  is  often  conducted  in  teams  made  up  of  professionals  with  different professional standards. The statistical practitioner must know how to work ethically in this environment.## The ethical statistical practitioner:1. Recognizes and respects that other professions may have different ethical standards and obligations. Dissonance in ethics may still arise even if all members feel that they are working towards the same goal. It is essential to have a respectful exchange of views.2. Prioritizes these Guidelines for the conduct of statistical practice in cases where ethical guidelines conflict.3. Ensures that  all communications  regarding  statistical  practices  are  consistent  with these     Guidelines. Promotes transparency in all statistical practices.4. Avoids compromising validity for expediency. Regardless of pressure on or within the team, does not use inappropriate statistical practices.## PRINCIPLE F: Responsibilities to Fellow Statistical Practitioners and the ProfessionStatistical practices occur in a wide range of contexts. Irrespective of job title and training, those who practice statistics have a responsibility to treat statistical practitioners, and the profession, with respect. Responsibilities to other practitioners and the profession include honest communication and engagement that can strengthen the work of others and the profession.## The ethical statistical practitioner:1. Recognizes that statistical practitioners may have different expertise and experiences, which  may  lead  to  divergent  judgments  about  statistical  practices  and  results.2. Constructive  discourse  with  mutual  respect  focuses  on  scientific  principles  and methodology and not personal attributes.2. Helps strengthen, and does not undermine, the work of others through appropriate peer review or consultation. Provides feedback or advice that is impartial, constructive, and objective.3. Takes full responsibility for their contributions as instructors, mentors, and supervisors of statistical practice by ensuring their best teaching and advising -- regardless of an academic or non-academic setting -- to ensure that developing practitioners are guided effectively as they learn and grow in their careers.4. Promotes reproducibility and replication, whether results are 'significant' or not, by sharing data, methods, and documentation to the extent possible.5. Serves as an ambassador for statistical practice by promoting thoughtful choices about data acquisition, analytic procedures, and data structures among non-practitioners and students. Instills appreciation for the concepts and methods of statistical practice.## PRINCIPLE G: Responsibilities of Leaders, Supervisors, and Mentors in Statistical PracticeStatistical practitioners leading, supervising, and/or mentoring people in statistical practice have specific obligations to follow and promote these Ethical Guidelines. Their support for - and insistence on - ethical statistical"
  },
  {
    "hash_code": "d1581ad00040c2a6d86e614d799c0e8a",
    "text": "ibility and replication, whether results are 'significant' or not, by sharing data, methods, and documentation to the extent possible.5. Serves as an ambassador for statistical practice by promoting thoughtful choices about data acquisition, analytic procedures, and data structures among non-practitioners and students. Instills appreciation for the concepts and methods of statistical practice.## PRINCIPLE G: Responsibilities of Leaders, Supervisors, and Mentors in Statistical PracticeStatistical practitioners leading, supervising, and/or mentoring people in statistical practice have specific obligations to follow and promote these Ethical Guidelines. Their support for - and insistence on - ethical statistical practice are essential for the integrity of the practice and profession of statistics as well as the practitioners themselves.## Those leading, supervising, or mentoring statistical practitioners are expected to :1. Ensure appropriate statistical practice that is consistent with these Guidelines. Protect the  statistical  practitioners  who  comply  with  these  Guidelines,  and  advocate  for  a culture that supports ethical statistical practice.2. Promote a respectful, safe, and productive work environment. Encourage constructive engagement to improve statistical practice.3. Identify and/or create opportunities for team members/mentees to develop professionally and maintain their proficiency.4. Advocate for appropriate, timely, inclusion and participation of statistical practitioners as contributors/collaborators. Promote appropriate recognition of the contributions of statistical practitioners, including authorship if applicable.5. Establish a culture that values validation of assumptions,  and  assessment  of model/algorithm  performance  over  time  and  across  relevant  subgroups,  as  needed. Communicate with relevant stakeholders regarding model or algorithm maintenance, failure, or actual or proposed modifications.## PRINCIPLE H: Responsibilities Regarding Potential MisconductThe  ethical  statistical  practitioner  understands  that  questions  may  arise  concerning potential misconduct related to statistical, scientific, or professional practice. At times, a practitioner may accuse someone of misconduct, or be accused by others. At other times, a  practitioner  may  be  involved  in  the  investigation  of  others'  behavior.  Allegations  of misconduct may arise within different institutions with different standards and potentially different outcomes. The elements that follow relate specifically to allegations of statistical, scientific, and professional misconduct.## The ethical statistical practitioner:1. Knows the definitions of, and procedures relating to, misconduct in their institutional setting.  Seeks  to  clarify  facts  and  intent  before  alleging  misconduct  by  others. Recognizes that differences of opinion and honest error do not constitute unethical behavior.2. Avoids  condoning  or  appearing  to  condone  statistical,  scientific,  or  professional misconduct. Encourages other practitioners to avoid misconduct or the appearance of misconduct.3. Does  not  make  allegations  that  are  poorly  founded,  or  intended  to  intimidate. Recognizes such allegations as potential ethics violations.4. Lodges complaints  of  misconduct  discreetly  and  to  the  relevant  institutional  body. Does not act on allegations of misconduct without appropriate institutional referral, including those allegations originating from social media accounts or email listservs.5. Insists  upon  a  transparent  and  fair  process  to  adjudicate  claims  of  misconduct. Maintains  confidentiality  when  participating  in  an  investigation.  Discloses  the investigation  results  honestly  to  appropriate  parties  and  stakeholders  once  they  are available.6. Refuses to publicly question or discredit the reputation of a person based on a specific accusation of misconduct while due process continues to unfold.7. Following an investigation of misconduct, supports the efforts of all parties involved to resume their careers in as normal a manner as possible, consistent with the outcome of the investigation.8. Avoids, and acts to discourage, retaliation against or damage to the employability of those who responsibly call attention to possible misconduct.## GUIDELINES APPENDIX## Responsibilities of organizations/institutionsWhenever  organizations  and  institutions  design  the  collection  of,  summarize,  process, analyze, interpret, or present, data; or develop and/or deploy models or algorithms, they have  responsibilities  to  use  statistical  practice  in  ways  that  are  consistent  with  these Guidelines, as well as promote ethical statistical practice.## Organizations and institutions engage in, and promote, ethical statistical practice by :1. Expecting and encouraging all employees and vendors who conduct statistical practice to adhere to these Guidelines. Promoting a workplace where the ethical practitioner may apply the Guidelines without being intimidated or coerced. Protecting statistical practitioners who comply with these Guidelines.2. Engaging  competent  personnel  to conduct statistical practice, and promote  a productive work environment.3. Promoting the professional development and maintenance of proficiency for employed statistical practitioners.4. Supporting"
  },
  {
    "hash_code": "1d8fe1f4bf6b90be9d1f3402e425044c",
    "text": "in  ways  that  are  consistent  with  these Guidelines, as well as promote ethical statistical practice.## Organizations and institutions engage in, and promote, ethical statistical practice by :1. Expecting and encouraging all employees and vendors who conduct statistical practice to adhere to these Guidelines. Promoting a workplace where the ethical practitioner may apply the Guidelines without being intimidated or coerced. Protecting statistical practitioners who comply with these Guidelines.2. Engaging  competent  personnel  to conduct statistical practice, and promote  a productive work environment.3. Promoting the professional development and maintenance of proficiency for employed statistical practitioners.4. Supporting  statistical practice that is objective and  transparent.  Not  allowing organizational objectives or expectations to encourage unethical statistical practice by its employees.5. Recognizing that the inclusion of statistical practitioners as authors, or acknowledgement  of  their  contributions  to  projects  or  publications,  requires  their explicit permission because it may imply endorsement of the work.6. Avoiding statistical practices that exploit vulnerable populations or create or perpetuate discrimination or unjust outcomes. Considering both scientific validity and impact on societal and human well-being that results from the organization's statistical practice.7. Using professional qualifications and contributions as the basis for decisions regarding statistical practitioners' hiring, firing, promotion, work assignments, publications and presentations, candidacy for offices and awards, funding or approval of research, and other professional matters.Those  in  leadership,  supervisory,  or  managerial  positions  who  oversee  statistical practitioners promote ethical statistical practice by following Principle G and:8. Recognizing that it is contrary to these Guidelines to report or follow only those results that conform to expectations without explicitly acknowledging competing findings and the basis for choices regarding which results to report, use, and/or cite.9. Recognizing that the results of valid statistical studies cannot be guaranteed to conform to the expectations or desires of those commissioning the study or employing/supervising the statistical practitioner(s).10. Objectively,  accurately,  and  efficiently  communicating  a  team's  or  practitioners' statistical work throughout the organization.11. In cases where  ethical issues are raised, representing them  fairly within the organization's leadership team.12. Managing  resources and organizational strategy to direct teams of statistical practitioners along the most productive lines in light of the ethical standards contained in these Guidelines."
  },
  {
    "hash_code": "3c9dcc18f8f268a06ee5fd9eaf92181d",
    "text": "## PREDICTING FUTURE STATES WITH SPATIAL POINT PROCESSES IN SINGLE MOLECULE RESOLUTION SPATIAL TRANSCRIPTOMICSBiraaj Rout ⋆, 1 , 4Priyanshi Borad ⋆, 2Mohammad Sadegh NasrParisa Boodaghi Malidarreh ⋆, 1 , 41 , 4Jillur Rahman Saurav 1 , 4Jai Prakash Veerla 1 , 4Kelli Fenelon 2Jacob M. Luber † , 1 , 3 , 4Theodora Koromila † , 2 , 51 Department of Computer Science and Engineering, University of Texas at Arlington2 Department of Biology, University of Texas at Arlington3 Department of Bioengineering, University of Texas at Arlington4 Multi-Interprofessional Center for Health Informatics, University of Texas at Arlington5School of Biology, Aristotle University of Thessaloniki## ABSTRACTIn this paper, we introduce a pipeline based on XGBoost (eXtreme Gradient Boosting) to predict the future distribution of cells that are expressed by the sog gene (active cells) in both the Anterior to posterior (AP) and the Dorsal to Ventral (DV) axis of the Drosophila in embryogenesis process. This method provides insights about how cells and living organisms control gene expression in super resolution whole embryo spatial transcriptomics imaging at sub cellular, single molecule level. An XGBoost model was used to predict the next stage active distribution based on the previous one. To achieve this goal, we leveraged temporally resolved, spatial point processes by including Ripley's K-function in conjunction with the cell's state in each stage of embryogenesis, and found average predictive accuracy of active cell distribution. This tool is analogous to RNA Velocity for spatially resolved developmental biology, from one data point we can predict future spatially resolved gene expression using features from the spatial point processes.Index Terms -XGBoost, Regression, Drosophila sog , Ripley's K-function, transcriptomics, embryogenesis## 1. INTRODUCTIONRecent technological advances have made it possible to capture high resolution images from embryogenesis processes that help researchers to study gene expression patterns.[1, 2]. One of the major challenges of the modern genomics era is better understand how gene expression is regulated to support spatiotemporal outputs that change over the course of development. The early Drosophila embryo has served as a paradigm for how enhancers control patterning and has⋆ Equal contribution.† Responsible authors.demonstrated that the patterning process is complex and dynamic. It is known that multiple, transiently acting enhancers function sequentially to regulate dynamic changes in gene expression outputs [2, 3, 4], whereas other genes are controlled by enhancers that act over a longer period and support changing spatial outputs over time. For example, expression of the gene short gastrulation ( sog ) is driven by at least two co-acting enhancers that support temporally dynamic expression. Live imaging experiments enable the potential to analyze gene expression dynamics with increased temporal resolution and linear quantification. However, genetic and live imaging techniques have outpaced analysis approaches to harvest the bountiful information contained within realtime movies of transcriptional dynamics with modern methods confined to static parameter cell and transcript tracking methods [1, 5, 6]. To assess these mutant enhancer phenotypes systematically, we developed a quantitative approach to measure the spatiotemporal outputs of enhancer-driven MS2 reporter constructs as captured by in vivo imaging to provide information about the timing, levels, and spatial domains of expression. Using transgenic fly lines, we conducted live imaging to visualize GFP signal associated with MS2 stemloop reporter sequence binding MCP-GFP, enabling dynamic tracking of RNA localization and expression in real time. This MS2 cassette contains 24 repeats of a DNA sequence that produces an RNA stem loop when transcribed. The stemloop structure is specifically bound by the phage MS2 coat protein (MCP). MCP fused to GFP binds to MS2-containing transcripts (i.e., sog Distal .MS2) producing a strong green signal within the nuclei of Drosophila embryos at sites of nascent transcript production. In this system, nuclear GFP fluorescence is observed as a single dot per nucleus in heterozygous individuals, corresponding to nascent transcription from a single copy of the MS2-containing reporter transgene integrated into the genome. Furthermore, the nuclearEmail:periphery is marked by a fusion of RFP to nuclear pore protein (Nup-RFP) [7]. We optimized the imaging protocol to provide spatial information across the entire dorsal-ventral (DV) axis of embryos with the fastest temporal resolution that also retains embryo viability. In brief, embryos were imaged on a Zeiss LSM 900"
  },
  {
    "hash_code": "810d093ef46a77035a55f8de4c8fccf3",
    "text": "sites of nascent transcript production. In this system, nuclear GFP fluorescence is observed as a single dot per nucleus in heterozygous individuals, corresponding to nascent transcription from a single copy of the MS2-containing reporter transgene integrated into the genome. Furthermore, the nuclearEmail:periphery is marked by a fusion of RFP to nuclear pore protein (Nup-RFP) [7]. We optimized the imaging protocol to provide spatial information across the entire dorsal-ventral (DV) axis of embryos with the fastest temporal resolution that also retains embryo viability. In brief, embryos were imaged on a Zeiss LSM 900 continuously over the course of 2hr at an interval of 30s per scan (twice as fast compared to previous studies). Importantly, this imaging protocol is not photo-toxic to embryos. Because spatial outputs likely change in time across the embryo for many gene expression patterns, we developed an image processing approach to collect detailed information in both time and space by capturing one lateral half of the embryos. With this qualified imaging dataset, our goal was to predict the distribution of active cells in each stage of embryonic development as the blastula transitions into gastrulation. Building on existing methodologies for predicting temporal variables; authors in [8] introduced the concept of RNA velocity, which is defined as the time derivative of gene expression, offering a novel approach for inferring dynamic changes in gene activity over time. This concept allows for the estimation of future states of individual cells in standard scRNA-seq protocols. In [9], authors proposed a method to capture spatial proteomics data to map cell states in order to predict cancer patient survival. They utilized the Ripley's K-function for capturing spatial features which inspired us in our proposed pipeline. We developed a feature extraction method and analysis pipeline that can be used to predict the future distribution of cells in which the sogD gene is expressed.## 2. METHODS## 2.1. Experimental set-up for embryo collectionVirgin females expressing MCP-GFP (green) and Nup-RFP (red) maternally were crossed with males carrying either the sog Distal eve2 promoter-MS2.yellow-attB [Broadly expressed repressors integrate patterning across orthogonal axes in embryos] or sogD ∆ Su ( H ) eve2 promoter-MS2.yellowattB [1]. Embryos were precisely timed and collected during nuclear cycles 10-11. After collection, embryos were carefully dechorionated and mounted in Halocarbon 27 between a slide and coverslip, with spacing achieved using double-sided tape, as previously described [10].## 2.2. Live ImagingEmbryos were collected on apple agar plates for 1 hour, rested for 30 minutes at room temperature, and manually dechorionated. They were mounted between a slide and coverslip using heptane-dissolved adhesive and immersed in Halocarbon 27 oil. Imaging was performed on a Zeiss LSM 900 Airyscan 2 (Zeiss, Oberkochen, Germany) during stages leading into gastrulation, with broad-view and super-resolution movies captured using a 40× water oil immersion objective.Images were acquired at varying resolutions and intervals, as described [10].## 2.3. Data Pre-processingWe conduct pre-processing, feature extraction, training, and testing Fig.2. Both the training and testing phases incorporate identical pre-processing and feature extraction steps. The videos shows real time images from embryonic development, which were manually given stage development labels: NC 13, NC14 A, NC 14 B, NC 14 C, NC 14 D. In the pre-processing step, we used a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells in each frame of the embryo development. Active cells were identified based on prevalance of green pixels indicative of gene expression within the cell, and the active mask underwent feature extraction. During this stage, the masked images underwent a gridding procedure with a predetermined size. Subsequently, the entire imaging dataset was transformed into a tabular format, taking into account the spatial information of each cell. We utilized four different metrics to capture both local and global features in a frame including m1, m2 for both AP and DV axes, Ripley's k-function, and n (total number of cells in each grid). Here, m1 and m2 denote the first and second moments, respectively, capturing the distribution of active cells at each stage. Furthermore, Ripley's k-function was employed to analyze spatial correlation and quantify deviations from a random spatial distribution. Equation 1 illustrates the formula for calculating Ripley's k-function. Where, A is the area under each window with constant radius, n is the number of data points, d ij is the distance between two points, and e ij is an edge correction weight. Then, the tabular data went through two steps of averaging on each stage and time correcting. Since our goal is to predict the distribution of"
  },
  {
    "hash_code": "aa282c91d8df1fc996189bfa6de45456",
    "text": ", m1 and m2 denote the first and second moments, respectively, capturing the distribution of active cells at each stage. Furthermore, Ripley's k-function was employed to analyze spatial correlation and quantify deviations from a random spatial distribution. Equation 1 illustrates the formula for calculating Ripley's k-function. Where, A is the area under each window with constant radius, n is the number of data points, d ij is the distance between two points, and e ij is an edge correction weight. Then, the tabular data went through two steps of averaging on each stage and time correcting. Since our goal is to predict the distribution of active cells in each stage and we have different numbers of frames for each stage, we averaged the whole feature values based on each stage. Also, to account for temporal alignment, we implemented a one-stage shift in features, where we utilized the features from the previous stage in prediction of the current stage.## 2.4. TrainingFollowing the completion of the feature extraction process, the dataset undergoes preparation for training an XGBoost model, a supervised learning algorithm. The outcome of this pipeline is the count of active cells within each grid at a given stage, determined by the features from the preceding stage.## 2.5. EvaluationSubsequent to training the model, its performance is evaluated using test data. During testing, all pre-processing and feature extraction steps are replicated, and the pre-trained XGBoostFig. 1 : Computational analysis of super-resolution live imaging compares nuclei activity and predicts stages. (A) Super-resolution live imaging set-up of handdechorionated Drosophila embryos of MCP -GFPNup -RFP ( ∗ .MCP -GFP ) XsogD ∆ Su ( H ) -MS 2 . (B) Implemented pipeline, starting with using Cellpose 2.2.3 for segmentation, followed by subsequent stages involving active celll detection, tabulating data and feature selection, training ,and testing. These steps collectively aim to predict the distribution of active cells for the next stage. (C) The MCP -GFP -MS 2 system tracks transcription via GFP -tagged MCP binding to MS 2 loops (Stage A -NC13, double-dot '..' NC14 A , NC14 B , Stage n-1 NC14 C ) and nuclei activity of live imaging snapshots is compared with Cellpose generated images.bar chartThe image is a figure from a scientific paper, specifically Figure 2, which is titled \"Testing of optimal sampling parameters.\" The figure is divided into four panels, each with a different aspect of the study. Panel A shows a schematic of the imaging setup, including a cell, a droplet, and a camera. Panel B presents a heatmap of the data, with a scale of 32x32 and 16x16. Panel C shows raw videos of different stages of a process, labeled as \"Stage A,\" \"Stage n-1,\" and \"Stage n.\" Panel D displays a graph with error values for different parameters, including MAE along AP, MAE along DV, RMSE along AP, RMSE along DV, KL Divergence along AP, and KL Divergence along DV. The figure is credited to the authors of the paper.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000000_b4dfe57698e4267c734295b3276813973506e0248ee120cafe0850c7f4a94aa4.png)model is employed to forecast the count of active cells for each grid across various stages.̸$$\\hat { K } _ { r } = \\frac { A } { n ( n - 1 ) } \\sum _ { i = 1 } ^ { n } \\sum _ { i = 1 , j \\neq i } ^ { n } 1 ( d _ { i j } \\leq r ) e _ { i j } \\quad ( 1 ) \\quad \\begin{matrix} \\text {used} \\\\ \\text {the tr} \\\\ \\text {for} \\end{matrix}$$## 3. EXPERIMENT AND RESULTS## 3.1. Comprehensive Analysis of super-resolution live moviesAs outlined in the methodology section, during the feature extraction phase, square grids were applied to images, and the number of active cells within each grid was predicted.Fig. 2 : Testing of optimal sampling parameters. (A-C) Depictions of three distinct grid configurations, labeled A, B, and C, corresponding to grid sizes of 32×32, 26×26, and 8×8, respectively. (D) presents the error plot associated with each grid configuration (A-C), facilitating the identification of the optimal grid size based on the lowest error value"
  },
  {
    "hash_code": "ed43977ebf3b59af6bc628218138a68e",
    "text": "3.1. Comprehensive Analysis of super-resolution live moviesAs outlined in the methodology section, during the feature extraction phase, square grids were applied to images, and the number of active cells within each grid was predicted.Fig. 2 : Testing of optimal sampling parameters. (A-C) Depictions of three distinct grid configurations, labeled A, B, and C, corresponding to grid sizes of 32×32, 26×26, and 8×8, respectively. (D) presents the error plot associated with each grid configuration (A-C), facilitating the identification of the optimal grid size based on the lowest error value.The key challenge was selecting the optimal grid size to enhance performance on test data. Consequently, we replicated the entire process of pre-processing and feature extraction for four distinct grid sizes: 250, 125, 62.5, and 31.25 (where the grid size of 'n' indicates the division of the entire image into n*n squares). We used three different metrics to calculate the model performance on test data for different grid sizes which are rmse (root mean squared error), mae (mean absolute error), and Kullback-Leibler (KL) Divergence. Fig.3 shows the experiment for different grid sizes. Our analysis revealed the same increasing trend in both rmse and mae as the grid size increases from 31.25 to 250 which indicated that a smaller grid size corresponds to a lower error. KL Divergence, which we also utilized as a metric, measures how one probability distribution diverges from a second one. Thus, the smaller value for it shows that two distributions are closer to each other. We used this criterion to see how well the pipeline can capture the trends in the active cells distribution. The KL Divergence for these four different grid sizes showed the different trend. Increasing the grid size from 31.25 to 250 yielded a decrease in KL Divergence. We had two options, the first one was to select 31.25 based on the lower rmse and mae. However, the problem was the average size of the cell was approximately 36 so if we set the grid size to 31.25 we have just one cell in each grid which changes the problem to a classification of active or inactive for each grid which was not our purpose. Another option was to select the optimal grid size based on KL Divergence, which finally, We selected the grid size of 62.5 over 31.25. The decision of selecting 63.5 over 125.0 al-vFig. 3 : The distribution of active cells achieving the best accuracy, based on mae values, is shown for the four stages of NC 14 (A-D). In panels A-D, green rectangles indicate the frames from the previous stage used to predict the blue frames of the current stage. The features from the previous stage frames were averaged to predict the average number of active cells in each grid for the current stage. For each stage, the right-hand plot illustrates the predicted and actual distribution of active cells along the DV axis, represented by dashed blue and red lines, respectively. In these plots, the grid numbers along the DV axis are shown from 0 to 16, the average number of active cells per grid is displayed from 0 to 50, and the embryo width along the DV axis spans from 0 to 100.bar chartThe image is a collage of four different images, each with a caption and a graph. The images are labeled A, B, C, and D. The graphs are labeled \"DV Distribution\" and \"EV (Entropy)\". The images and graphs are arranged in a grid with four rows and four columns. The images are all in color and the graphs are all in blue. The images and graphs are all in high resolution. The images and graphs are all in the same orientation. The images and graphs are all in the same scale. The images and graphs are all in the same aspect ratio. The images and graphs are all in the same aspect ratio. The images and graphs are all in the same aspect ratio. The images and graphs are all in the same aspect ratio. The images and graphs are all in the same aspect ratio. The images and graphs are all in the same aspect ratio. The images and graphs are all in the same aspect ratio. The images and graphs are all![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000001_ca837fdb7d0b4d608e4c49b26f5c2801696ffdd39323f06945854009ab87ded2.png)though the 125 had lower KL Divergence, is attributed to the computational constraints of calculating Ripley's k-function for larger grid sizes in our setup.In subsequent experiment, we conducted an ablation study to discern the relative"
  },
  {
    "hash_code": "329068bab9ce35abb5d46e366b4600d9",
    "text": "images and graphs are all![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000001_ca837fdb7d0b4d608e4c49b26f5c2801696ffdd39323f06945854009ab87ded2.png)though the 125 had lower KL Divergence, is attributed to the computational constraints of calculating Ripley's k-function for larger grid sizes in our setup.In subsequent experiment, we conducted an ablation study to discern the relative importance of features, identifying those deemed crucial for inclusion in the final release and those that may be omitted. Table 1 indicates the performance of different combinations of features. It can be concluded that features of the first row including Ripley's k-function and n are the most important features that we used them for training and testing the pipeline. All reported mae values underwent the K-fold cross validation method to mitigate the influence of random results.To visualize the performance of the pipeline with selectedTable 1 : The average mae value on K-fold cross validation over test dataset for different combinations of features for ablation study.| Feature list                          |   mae ||---------------------------------------|-------|| n, Ripley's k-function                | 3.799 || m2, n, Ripley's k-function            | 3.86  || m2, m1 AP, n, Ripley's k-function     | 3.92  || Ripley's k-function                   | 3.93  || m2, m1 AP, m1 DV, Ripley's k-function | 3.94  |features and parameters, we tested the pre-trained model on test dataset. Fig 4 shows the active cell distribution for the best prediction based on the average mae values.## 3.2. Comparative Evaluation of sogD and sogD ∆ Su ( H )As, we had 6 videos for sogD ∆ Su ( H ) and 7 for sogD , we randomly selected 3 videos from each group for training and 1 for testing. Then, we averaged the AP mae, DV mae, and mean mae for whole sogD ∆ Su ( H ) and sogD experiments and calculated the difference between sogD ∆ Su ( H ) and sogD sogD ∆ Su ( H ) -sogD ) for each of these metrics and the results were 0.210, 1.511, and 0.86 respectively. We also used cross-validation to avoid overfitting. These results show there is a difference between the performance of our pipeline on sogD ∆ Su ( H ) and sogD in AP mean, mean mae and DV mean. In other words, our method works better in predicting along the AP axis, the mean of AP, and DV on the sogD data compared to the sogD ∆ Su ( H ) one. In order to substantiate this assertion, we conducted two additional experiments:First, we leveraged Mixed-Effects modelling, which can account for both fixed effects (like the group: sogD ∆ Su ( H ) or sogD and random effects (like the variation within videos and stages). The mixed-effects model can help in understanding the influence of these fixed and random effects on our dependent variables like DV mae, AP mae, mean mae. The goal is to understand whether there is a significant difference in any metrics between the sogD ∆ Su ( H ) and sogD groups, accounting for the variability introduced by different stages. The sogD has, on average, a lower AP mae compared to the sogD ∆ Su ( H ) by about 0.310 units with the P value of 0.476. It shows based on this test, there is not a statistically significant difference in AP mae between sogD ∆ Su ( H ) and sogD groups. However, the result for DV mae shows the sogD has lower value by 1.620 units and 0.001 P value. Also, the result for mean mae indicates sogD has lower value by 0.971 units and 0.019 P value. Two latter results for DV mae and mean mae indicate significant difference between sogD -∆ Su ( H ) and sogD . In other words, our performance on the sogD outperforms sogD ∆ Su ( H ) one based on DV mae and mean mae.In addition, we implemented another empirical hypothesis testing called the Bootstrap method. Bootstrap methods can be used to estimate the distribution of our metrics under the null hypothesis. To implement the bootstrap, we used the same metrics as previous method. we drew samples from the original dataset with replacement, to create a new dataset. Then, for each bootstrap sample, we computed the statistics of interest which"
  },
  {
    "hash_code": "6e0b5d1b28504e3f4b067df238849748",
    "text": "and mean mae indicate significant difference between sogD -∆ Su ( H ) and sogD . In other words, our performance on the sogD outperforms sogD ∆ Su ( H ) one based on DV mae and mean mae.In addition, we implemented another empirical hypothesis testing called the Bootstrap method. Bootstrap methods can be used to estimate the distribution of our metrics under the null hypothesis. To implement the bootstrap, we used the same metrics as previous method. we drew samples from the original dataset with replacement, to create a new dataset. Then, for each bootstrap sample, we computed the statistics of interest which are DV mae, AP mae, and mean mae. By analyzing the bootstrap distribution we can find the confidence intervals for each metrics. Fig. 4 -B shows the bootstrap distribution of mean difference in AP mae, DV mae, and mean mae. It indicates that with 95% confidence interval the mean difference of DV mae, (DV mae( sogD ∆ Su ( H ) ) - DV mae( sogD )) was between [0.409 2.61]. It can be concluded that with 95% confidence interval the DV mae for sogD ∆ Su ( H ) is at least 0.409 units higher than sogD , which means the performance of the pipeline for sogD outperforms sogD ∆ Su ( H ) one. These ranges for AP mae and mean mae are respectively, [-0.72 1.10] and [-0.18 1.74 ]. It can be seen that for AP mae and mean mae the ranges include zero means the performance of sogD can be better, equal, or worse than sogD ∆ Su ( H ) . The results with Bootstrap method confirms the results derived from mixed effects method, which makes sense given that large amounts of training data are needed to model transgenic effects.## 4. DISCUSSIONIn this study, we aimed to investigate whether a machine learning model can be trained to capture the trend of active cells during the embryogenesis process of Drosophila . Following a comprehensive ablation study, we identified the optimal model architecture, feature set, and grid size configuration. We then evaluated our pre-trained model on test data from both the sogD ∆ Su ( H ) and sogD datasets. Figure 3 presents the model's predictions on the sogD dataset, demonstrating strong performance in capturing the distribution of active cells along the DV axis, which is the primary focus of our study.Furthermore, we conducted an experiment to compare the model's performance on the sogD ∆ Su ( H ) and sogD datasets. The results indicate that the model performs better on the sogD dataset, as reflected by lower DV MAE values. However, despite the overall superior performance on the sogD dataset, the model successfully detected differences between the sogD ∆ Su ( H ) and sogD conditions, particularly in NC 13C and NC 14D, where a reduction in embryo width of the active cell distribution along the DV axis was observed.## 5. CONCLUSIONOur work presents several key contributions. Firstly, we have developed a novel and optimized imaging technology that delivers spatial information throughout the entire DV axis of an embryo. Secondly, we introduce an automated pipeline that effectively discriminates cell types with high accuracy. Lastly, our approach enables the accurate prediction of the stage-level distribution of active cells, based on data from the preceding stage.## 6. COMPLIANCE WITH ETHICAL STANDARDSAll animal experiments were approved by the UTA IACUC review board. This study was performed in line with the principles of the Declaration of Helsinki. Approval was granted by the Ethics Committee of my institution.## 7. ACKNOWLEDGMENTSThis work was supported by the Cancer Prevention and Research Institute of Texas (CPRIT) Recruitment of First-Time, Tenure-Track Faculty Members Grant (RR220015) (JML) and University of Texas System Rising STARs award (JML and TK). We would like to thank Mike Levine for providing fly lines and plasmids. We are also grateful to Angela Stathopoulos for generously supplying us with fly lines. We also thank all members of the Luber Lab for their productive discussions and the Koromila Lab for their help with administrative tasks and fly husbandry.## 8. REFERENCES- [1] Theodora Koromila and Angelike Stathopoulos, 'Distinct roles of broadly expressed repressors support dynamic enhancer action and change in time,' Cell reports , vol. 28, no. 4, pp. 855-863, 2019.- [2] Leslie Dunipace, Abbie Saunders, Hilary L Ashe, and Angelike Stathopoulos, 'Autoregulatory feedback controls sequential action of cis-regulatory modules at the brinker locus,' Developmental cell , vol. 26, no"
  },
  {
    "hash_code": "fcdfe054bc563c952d97e624de61d0f4",
    "text": "administrative tasks and fly husbandry.## 8. REFERENCES- [1] Theodora Koromila and Angelike Stathopoulos, 'Distinct roles of broadly expressed repressors support dynamic enhancer action and change in time,' Cell reports , vol. 28, no. 4, pp. 855-863, 2019.- [2] Leslie Dunipace, Abbie Saunders, Hilary L Ashe, and Angelike Stathopoulos, 'Autoregulatory feedback controls sequential action of cis-regulatory modules at the brinker locus,' Developmental cell , vol. 26, no. 5, pp. 536-543, 2013.- [3] Hannah K Long, Sara L Prescott, and Joanna Wysocka, 'Ever-changing landscapes: transcriptional enhancers in development and evolution,' Cell , vol. 167, no. 5, pp. 1170-1187, 2016.- [4] Michael W Perry, Jacques P Bothma, Ryan D Luu, and Michael Levine, 'Precision of hunchback expression in the drosophila embryo,' Current biology , vol. 22, no. 23, pp. 2247-2252, 2012.- [5] Bomyi Lim, Tyler Heist, Michael Levine, and Takashi Fukaya, 'Visualization of transvection in living drosophila embryos,' Molecular cell , vol. 70, no. 2, pp. 287-296, 2018.- [6] Anthony Birnie, Audrey Plat, Cemil Korkmaz, and Jacques P Bothma, 'Precisely timed regulation of enhancer activity defines the binary expression pattern ofFig. 4 : (A) Distribution of active cells along the DV axis for the case dataset, where the red line represents the actual distribution and the dashed blue line corresponds to the predicted distribution. (B) Bootstrap distribution results for AP -mae , DV -mae , and mean -mae presented from left to right, respectively. (C) Actual DV distribution for the case and control datasets, shown in light and dark red, respectively, to illustrate changes in width over time. (D) Predicted DV distribution for the case and control datasets, represented in dashed light and dark blue, respectively.bar chartThe image is a collection of four graphs (A, B, C, and D) that display the distribution of a variable, likely a financial or economic variable, across different scenarios or conditions. Each graph has a title, a legend, and a y-axis label. The x-axis in each graph is labeled with a range of values, from 0 to 100. The graphs are arranged in a 2x2 grid, with each graph having a unique title and a different color scheme. The graphs are labeled with \"NC_14_A\" to \"NC_14_D\", which could indicate different conditions or scenarios. The graphs show a comparison between two distributions, \"sogD\" and \"sog_Su(H)\", which could be different methods or models used to calculate the variable. The graphs also show a comparison between a \"DV Distribution Comparison (0-100% Embryo Width)\" and a \"Predict![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000002_10c8cd2a15a05e55f0e1ddcbc4e5c31558dcf478d5e64f21472c3b18c6aff506.png)fushi tarazu in the drosophila embryo,' Current Biology , 2023.- [7] Tanguy Lucas, Teresa Ferraro, Baptiste Roelens, Jose De Las Heras Chanes, Aleksandra M Walczak, Mathieu Coppey, and Nathalie Dostatni, 'Live imaging of bicoid-dependent transcription in drosophila embryos,' Current biology , vol. 23, no. 21, pp. 2135-2139, 2013.- [8] Gioele La Manno, Ruslan Soldatov, Amit Zeisel, Emelie Braun, Hannah Hochgerner, Viktor Petukhov, Katja Lidschreiber, Maria E Kastriti, Peter L¨ onnerberg, Alessandro Furlan, et al., 'Rna velocity of single cells,' Nature , vol. 560, no. 7719, pp. 494-498, 2018.- [9] Monica T Dayao, Alexandro Trevino, Honesty Kim, Matthew Ruffalo, H Blaize D'Angio, Ryan Preska, Umamaheswar Duvvuri, Aaron T Mayer, and Ziv BarJoseph, '"
  },
  {
    "hash_code": "d271fef893040dd6b59754f8f948d7d7",
    "text": "gerner, Viktor Petukhov, Katja Lidschreiber, Maria E Kastriti, Peter L¨ onnerberg, Alessandro Furlan, et al., 'Rna velocity of single cells,' Nature , vol. 560, no. 7719, pp. 494-498, 2018.- [9] Monica T Dayao, Alexandro Trevino, Honesty Kim, Matthew Ruffalo, H Blaize D'Angio, Ryan Preska, Umamaheswar Duvvuri, Aaron T Mayer, and Ziv BarJoseph, 'Deriving spatial features from in situ proteomics imaging to enhance cancer survival analysis,' Bioinformatics , vol. 39, no. Supplement 1, pp. i140i148, 2023.- [10] Kelli D Fenelon, Priyanshi Borad, Biraaj Rout, Parisa Boodaghi Malidarreh, Mohammad Sadegh Nasr, Jacob M Luber, and Theodora Koromila, 'Su (h) modulates enhancer transcriptional bursting in prelude to gastrulation,' Cells , vol. 13, no. 21, pp. 1759, 2024.## 9. SUPPLEMENTAL MATERIALbar chart![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000003_1252972eba79572fda94608e95744c161da3ea03c20e89442e5305105c86c3a3.png)Sup. Fig-1: Detailed information about our pipeline, (A) Ripley's K function are used as a feature, (B) XGBoost is the machine learning method we used (C) the output of the pipeline that shows the distribution of active cells.bar chartLine 1 and line 2 trend roughly overlapping with line 3 trending above them.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000004_8d117c23c09f680c03931acb6f0cf19c9c7fbc89a267f85a145215719a3b7703.png)Regression ModelsSup. Fig-2: The comparison of different machine learning methods on control dataset based on three different metrics: MSE, MAE, and Absolute Error. It shows the best result corresponds to XGBoost model.bar chartThe image is a collection of four graphs, each representing a different stage of a process, labeled as \"Stage NC 14 A\", \"Stage NC 14 B\", \"Stage NC 14 C\", and \"Stage NC 14 D\". Each graph has two axes: the x-axis labeled \"AP Axis\" and the y-axis labeled \"DV Axis\". The graphs show a series of data points, each represented by a different color, scattered across the axes. The data points are connected by lines, forming a pattern that varies between the graphs.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000005_49467f21b446ed5fb1dbb1e5a641cf713a65c20e0bc93922caefff4ef2f25557.png)Marker Scale when Absolute ErrorMarker Scale when Absolute ErrorMarker Scale when Absolule ErrorBlue; when True ValuePredicted ValueRed; when True ValuePredicted ValueTruePredictedSup. Fig-3: The distribution of active cell for the best accuracy based on mae values for control ( SogD ) dataset stages are NC 14 A-D. For each stage the top and right plot shows the distribution of active cells along AP and DV axis respectively. The middle plot shows the absolute error in each grid of the embryo to see where the prediction performs well.Sup. Fig-4: The distribution of active cell for the best accuracy based on mae values for case ( sogD ∆ Su ( H ) ) dataset stages are NC 14 A-D. For each stage the top and right plot shows the distribution of active cells along AP and DV axis respectively. The middle plot shows the absolute error in each grid of the embryo to see where the prediction demonstrates high reliability.bar chartThe image is a collection of four graphs, each representing a different stage of a process, labeled as \"Stage NC 14 A,\" \"Stage NC 14 B,\" \"Stage NC 14 C,\" and \"Stage NC 14 D.\" Each graph is divided into two sections: the left side shows the absolute error for grid-based prediction, and"
  },
  {
    "hash_code": "719c38c4ea601c20ee1fbf716c726547",
    "text": "H ) ) dataset stages are NC 14 A-D. For each stage the top and right plot shows the distribution of active cells along AP and DV axis respectively. The middle plot shows the absolute error in each grid of the embryo to see where the prediction demonstrates high reliability.bar chartThe image is a collection of four graphs, each representing a different stage of a process, labeled as \"Stage NC 14 A,\" \"Stage NC 14 B,\" \"Stage NC 14 C,\" and \"Stage NC 14 D.\" Each graph is divided into two sections: the left side shows the absolute error for grid-based prediction, and the right side shows the absolute error for the model. The graphs are arranged in a 2x2 grid, with each graph having a similar structure and color scheme.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.02564v2_artifacts\\image_000006_e9e34dfa517cce99d53506404eb2e967be85b12df588c7b1477d3ea9add6b94a.png)"
  },
  {
    "hash_code": "095df1ed82652328c47ef601f9befd65",
    "text": "1## Leveraging IS and TC: Optimal order execution subject to reference strategiesXue Cheng 1 , Peng Guo 1 , and Tai-Ho Wang 2LMEQF, Department of Financial Mathematics, School of Mathematical Sciences, Peking University, Beijing 100871, China. 2Department of Mathematics, Baruch College, CUNY, 1 Bernard Baruch Way, New York, NY 10010, USAMarch 5, 2025## AbstractThe paper addresses the problem of meta order execution from a broker-dealer's point of view in Almgren-Chriss model under execution risk. A broker-dealer agency is authorized to execute an order of trading on some client's behalf. The strategies that the agent is allowed to deploy is subject to a benchmark, referred to as the reference strategy, regulated by the client. We formulate the broker's problem as a utility maximization problem in which the broker seeks to maximize his utility of excess profit-and-loss at the execution horizon, of which optimal feedback strategies are obtained in closed form. In the absence of execution risk, the optimal strategies subject to reference strategies are deterministic. We establish an affine structure among the trading trajectories under optimal strategies subject to general reference strategies using implementation shortfall (IS) and target close (TC) orders as basis. Furthermore, an approximation theorem is proposed to show that with small error, general reference strategies can be approximated by piece-wise constant ones, of which the optimal strategy is piece-wise linear combination between IS and TC orders. We conclude the paper with numerical experiments illustrating the trading trajectories as well as histograms of terminal wealth and utility at investment horizon under optimal strategies versus those under TWAP strategies.Keywords: Optimal execution; Price impact; Execution risk; Utility maximization; Reference strategy; Affine structure; Implementation shortfall order; Target close order## Contents| 1   | Introduction                            |   3 ||-----|-----------------------------------------|-----|| 2   | Model setup                             |   5 || 2.1 | Price impact model .                    |   5 || 2.2 | Reference strategy                      |   6 || 3   | Optimal execution as utility maximizing |   7 ||     | Optimal execution with risk aversion    |   7 || 4 Zero execution risk   | 4 Zero execution risk   | 4 Zero execution risk                                             |   10 ||-------------------------|-------------------------|-------------------------------------------------------------------|------||                         | 4.1                     | General reference strategy                                        |   11 ||                         | 4.2                     | Implementation shortfall (IS) order and target close (TC) order . |   12 ||                         | 4.3                     | Endpoints-only reference strategy .                               |   14 ||                         | 4.4                     | Piece-wise constant reference strategy                            |   16 || 5                       | Numerical examples      | Numerical examples                                                |   18 ||                         | 5.1                     | Sample trading trajectories                                       |   18 ||                         | 5.2                     | Performance analysis and stress test                              |   22 || 6                       | Conclusion              | Conclusion                                                        |   25 ||                         | A Appendix              | A Appendix                                                        |   25 |## 1 IntroductionIn the financial industry, large position holders such as pension funds or investment banks for various reasons are required to trade in or trade out from their current position to an updated target, possibly subject to a given execution horizon which may vary from days to weeks. The net holdings to be adjusted between the current and the target positions are usually too large to be simply dumped to the market without a priori deliberately assessing the trade's market impact. Untamed price impact by trading may result in significant transaction cost, potentially turned into a substantial loss. Such an execution risk requires to be properly managed and controlled; otherwise it would eventually influence the position holder's overall profit and loss (P&amp;L). A common practice is to delegate the execution to the firm's order execution department or outsource to a broker-dealer agency.The large position holders, while delegating their tasks to an agency for execution, may have in mind their own preferred strategies or benchmarks that they would like the delegated agent to closely track along. For instance, implementation shortfall (IS) orders [1] are frequently employed by managers for the purpose of short-term alpha pursuit. IS orders are constructed with a pre-trade benchmark price in mind, aiming at executing orders at an average price that remains relatively close to the market price at the beginning of the trade. Managers can often use arrival prices to help measure total trading costs: the closer the execution price is to the arrival price, the lower the associated costs (see CFA-level-II [2]). On the contrary, target close (TC) orders [3], often deployed by index-fund managers for the purpose of minimizing fund risk"
  },
  {
    "hash_code": "db7fcb55f75b42d72b4b460b0846aa26",
    "text": ". For instance, implementation shortfall (IS) orders [1] are frequently employed by managers for the purpose of short-term alpha pursuit. IS orders are constructed with a pre-trade benchmark price in mind, aiming at executing orders at an average price that remains relatively close to the market price at the beginning of the trade. Managers can often use arrival prices to help measure total trading costs: the closer the execution price is to the arrival price, the lower the associated costs (see CFA-level-II [2]). On the contrary, target close (TC) orders [3], often deployed by index-fund managers for the purpose of minimizing fund risk and tracking error, are formulated with a post-trade benchmark price in order to secure a price in average that remains relatively close to the closing price. This is often important for mutual fund managers who manage funds that only calculate NAV once daily at closing. Volume weighted average price (VWAP) and Time weighted average price (TWAP) strategies are benchmarks specified for trading sequences with constant trading rate in wall clock time (TWAP) and in volume time (VWAP) respectively. These algorithmic trading strategies are examples of benchmarks that may be imposed as trading constraints to the agent who is missioned to trade in or trade out the position. We shall delve into this type of order execution problems subject to a pre-specified benchmark strategy, which we refer to as the reference strategy (RS for short), as a stochastic control problem and determine their corresponding optimal strategies in feedback form.The pioneering works in [4], [1], [5] and [6] are among the first to deal with the problem of order execution under price impact. Since its introduction to the order execution problem, numerous progresses and extensions on the classical Almgren-Chriss framework have been made extensively. For instance, [7] and [8] introduced the notion of transient impact to account for the dissipation of price impacts from the past trades. [9] discusses the objective in the optimization problem, while [10] extends the classical mean-variance framework first considered in [4] to encompass general risk measures as penalty for risk aversion. [11], [12], [13] solve the optimal execution problem in relation to a VWAP benchmark, while [14] asserts the use of the arrival price as a benchmark within the Almgren-Chriss framework. In fact, the arrival price, also known as 'pre-trade benchmark', appears to be the most commonly used benchmark in academic papers. The closing price is the value of a security at its last transaction during a trading session [3] [15]. Average price benchmarks may also appear in accelerated share repurchase (ASR) contracts [16][17], which can be regarded as optimal execution problems with optimal stopping. Above all, [18] introduces the concept of execution risk, thatpre-scheduled orders may not be fully executed, of which the empirical evidence is confirmed by [19] that the inventory processes of traders invariably contain a Brownian motion term, which contradicts the common assumption adopted in most optimal execution models that the inventory process is absolute continuous. Another perspective on introducing the noise term into the inventory process is that when a centralized trading desk aggregates order flows within a financial institution, stochastic order flow arises [20] [21]. The aforementioned papers are by no means meant for an exhausting list in literature on this line of active research.In the current paper, we consider the order execution problem from a broker-dealer's point of view. Assume that a broker is delegated to reallocate a client's holdings of a certain stock under the Almgren-Chriss model with execution risk. The broker is regulated by his client to track a benchmark strategy, i.e., the reference strategy, to the client's preference. The broker's incentive of executing client's order in this circumstance is to maximize his own expected P&amp;L excess to that of the reference strategy, marked-to-market. To account for risk aversion, we recast the broker's order execution problem as a utility maximization problem and, in certain cases, are able to solve the problem in closed form. In particular, when execution risk vanishes, or becomes negligible, we show that there exists an 'affine structure' among the optimal strategies induced from various reference strategies. This algebraic structure is supposed to help the broker for concocting and understanding optimal strategies subject to client's general reference strategies. We argue that the framework is highly versatile in the sense that it encompasses commonly deployed execution strategies such as IS, TC as well as TWAP and VWAP orders as special cases for benchmarking. As per the algebraic structure, it follows that, for any given continuous reference strategy which can be approximated by a piece-wise constant function, we show that its corresponding optimal strategy can also be approximated by those induced from the piece-wise constant strategies.The rest of the paper is organized as follows. In Section 2, we lay out the price impact model of AlmgrenChriss under execution risk and incorporate reference"
  },
  {
    "hash_code": "f312452641c2d6a42beecdbe3b1cda7a",
    "text": "general reference strategies. We argue that the framework is highly versatile in the sense that it encompasses commonly deployed execution strategies such as IS, TC as well as TWAP and VWAP orders as special cases for benchmarking. As per the algebraic structure, it follows that, for any given continuous reference strategy which can be approximated by a piece-wise constant function, we show that its corresponding optimal strategy can also be approximated by those induced from the piece-wise constant strategies.The rest of the paper is organized as follows. In Section 2, we lay out the price impact model of AlmgrenChriss under execution risk and incorporate reference strategies into the problem of order execution. Section 3 presents the optimal feedback control of the order execution problem in Theorem 1 as one of the main results in the paper. Section 4 focuses and provides detailed discussions on the optimal strategies when execution risk vanishes. Reference strategies and their associated optimal strategies considered in Section 4 include IS and TC orders as well as piece-wise constant strategies. The emphasis is put on an 'affine structure' among the trading trajectories induced by general reference strategies using unit IS and unit TC orders as a basis. Numerical examples illustrating the trading trajectories and the performance analysis under the optimal and TWAP strategies are shown and discussed in Section 5. For the sake of smooth reading, technical proofs of all the theorems, propositions and lemmas are postponed and collected till the end of the paper as an appendix in Section A.Throughout the paper, (Ω , F , P ) denotes a complete probability space equipped with a filtration describing the information structure F := {F t } t ∈ [0 ,T ] , where t is the time variable and T &gt; 0 the fixed finite liquidation horizon. Let { W t , Z t } t ∈ [0 ,T ] be a two-dimensional Brownian motion with constant correlation ρ defined on (Ω , F , P ) . The filtration F is generated by the trajectories of the above Brownian motion, completed with all P -null measure sets of F .## 2 Model setup## 2.1 Price impact modelAssume a broker is delegated to reallocate a client's holdings of a certain stock from x 0 shares to A shares within a given horizon T . Let x t be the number of shares the broker holds at time t ∈ [0 , T ] during the reallocation process and ˜ S t the transacted price at time t . The price dynamic is assumed to follow the Almgren-Chriss model [4] [1] [5]. In the Almgren-Chriss framework, the transacted price ˜ S t consists of the fair price S t and a slippage. The fair price S t is driven by the SDE$$d S _ { t } = \\mu d t + \\gamma d x _ { t } + \\sigma d W _ { t } ,$$$$S _ { t } = S _ { 0 } + \\mu t + \\gamma ( x _ { t } - x _ { 0 } ) + \\sigma W _ { t } ,$$where µt describes the tendency of the stock and ( W t ) t ∈ [0 ,T ] is a standard Brownian motion. The term γ ( x t -x 0 ) , for γ ≥ 0 , is usually referred to as the permanent impact . Penalized by a price slippage, the transacted price is thus given by$$S _ { t } = S _ { t } - \\eta v _ { t } ,$$where v t denotes the broker's intended trading rate at the instant t . The slippage -ηv t , for η ≥ 0 , is also referred to as the temporary impact . We remark that in the original setting of Almgren-Chriss and its extensions, the trading rate v t plays a dual role. On the one hand, it serves as the realized trading rate per the relationship v t = -˙ x t between x t and v t . On the other hand, it is regarded as the control variable in the problem of optimal execution. These two seemingly distinct roles coincide if all the scheduled orders are guaranteed fully executed. However, in reality it is well noticed among practitioners that, while executing a sequence of pre-scheduled orders, the orders in the sequence may not be fully executed, resulting in an uncontrollable realized order flow. This introduces an additional risk to the order execution problem, the execution risk . To account for this uncertainty, we introduce a noise component driven by a correlated Brownian motion Z t into the dynamic of the position x t as$$d x _ { t } = - v _ { t } d t + m ( v _ { t } ) d Z _ { t } .$$The diffusion term m ( v ) characterizes the magnitude of execution risk. It is worth"
  },
  {
    "hash_code": "433c385d915afafaa8b634b49f3971ea",
    "text": ", while executing a sequence of pre-scheduled orders, the orders in the sequence may not be fully executed, resulting in an uncontrollable realized order flow. This introduces an additional risk to the order execution problem, the execution risk . To account for this uncertainty, we introduce a noise component driven by a correlated Brownian motion Z t into the dynamic of the position x t as$$d x _ { t } = - v _ { t } d t + m ( v _ { t } ) d Z _ { t } .$$The diffusion term m ( v ) characterizes the magnitude of execution risk. It is worth to reiterate that in a recent work in [19], the authors showed that the presence of a Brownian component in the broker's inventory during reallocation process is statistically significant. Moreover, because of this execution risk, the broker is no longer guaranteed to achieve his intended position at the terminal time T , which gives rise to an additional opportunity cost. As a result, the broker is obligated to take a final block trade at time T at a worse price. Overall, the realised P&amp;L at the horizon T is given by$$\\Pi \\colon = ( x _ { T } - A ) S _ { T } - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( - \\tilde { S } _ { t } \\right ) d x _ { t } ,$$or equivalently,where the term β ( x T -A ) 2 , for β ≥ 0 , penalizes the discrepency from a final block trade.The following proposition shows that the P&amp;L can be written as an Itô process.Proposition 1. The realised P&amp;L can be rewritten as$$\\Pi = & ( x _ { 0 } - A ) S _ { 0 } - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - A ) + \\rho \\sigma m ( v _ { t } ) - \\eta v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\\\ & + \\int _ { 0 } ^ { T } \\sigma ( x _ { t } - A ) d W _ { t } + \\int _ { 0 } ^ { T } ( \\eta v _ { t } + \\gamma ( x _ { t } - A ) ) m ( v _ { t } ) d Z _ { t } .$$## 2.2 Reference strategyA common practice in order execution brokerage is that clients may come forward to brokers with their own preferred strategies for benchmarking. These benchmark strategies can be either strategies suggested by elite investors or commonly used ones such as TWAP strategies. We shall refer to these pre-specified benchmark strategies as reference strategies . We consider the reference strategies that can be represented by a deterministic function ( R t ) t ∈ [0 ,T ] with R 0 ≡ x 0 and R T ≡ A hereafter. Moreover, we assume that R t is a differentiable a.e. function. The broker's incentive of executing client's order is thus to maximize his own expected P&amp;L excess to that of the reference strategy, marked-to-market. Specifically, by disregarding the price impact and slippages incurred from order execution, the stock price S 0 t reads$$S _ { t } ^ { 0 } = S _ { 0 } + \\mu t + \\sigma W _ { t } .$$The marked-to-market P&amp;L Π R for the reference strategy R is evaluated as$$\\Pi ^ { R } & \\coloneqq \\int _ { 0 } ^ { T } \\left ( - S _ { t } ^ { 0 } \\right ) d R _ { t } \\\\ & = - \\left ( S _ { t } ^ { 0 } R _ { t } \\right ) \\left | _ { 0 } ^ { T } + \\int _ { 0 } ^ { T } R _ { t } ( \\rho d t + \\sigma d W _ { t } ) \\\\ & = - \\, . A S _ { T } ^ { 0 } + x _ { 0 } S _ { 0 } + \\int _ { 0 } ^ { T } \\mu R _ { t } d t + \\int _ { 0 } ^ { T } \\sigma R _ { t } d W _ { t } \\\\ & = ( x _ { 0 } - A ) S _ { 0 } + \\int"
  },
  {
    "hash_code": "b66ab1ec0fd5a1447d7f8575569c0e0e",
    "text": "_ { 0 } ^ { T } R _ { t } ( \\rho d t + \\sigma d W _ { t } ) \\\\ & = - \\, . A S _ { T } ^ { 0 } + x _ { 0 } S _ { 0 } + \\int _ { 0 } ^ { T } \\mu R _ { t } d t + \\int _ { 0 } ^ { T } \\sigma R _ { t } d W _ { t } \\\\ & = ( x _ { 0 } - A ) S _ { 0 } + \\int _ { 0 } ^ { T } \\mu ( R _ { t } - A ) d t + \\int _ { 0 } ^ { T } \\sigma ( R _ { t } - A ) d W _ { t } .$$Hence, the broker's excess P&amp;L ˜ Π , defined by the difference between Π and Π R , is given by$$\\tilde { \\Pi } \\coloneqq & \\Pi - \\Pi ^ { R } \\\\ & = - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - R _ { t } ) + \\rho \\sigma m ( v _ { t } ) - \\eta v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\\\ & \\quad + \\int _ { 0 } ^ { T } \\sigma ( x _ { t } - R _ { t } ) d W _ { t } + \\int _ { 0 } ^ { T } ( \\eta v _ { t } + \\gamma ( x _ { t } - A ) ) m ( v _ { t } ) d Z _ { t } .$$The broker's goal is thus to maximize his expected excess P&amp;L in a risk aversion manner which we recast as a utility maximization problem in the section that follows.## 3 Optimal execution as utility maximizingIn this section, we recast the problem of order execution as a utility maximization problem as follows. Recall that (Ω , F , P ) denotes a complete probability space equipped with a filtration ( F t ) t ≥ 0 satisfying the usual conditions. We assume that all random variables and stochastic processes are defined on (Ω , F , ( F t ) t ≥ 0 , P ) . The set of all real-valued progressively measurable processes are denote by M , while the collection of admissible controls A is set as$$\\mathcal { A } \\colon = \\left \\{ v \\in \\mathcal { M } \\colon \\int _ { 0 } ^ { T } \\mathbb { E } [ v _ { t } ^ { 2 } ] d t < \\infty \\right \\} .$$to meet the assumptions given in [22] and [23], which assure the solvability of both the risk-neutral problem and the risk-aversion one. The broker's problem is to determine an optimal admissible strategy v ∗ that maximizes his expected exponential utility at the horizon T , i.e.,$$\\sup _ { v \\in \\mathcal { A } } \\mathbb { E } \\left [ u \\left ( \\tilde { \\mathbf I } \\right ) \\right ] ,$$where the utility function u ( x ) := 1 θ (1 -e -θx ) , θ &gt; 0 represents the broker's preference following CARA (Constant Absolute Risk Aversion) preference and θ &gt; 0 is the risk-aversion parameter. Note that as θ ↓ 0 , the utility function u ( x ) = lim θ ↓ 0 1 θ (1 -e -θx ) = x reflects a risk-neutral preference. When the utility function is selected as u ( x ) = x , we solve the stochastic control problem and give the close form of the value function E [ ˜ Π ] ∣ ∣ ∣ ∣ v = v ∗∗ = sup v ∈A E [ ˜ Π ] &lt; ∞ , where v ∗∗ ∈ A is the optimal feedback control in the risk-neutral case, see in Appendix A.2. Moreover, when u ( x ) = 1 θ (1 -e -θx ) , u ′ ( x ) = e -θx &gt; 0 , u ′′ ( x ) = -θe -θx &lt; 0 , so u ( · ) is a increasing conc"
  },
  {
    "hash_code": "6d369d6a26258c25bdc4ee253ee7eb7f",
    "text": "] ∣ ∣ ∣ ∣ v = v ∗∗ = sup v ∈A E [ ˜ Π ] &lt; ∞ , where v ∗∗ ∈ A is the optimal feedback control in the risk-neutral case, see in Appendix A.2. Moreover, when u ( x ) = 1 θ (1 -e -θx ) , u ′ ( x ) = e -θx &gt; 0 , u ′′ ( x ) = -θe -θx &lt; 0 , so u ( · ) is a increasing concave function. For any admissible control ˜ v , by Jensen's inequality,$$\\mathbb { E } \\left [ u \\left ( \\tilde { \\Pi } \\right ) \\right ] \\Big | _ { v = \\tilde { v } } \\leq u \\left ( \\mathbb { E } \\left [ \\tilde { \\Pi } \\right ] \\Big | _ { v = \\tilde { v } ^ { * } } \\right ) \\leq u \\left ( \\mathbb { E } \\left [ \\tilde { \\Pi } \\right ] \\Big | _ { v = v ^ { * } } \\right ) < \\infty ,$$which shows the finiteness of the utility maximizing problem. In the following, we solve the utility maximization problem (2) and present solutions in closed form in the cases where the execution risk m ( v ) is either a constant or zero. The case of zero execution risk is postponed and will be discussed in more details in Section 4.## 3.1 Optimal execution with risk aversionWhen the execution risk is constant, i.e., m ( v ) ≡ m 0 for some fixed constant m 0 , the utility maximizing problem reduces to the following Stochastic Linear Exponential Quadratic (SLEQ) control problem [23] [24]$$\\begin{cases} \\sup _ { v \\in A } \\left [ u ( \\tilde { H } ) \\right ] = & \\sup _ { v \\in A } \\left [ \\frac { 1 } { \\theta } \\left ( 1 - \\exp \\left ( \\theta \\beta ( x _ { T } - A ) ^ { 2 } - \\int _ { 0 } ^ { T } \\theta \\left ( \\mu ( x _ { T } - R _ { T } ) + \\rho \\sigma m _ { 0 } - \\mu v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { T } - A ) \\right ) d t \\\\ & - \\int _ { 0 } ^ { T } \\theta \\sigma ( x _ { T } - R _ { T } ) d W _ { T } - \\int _ { 0 } ^ { T } \\theta ( \\mu v _ { t } + \\gamma ( x _ { T } - A ) m _ { 0 } d Z _ { T } ) \\right ) \\right ] , \\\\ & s . t . \\ x _ { T } = x _ { 0 } - \\int _ { 0 } ^ { T } v _ { s d s } + m _ { 0 } z _ { T } . \\end{cases}$$Define the value function of the problem by$$V ( t , x ) \\coloneqq & \\sup _ { v \\in A } \\left [ \\left \\lceil \\frac { 1 } { \\theta } \\left ( 1 - \\exp \\left ( \\theta \\beta ( x _ { T } - A ) ^ { 2 } - \\int _ { t } ^ { T } \\theta \\left ( \\mu ( x _ { s } - R _ { s } ) + \\rho \\sigma m _ { 0 } - \\eta v _ { s } ^ { 2 } - \\gamma v _ { s } ( x _ { s } - A ) \\right ) d s \\\\ & - \\int _ { t } ^ { T } \\theta \\sigma ( x _ { s } - R _ { s } ) d W _ { s } - \\int _ { t } ^ { T } \\theta ( \\eta v _ { s } + \\gamma ( x _ { s } - A ) ) m _ { 0 } d Z _ { s } \\right ) \\right \\rceil x _ { t } = x \\right ] ,$$where A t := { v is progressively measurable in [ t, T ] and ∫ T t E [ v 2 s ]d s &lt; ∞ } ."
  },
  {
    "hash_code": "eb9b84bae8600d843fe850960eaa2636",
    "text": "T } \\theta \\sigma ( x _ { s } - R _ { s } ) d W _ { s } - \\int _ { t } ^ { T } \\theta ( \\eta v _ { s } + \\gamma ( x _ { s } - A ) ) m _ { 0 } d Z _ { s } \\right ) \\right \\rceil x _ { t } = x \\right ] ,$$where A t := { v is progressively measurable in [ t, T ] and ∫ T t E [ v 2 s ]d s &lt; ∞ } . The optimal feedback control can be obtained in closed form. We summarize the result in the following theorem.Theorem 1. The value function (4) of the utility maximization problem has the closed form expression$$V ( t , x ) = \\frac { 1 } { \\theta } \\left ( 1 - \\exp \\left \\{ \\left ( b _ { 2 } ( t ) + \\frac { \\theta \\gamma } { 2 } \\right ) ( x - A ) ^ { 2 } + b _ { 1 } ( t ) ( x - A ) + b _ { 0 } ( t ) \\right \\} \\right ) .$$where the parameters H , l 1 , and l 3 are given by$$\\begin{cases} \\, H = \\theta \\eta + \\frac { 1 } { 2 } \\theta ^ { 2 } \\eta ^ { 2 } m _ { 0 } ^ { 2 } , \\\\ \\, l _ { 3 } = \\frac { 1 } { 2 } m _ { 0 } \\eta \\theta ^ { 2 } \\rho \\sigma , \\\\ \\, l _ { 1 } = \\frac { \\theta ^ { 2 } \\sigma ^ { 2 } } { 2 } H . \\end{cases}$$The time dependent function b 2 is given in closed form by$$b _ { 2 } ( t ) \\coloneqq \\sqrt { l _ { 1 } } \\coth \\left ( A _ { 0 } + \\frac { \\sqrt { l _ { 1 } } } { H } ( T - t ) \\right ) - l _ { 3 } , \\quad \\text {where} \\ \\ A _ { 0 } \\coloneqq \\coth ^ { - 1 } \\left ( \\frac { l _ { 3 } + \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) } { \\sqrt { l _ { 1 } } } \\right ) ,$$b 1 is the solution to the following terminal value problem$$b _ { 1 } ^ { \\prime } ( t ) = \\theta \\mu + \\frac { 1 } { H } b _ { 1 } ( t ) \\left ( b _ { 2 } ( t ) + l _ { 3 } \\right ) + \\frac { \\eta \\theta ^ { 2 } \\sigma } { H } ( R _ { t } - A ) \\left ( \\theta \\sigma + \\frac { 1 } { 2 } m _ { 0 } ^ { 2 } \\theta _ { 0 } ^ { 2 } \\eta \\sigma ( 1 - \\rho ^ { 2 } ) - \\rho m _ { 0 } b _ { 2 } ( t ) \\right ) , \\ b _ { 1 } ( T ) = 0 ,$$and$$b _ { 0 } ( t ) \\colon = \\int _ { t } ^ { T } \\left [ \\frac { l _ { 1 } - l _ { 3 } ^ { 2 } } { H } ( R _ { s } - A ) ^ { 2 } + \\left ( \\theta \\mu + \\frac { l _ { 3 } } { H } b _ { 1 } \\right ) ( R _ { s ^ { - } } - A ) + \\left ( - \\frac { b _ { 1 } ^ { 2 } } { 4 H } + m _ { 0 } ^ { 2 } \\left ( b _ { 2 } + \\frac { \\theta \\gamma } { 2 } \\right ) - \\rho m _ { 0 } \\theta \\sigma \\right ) \\right ] d s .$$Moreover, the optimal feedback control v ∗ ∈ A of the utility maximization problem (2) is given by$$v _ { t } ^ { * } = \\frac { ( 1 + \\theta"
  },
  {
    "hash_code": "878cb249bda17328942584457d5a5ade",
    "text": "left ( - \\frac { b _ { 1 } ^ { 2 } } { 4 H } + m _ { 0 } ^ { 2 } \\left ( b _ { 2 } + \\frac { \\theta \\gamma } { 2 } \\right ) - \\rho m _ { 0 } \\theta \\sigma \\right ) \\right ] d s .$$Moreover, the optimal feedback control v ∗ ∈ A of the utility maximization problem (2) is given by$$v _ { t } ^ { * } = \\frac { ( 1 + \\theta \\eta m _ { 0 } ^ { 2 } ) _ { t } ^ { 2 } } { H } \\cdot ( x _ { t } - A ) + \\frac { 1 + \\theta \\eta m _ { 0 } ^ { 2 } } { 2 H } \\cdot b _ { 1 } ( t ) + \\frac { l _ { 3 } } { H } \\cdot ( R _ { t } - x _ { t } ) ,$$It's worth mentioning that this theorem does not contain much mathematical breakthrough, instead, we regard it as a fundamental result to introduce valuable results in section 4, where the affine structure is what we find the most interesting.Remark 1. The optimal feedback control v ∗ t in (5) consists of three parts:- The first part (1+ θηm 2 0 ) b 2 H · ( x t -A ) has the same sign as x t -A . Without loss of generality, we assume that x 0 &gt; A , when x t ≥ A , which implies the reallocation process has not finished, so the broker shouldcontinue to sell the stock. Conversely, when x t &lt; A , which implies the strategy is over-shooting, so the broker should buy some shares of the stock back.- The second part 1+ θηm 2 0 2 H · b 1 , which is a pre-specified deterministic function in time t , does not depend on the state variable x t .- The third part l 3 H · ( R t -x t ) has the same sign as ρ ( R t -x t ) , where ρ is the correlation between the stock price process and the execution risk. The discussions in [25] state the fact that the sign of ρ depends on the order type adopted by the trader due to adverse selection: when the trader uses market orders, the correlation ρ is negative while ρ is positive whenever trading with limit orders. In this paper, we regard ρ as a market parameter reflecting whether the market is trader-friendly or not.- -When ρ ≥ 0 , the stock price and execution risk tend to increase or decrease together: when the stock price increases, execution risk makes traders buy more or sell less than the amount they submit, which is benefit to the traders; when the stock price decreases, execution risk forces traders to buy less or sell more, which is also benefit to the traders. In such trader-friendly market, like the figure (1a), the broker should keep away from the reference strategy R t because an overly conservative strategy is not necessary.- -On the contrary, ρ &lt; 0 implies a trader-harmful market: when the stock price increases, execution risk makes traders buy less or sell more than the amount they submit, which is harm to the traders; when the stock price decreases, execution risk forces traders to buy more or sell less, which is also harm to the traders. In such trader-harmful market, like the figure (1b), the broker should keep close to the reference strategy R t to attain lower risk.Figure 1: Market parameter ρotherThe diagram consists of two figures, labeled as (a) and (b). In figure (a), there is a straight line segment labeled as x. On this line segment, there are two points labeled as A and B. A line segment is drawn from point A to a point labeled as C, forming a right angle with the line segment x. In figure (b), there is a straight line segment labeled as x. On this line segment, there are two points labeled as A and B. A line segment is drawn from point A to a point labeled as C, forming a right angle with the line segment x.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000000_d741fe3abc50ba8b101f99984b7f56633e5cd054dd136ca3c292cb28539656a0.png)Remark 2. By applying the optimal strategy (5), one obtains the optimal liquidation"
  },
  {
    "hash_code": "1d6fed7835b6626a7969173cc0dfdaf7",
    "text": "there are two points labeled as A and B. A line segment is drawn from point A to a point labeled as C, forming a right angle with the line segment x.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000000_d741fe3abc50ba8b101f99984b7f56633e5cd054dd136ca3c292cb28539656a0.png)Remark 2. By applying the optimal strategy (5), one obtains the optimal liquidation trajectory x ∗ t as follows. For simplicity, take the parameters A = ρ = 0 , R t ≡ 0 and µ = 0 , x ∗ t satisfies the SDE$$d x _ { t } ^ { * } = - \\left ( \\frac { ( 1 + \\theta \\eta m _ { 0 } ^ { 2 } ) b _ { 2 } } { H } x _ { t } ^ { * } + \\frac { ( 1 + \\theta \\eta m _ { 0 } ^ { 2 } ) b _ { 1 } } { 2 H } \\right ) \\, \\mathrm d t + m _ { 0 } \\mathrm d Z _ { t } ,$$as β → + ∞ , x ∗ t has the limit$$x _ { t } ^ { * * } = \\left ( \\frac { \\sinh \\left ( \\frac { \\sqrt { l _ { 1 } } } { H } ( T - t ) \\right ) } { \\sinh \\left ( \\frac { \\sqrt { l _ { 1 } } } { H } T \\right ) } \\right ) ^ { \\frac { 2 H } { \\theta \\eta } - 1 } x _ { 0 } + m _ { 0 } \\int _ { 0 } ^ { t } \\left ( \\frac { \\sinh \\left ( \\frac { \\sqrt { l _ { 1 } } } { H } ( T - t ) \\right ) } { \\sinh \\left ( \\frac { \\sqrt { l _ { 1 } } } { H } ( T - s ) \\right ) } \\right ) ^ { \\frac { 2 H } { \\theta \\eta } - 1 } d Z _ { s } .$$Furthermore, when m 0 → 0 , H → θη , the limit of the expression above is$$\\lim _ { m _ { 0 } \\to 0 } x _ { t } ^ { * * } = \\frac { \\sinh ( \\kappa ( T - t ) ) } { \\sinh ( \\kappa T ) } x _ { 0 } ,$$where κ = √ θσ 2 2 η . It recovers the classical implementation shortfall (IS) strategy (see in section 4.2), which implies the fact that our strategy can be regarded as a version of adaptive IS strategy. Furthermore, we may find that$$\\lim _ { m _ { 0 } \\to 0 } \\frac { x _ { t } ^ { * * } - \\frac { \\sinh ( \\kappa ( T - t ) ) } { \\sinh ( \\kappa T ) } x _ { 0 } } { m _ { 0 } } = \\int _ { 0 } ^ { t } \\frac { \\sinh \\left ( \\kappa ( T - t ) \\right ) } { \\sinh \\left ( \\kappa ( T - s ) \\right ) } d Z _ { s } ,$$which is an Ornstein-Uhlenbeck bridge [26]. Therefore, when m 0 is small, one may find an approximation to the optimal strategy$$x _ { t } ^ { * * } \\approx \\frac { \\sinh ( \\kappa ( T - t ) ) } { \\sinh ( \\kappa T ) } x _ { 0 } + m _ { 0 } \\int _ { 0 } ^ { t } \\frac { \\sinh \\left ( \\kappa ( T - t ) \\right ) } { \\sinh \\left ( \\kappa ( T - s ) \\right ) } d Z _ { s } ,$$which is a classical IS strategy plus an Ornstein-Uhlenbeck bridge, which reduces the strategy's inventory risk. Figure (2) shows the inventory risk of the optimal strategy and the classical IS strategy, which shows the fact that our optimal strategy has lower inventory risk than the classical IS strategy, especially at the end"
  },
  {
    "hash_code": "02a58c67fc284f9a329663b4c5b1416b",
    "text": "_ { 0 } \\int _ { 0 } ^ { t } \\frac { \\sinh \\left ( \\kappa ( T - t ) \\right ) } { \\sinh \\left ( \\kappa ( T - s ) \\right ) } d Z _ { s } ,$$which is a classical IS strategy plus an Ornstein-Uhlenbeck bridge, which reduces the strategy's inventory risk. Figure (2) shows the inventory risk of the optimal strategy and the classical IS strategy, which shows the fact that our optimal strategy has lower inventory risk than the classical IS strategy, especially at the end of the trading process.Figure 2: Inventory risk of the optimal strategy and the classical IS strategyline chartThe image you've provided is a line graph with two lines, one labeled \"Almgren-Chriss\" and the other \"Optimal\". The \"Almgren-Chriss\" line starts at the origin (0,0) and increases linearly, reaching the top right corner of the graph. The \"Optimal\" line starts at (0.2, 0.2) and decreases linearly, also reaching the bottom left corner of the graph. The graph is set against a light gray background.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000001_af20e34e77e45e19240dc271a5c58471da60864335ff34830d4e09dd7cb274eb.png)To summary the section, it should be emphasized that despite the optimal feedback control behaves linearly in the states carries a certain meaning which we discuss in Remark 1, however, the case which we care most is where the execution risk does not exist, which is a limit case of Theorem 1, discussed in the next section.## 4 Zero execution riskFor the purpose of better understanding as well as visualizing the optimal strategy obtained in (5), we present in this section the trading trajectory x t under optimal strategy in the case when m 0 = 0 for various reference strategies. In Section 4.1, we give the optimal strategy for general reference strategies. In Section 4.2, we present two classic trading strategies: IS order and TC order, both widely used in the market, and show thatthey are special cases of our model. Next, we consider the case when the reference strategy is an endpointsonly one in Section 4.3, of which the optimal strategy has a heuristic form. Last but not least, piece-wise constant reference strategies are studied in Section 4.4.Notice that in this case the setting of the problem reduces to that of the Almgren-Chriss but subject to a reference strategy. Furthermore, we shall set the final penalty parameter β → + ∞ , indicating that a final block trade at terminal time T is strictly prohibited.## 4.1 General reference strategyFor a given generic reference strategy R t , the following theorem shows a representation for the trading trajectory x t under optimal control v ∗ .Theorem 2. Let m ( v ) ≡ 0 and β → + ∞ . The trajectory x t under the solution to the optimization problem (2) is given by$$x _ { t } = & \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( R _ { s } \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) d s \\\\ & + \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + A + \\left ( - A + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } ,$$with the tuning parameter κ := √ θσ 2 2 η .A few remarks on the trajectory x t shown in Theorem 2 are in order.Remark 3. The parameter κ can be regarded as a tuning parameter between the TWAP and the reference strategies if µ = 0 . Specifically, in the limit as κ approaches zero, the optimal strategy converges to the TWAP strategy, i.e., for any t ∈ [0 , T ] , we have$$\\lim _ { \\kappa \\to 0 } x _ { t } = A + ( x _ {"
  },
  {
    "hash_code": "0d3f55220fa61a93adfe173183437a25",
    "text": "$$with the tuning parameter κ := √ θσ 2 2 η .A few remarks on the trajectory x t shown in Theorem 2 are in order.Remark 3. The parameter κ can be regarded as a tuning parameter between the TWAP and the reference strategies if µ = 0 . Specifically, in the limit as κ approaches zero, the optimal strategy converges to the TWAP strategy, i.e., for any t ∈ [0 , T ] , we have$$\\lim _ { \\kappa \\to 0 } x _ { t } = A + ( x _ { 0 } - A ) \\cdot \\left ( 1 - \\frac { t } { T } \\right ) .$$On the other extreme as κ →∞ , the optimal trajectory converges to the reference strategy R t itself$$\\lim _ { \\kappa \\rightarrow \\infty } x _ { t } = R _ { t }$$for any t ∈ [0 , T ] .Remark 4. The optimal trajectory may also be written as an 'affine transformation' as$$x _ { t } = \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + a _ { t } \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + b _ { t } \\cdot \\frac { \\sinh \\kappa t } { \\sinh \\kappa T } ,$$$$t$$$$a _ { t } & = x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + \\int _ { 0 } ^ { t } \\kappa R _ { s } \\sinh \\kappa s d s , \\\\ b _ { t } & = A - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + \\int _ { t } ^ { T } \\kappa R _ { s } \\sinh \\kappa ( T - s ) d s .$$$$l _ { S } .$$A more detailed discussion on this affine transformation can be found in Section 4.4.whereNext, we further specialize the reference strategies and present their corresponding trajectories in the following sections.## 4.2 Implementation shortfall (IS) order and target close (TC) orderAssume µ = 0 and the broker is asked to take as the reference strategy a block trade of size | A -x 0 | at t = 0 , i.e.,$$R _ { t } ^ { I S } = \\begin{cases} x _ { 0 } , & t = 0 , \\\\ A , & 0 < t \\leq T . \\end{cases}$$The corresponding marked-to-market P&amp;L of the reference strategy R IS is given by$$\\Pi ^ { I S } \\coloneqq \\int _ { 0 } ^ { T } \\left ( - S _ { t } ^ { I S } \\right ) d R _ { t } ^ { I S } & = - \\left ( x _ { 0 } - A \\right ) \\cdot \\frac { S _ { 0 - } ^ { I S } + S _ { 0 + } ^ { I S } } { 2 } \\\\ & = - \\left ( x _ { 0 } - A \\right ) S _ { 0 } + \\frac { \\gamma } { 2 } ( x _ { 0 } - A ) ^ { 2 } ,$$which is equivalent to choosing the initial price S 0 as reference price. This model is referred to as an implementation shortfall model in [27], of which the optimal strategy is$$x _ { t } = A + ( x _ { 0 } - A ) \\cdot \\text {IS} _ { t } ,$$$$IS _ { t } \\colon = \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } .$$We shall refer to the strategy IS t as the unit implementation shortfall (IS) order . Hence, the trajectory in (6) can be interpreted as executing x 0 -A unit IS orders if a broker is missioned to make transition of his position from x 0 to A shares. We remark that, as we will show in Section 4.4, the unit IS orders are used as one of the building blocks for the construction of optimal trajectories for more general reference strategies.Figure 3: Basic strategies in varying parameter κ .line chartThe graph shows the relationship between the number of hours studied and the final exam score.![Image](C:\\Users\\KITES"
  },
  {
    "hash_code": "0420cf44a331dc4fda93d28a3c60c02e",
    "text": "unit implementation shortfall (IS) order . Hence, the trajectory in (6) can be interpreted as executing x 0 -A unit IS orders if a broker is missioned to make transition of his position from x 0 to A shares. We remark that, as we will show in Section 4.4, the unit IS orders are used as one of the building blocks for the construction of optimal trajectories for more general reference strategies.Figure 3: Basic strategies in varying parameter κ .line chartThe graph shows the relationship between the number of hours studied and the final exam score.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000002_a53f0e1665b766b1a458bb0660f2cb375265336d1541d0794be173f81da1712d.png)Figure (3a) shows examples of the unit implementation shortfall orders under various values of κ . Notice that, since κ = √ θσ 2 2 η , as θ, σ increase or η decreases, thus κ increases, the trajectories suggest the broker wheretrade faster at the beginning and more slowly for the rest of the reallocation process. Indeed, the larger the κ , the faster the trading at the beginning. The financial rationale is as follows. Recall that θ is the coefficient of relative risk aversion for the utility function and σ is the volatility of stock price. Hence, if either of the parameters increases, the broker is more concerned with the price risk than the execution risk, he tends toward trading faster at the beginning and more slowly thereafter. Also, since η is the coefficient of slippage, as it decreases, the slippage decreases as well, resulting in lower cost from faster transactions. Therefore, again to mitigate the concern of price risk during reallocation process, the broker should also trade faster at the beginning as well.Finally, we remark that, as κ → 0 , the unit IS order (7) converges to the TWAP (Time-Weight-AveragePrice) strategy: for any t ∈ [0 , T ] ,$$\\lim _ { \\kappa \\to 0 } I S _ { t } = \\lim _ { \\kappa \\to 0 } \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } = \\frac { T - t } { T } .$$On the flip side, as κ → + ∞ , the optimal trajectory (7) converges to a block trade at time 0 :$$\\lim _ { \\kappa \\to \\infty } \\text {IS} _ { t } = \\lim _ { \\kappa \\to \\infty } \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } = \\lim _ { \\kappa \\to \\infty } e ^ { - \\kappa t } \\cdot \\frac { 1 - e ^ { - 2 \\kappa ( T - t ) } } { 1 - e ^ { - 2 \\kappa T } } = \\begin{cases} 1 , t = 0 , \\\\ 0 , 0 < t \\leq T . \\end{cases}$$Assume again that µ = 0 . Here the broker is given as the reference strategy to take only a block trade of size | A -x 0 | at terminal time T . That is,$$R _ { t } ^ { T C } = \\begin{cases} x _ { 0 } , \\ 0 \\leq t < T , \\\\ A , t = T . \\end{cases}$$The marked-to-market P&amp;L Π TC for this reference strategy is given by$$\\Pi ^ { T C } = \\int _ { 0 } ^ { T } \\left ( - S _ { t } ^ { T C } \\right ) d R _ { t } ^ { T C } = & - ( x _ { 0 } - A ) \\cdot \\frac { S _ { T - } ^ { T C } - S _ { T + } ^ { T C } } { 2 } \\\\ = & - ( x _ { 0 } - A ) S _ { T + } ^ { T C } - \\frac { \\gamma } { 2 } ( x _ { 0 } - A ) ^ { 2 } ,$$which is equivalent to choosing the stock price S T at terminal time as the reference price. This model is referred to as an target close model [27][3], of which the optimal strategy is given by$$x _ { t } = A +"
  },
  {
    "hash_code": "d8aac5d8d4405e944bd891ceb26e21c6",
    "text": "} ^ { T C } - S _ { T + } ^ { T C } } { 2 } \\\\ = & - ( x _ { 0 } - A ) S _ { T + } ^ { T C } - \\frac { \\gamma } { 2 } ( x _ { 0 } - A ) ^ { 2 } ,$$which is equivalent to choosing the stock price S T at terminal time as the reference price. This model is referred to as an target close model [27][3], of which the optimal strategy is given by$$x _ { t } = A + ( x _ { 0 } - A ) \\cdot T C _ { t } ,$$$$T C _ { t } \\coloneqq \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } .$$We refer to the strategy TC t in (9) as the unit target close (TC) order . Thus, similar to the case of implementation shortfall order in Section 4.2, the trajectory in (8) can be interpreted as executing x 0 -A unit TC orders if a broker is missioned to make transition of his position from x 0 to A shares. Likewise as for the unit IS orders, we show in Section 4.4 that the unit TC orders are used as the other building blocks for the whereconstruction of optimal trajectories for more general reference strategies.Figure (3b) shows examples of plots for unit target close orders in different values of κ . We observe that the trading trajectories are concave in time as opposed to convex in time as that of the unit IS orders. Also, as κ increases, the optimal trajectory moves towards the reference strategy and towards the TWAP trajectory as κ decreases. In fact, similar to the case for implementation shortfall, in the limit as κ → 0 , the unit TC order (9) converges to the TWAP strategy: for any t ∈ [0 , T ] ,$$\\lim _ { \\kappa \\to 0 } T C _ { t } = \\lim _ { \\kappa \\to 0 } \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } = 1 - \\frac { t } { T } ;$$whereas in the other extreme as κ → + ∞ , the optimal trajectory converges to a block trade at terminal time T :$$\\lim _ { \\kappa \\to \\infty } T C _ { t } & = \\lim _ { \\kappa \\to \\infty } \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } = 1 - \\lim _ { \\kappa \\to \\infty } e ^ { - \\kappa ( t - T ) } \\cdot \\frac { 1 - e ^ { - \\kappa t } } { 1 - e ^ { - \\kappa T } } = \\begin{cases} 0 , \\, 0 \\leq t < T , \\\\ 1 , \\, t = T . \\end{cases}$$## 4.3 Endpoints-only reference strategyIn this section, we consider the following reference strategy R t , which we called the endpoints-only .$$R _ { t } = \\begin{cases} x _ { 0 } , t = 0 , \\\\ R , 0 < t < T , \\\\ A , t = T . \\end{cases}$$The strategy is simply to hold R shares during the entire reallocation process, except at the initial and terminal times,.The following lemma will prove itself useful in the determination of the optimal strategy subject to the reference strategy (10).## Lemma 1. We have that$$& \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) d s \\\\ = & \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } - \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } \\\\ = & T C _ { t } - I S _ { t } .$$In other words, the given integral can be written as the difference between a unit TC and a unit IS order.We summarize the result for optimal strategies subject to (10) in the proposition that follows.Proposition 2. With the reference strategy given in (10), the optimal"
  },
  {
    "hash_code": "103f543ee9db3e1d19aaf303b12aeb57",
    "text": "frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } - \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } \\\\ = & T C _ { t } - I S _ { t } .$$In other words, the given integral can be written as the difference between a unit TC and a unit IS order.We summarize the result for optimal strategies subject to (10) in the proposition that follows.Proposition 2. With the reference strategy given in (10), the optimal trajectory is given by an affine combination of a unit IS order and a unit TC order as$$x _ { t } = \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } - R \\right ) \\cdot I S _ { t } + \\left ( - A + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + R \\right ) \\cdot T C _ { t } + A .$$Note that when µ = 0 and R = x 0 , the optimal strategy reduces to the following TC strategy as in (8)$$x _ { t } = ( x _ { 0 } - A ) \\cdot T C _ { t } + A ;$$whereas when µ = 0 and R = A , the optimal strategy reduces to the following IS strategy as in (6)$$x _ { t } = ( x _ { 0 } - A ) \\cdot I S _ { t } + A .$$Note that for any given reference level R , the optimal trajectory in (11) at any point in time is an affine combination of a unit IS order and a unit TC order. The coefficients represent a trade-off between the two strategies. For example, if R is closer to the initial position x 0 and further away from the target position A , the strategy in (11) suggests we weigh in more on the unit TC order and less on the unit IS order; conversely, one should then weigh in more on the unit IS order. Figure (4) shows the plots of optimal trajectories in different reference levels R , assuming x 0 = 1 and A = 0 . Again, we end up with a unit TC order when R = x 0 and a unit IS order when R = A .Figure 4: Optimal strategies in different reference levels R .line chartThe image is a pair of graphs, each depicting a different strategy in a game. The left graph shows the optimal strategy, represented by a dashed line, while the right graph shows the reservation strategy, represented by a dotted line. The optimal strategy starts at a higher value and decreases over time, while the reservation strategy starts at a lower value and increases over time. The optimal strategy reaches a peak at around 0.5, while the reservation strategy reaches a peak at around 0.25. The optimal strategy is more volatile, with larger fluctuations, while the reservation strategy is more stable, with smaller fluctuations.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000003_185ec9d774fa5d922bc1f5d2c46e0de7943f7be59768355bc5fb0df5e1741cef.png)As shown in Figure 4, when R &gt; x 0 = 1 the optimal strategy suggest overshoot the target, meaning buying more than needed then sell back later and quicker when time approaches the terminal time. On the other hand, if R &lt; A = 0 , the optimal strategy suggests undershoots the target. We provide conditions on the parameters under which an overshooting or an undershooting occur in the following proposition.Proposition 3. Without loss of generality, assume x 0 &gt; A . Let x t be the optimal strategy given in (11) . We have that, if R + µ θσ 2 &gt; x 0 , then x t is concave in t and it becomes convex in t if R + µ θσ 2 &lt; A . It follows that x t overshoots if and only if$$R + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } > x _ { 0 } + \\frac { 1 } { \\cosh \\kappa T - 1 } \\cdot ( x _ { 0 } - A )$$and it undershoots if and only if$$R + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } < A - \\frac { 1 } { \\cosh \\k"
  },
  {
    "hash_code": "e545d97c7b1179a127a5939e1256e977",
    "text": "θσ 2 &lt; A . It follows that x t overshoots if and only if$$R + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } > x _ { 0 } + \\frac { 1 } { \\cosh \\kappa T - 1 } \\cdot ( x _ { 0 } - A )$$and it undershoots if and only if$$R + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } < A - \\frac { 1 } { \\cosh \\kappa T - 1 } \\cdot ( x _ { 0 } - A ) .$$It appears that, when µ is nonzero, the terms in the optimal trajectory (11) that have µ involved always come in the form of µ θσ 2 . We provide a financial rationale of this combination of parameters as follows. Note that the combination resembles the Merton ratio in Merton's optimal portfolio problem in the market consisting of two assets: one risky and the other risk-less. The agent is risk averse with power utility u ( x ) = x γ and the dynamic of risky asset is governed by a geometric Brownian motion with expected return µ and volatility σ . The risk-less asset is assumed accruing zero interest. In this setting, the Merton ratio, which represents the percentage of wealth invested in the risky asset, is given by µ (1 -γ ) σ 2 . In our setting, the broker is with exponential utility u ( x ) = 1 θ (1 -e -θx ) and the stock price S t is assumed following an arithmetic Brownian motion with drift µ and volatility σ , i.e.,$$d S _ { t } = \\mu d t + \\sigma d W _ { t } .$$As in the derivation of the Merton ratio in Merton's problem, one may show that the 'Merton ratio' [28] in this case is given by µ θσ 2 except that this ratio does not represent the percentage of wealth invested in the risky asset as in the original Merton's problem. Indeed, this ratio represents the number of shares to be held in the optimal portfolio.## 4.4 Piece-wise constant reference strategyWe consider in this section the reference strategies that are piece-wise constant across time. The consideration is that, to possibly account for real time market environments during the entire reallocation process, the broker's strategies may be subject to interval TWAP or VWAP strategies. For example, in an intraday trading activity, it is documented that the market trades much more actively at the times close to opening and closing than that in the middle of common trading days. Also, in the case where trading horizon across multiple days, the broker is most likely treating each trading day individually. In these regards, a piece-wise constant reference strategy is considered plausible.Theoretically, as will be demonstrated, optimal trajectories subject to piece-wise constant reference strategies exhibit an elegant algebraic structure, derived from the unit IS and unit TC orders discussed in Sections 4.2. By applying standard approximation and limiting processes for integrable functions, this structure offers practical and insightful guidance for developing optimal strategies under general integrable reference strategies. Concretely, a generic piece-wise constant reference strategy R t is defined as$$R _ { t } = ( x _ { 0 } - R ^ { ( 1 ) } ) \\, \\mathbb { I } _ { t = 0 } \\, + \\sum _ { k = 1 } ^ { n } R ^ { ( k ) } \\, \\mathbb { I } _ { \\{ \\frac { ( k - 1 ) T } { n } \\leq t \\leq \\frac { k T } { n } \\} } + A \\, \\mathbb { I } _ { \\{ t = T \\} } ,$$where 1 {·} denotes the indicator function, n is the number of periods, and the R ( k ) 's are fixed constants. The following proposition summarizes the optimal trajectory under the piece-wise constant reference strategy in (12).Proposition 4. When µ = m 0 = 0 , β → + ∞ , denote R (0) := x 0 and R ( n +1) := A , the solution to the previous optimization problem under the piece-wise constant reference strategy given in (12) is of the following form, for 1 ≤ k ≤ n ,$$x _ { t } = & \\left \\{ a _ { k } + \\left ( R ^ { ( k ) } - a _ { k } \\right ) T C _ { t - \\frac { ( k - 1 ) T } { n } } + \\left ( a _ { k - 1 } - R ^ { ( k"
  },
  {
    "hash_code": "eae432d82ec9bcfcd0a912740c1a68c5",
    "text": "denote R (0) := x 0 and R ( n +1) := A , the solution to the previous optimization problem under the piece-wise constant reference strategy given in (12) is of the following form, for 1 ≤ k ≤ n ,$$x _ { t } = & \\left \\{ a _ { k } + \\left ( R ^ { ( k ) } - a _ { k } \\right ) T C _ { t - \\frac { ( k - 1 ) T } { n } } + \\left ( a _ { k - 1 } - R ^ { ( k ) } \\right ) I _ { t - \\frac { ( k - 1 ) T } { n } } \\right \\} 1 _ { \\{ \\frac { ( k - 1 ) T } { n } \\leq t < \\frac { T } { n } \\} } , \\\\ \\intertext { w i t h \\, \\kappa \\, = \\, \\sqrt { \\frac { 9 \\frac { 2 ^ { n } } { 2 n } } { 2 n } } , } & \\quad \\mathbb { I } _ { S } \\colon = \\frac { \\sinh \\kappa \\left ( \\frac { T } { n } - t \\right ) } { \\sinh \\kappa \\frac { T } { n } } , \\quad T C _ { t } \\colon = \\frac { \\sinh \\kappa \\frac { T } { n } - \\sinh \\kappa t } { \\sinh \\kappa \\frac { T } { n } } ,$$$$IS _ { t } \\coloneqq \\frac { \\sinh \\kappa \\left ( \\frac { T } { n } - t \\right ) } { \\sinh \\kappa \\frac { T } { n } } , \\quad T C _ { t } \\coloneqq \\frac { \\sinh \\kappa \\frac { T } { n } - \\sinh \\kappa t } { \\sinh \\kappa \\frac { T } { n } } ,$$and a k is denoted as a weighted average of R ( i ) 's:$$a _ { k } \\coloneqq \\frac { \\sinh \\kappa \\left ( T - \\frac { k T } { n } \\right ) } { \\sinh \\kappa T } \\cdot \\sum _ { i = 0 } ^ { k } b _ { i } R ^ { ( i ) } + \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } \\cdot \\sum _ { i = k + 1 } ^ { n + 1 } b _ { n - i + 1 } R ^ { ( i ) } , \\ 1 \\leq k \\leq n - 1 ,$$where$$b _ { i } = \\begin{cases} 1 , & i = 0 , \\\\ \\cosh \\left ( \\kappa \\frac { i T } { n } \\right ) - \\cosh \\left ( \\kappa \\frac { ( i - 1 ) T } { n } \\right ) , & 1 \\leq i \\leq n - 1 . \\end{cases}$$Notice that the IS t and TC t in Proposition 4 are indeed respectively the unit IS order given in (7) and the unit TC order in (9). Proposition 4 essentially states that, for a given piece-wise constant reference strategy, its corresponding optimal solution in each sub-interval is given by an affine transformation of a unit IS order and a unit TC order. In this sense we may regard the unit IS orders and the unit TC orders obtained in Sections 4.2 as the building blocks or bases for optimal trajectories under piece-wise constant reference strategies. The algebraic structure of the optimal trajectories is thus depicted by the affine space generated by the unit IS and unit TC orders. What are left to be determined are the coefficients that are sub-interval dependent. In practice, one should firstly calculate a k 's by linear combinations of R ( i ) 's, then optimal strategy is obtained by connecting a k 's using unit IS and unit TC orders. As an example, Figure (5) shows the plots of a piece-wise constant reference strategy (in dotted red) its optimal trajectory (in blue) with three sub-intervals n = 3 , the other parameters are given by T = 3 , κ = 5 , R (1) = 15 2 , R (2) = 9 2 , R (3) = 3 2 , x 0"
  },
  {
    "hash_code": "284fbdb5bb5b870e5606cbe2fe78a179",
    "text": ", one should firstly calculate a k 's by linear combinations of R ( i ) 's, then optimal strategy is obtained by connecting a k 's using unit IS and unit TC orders. As an example, Figure (5) shows the plots of a piece-wise constant reference strategy (in dotted red) its optimal trajectory (in blue) with three sub-intervals n = 3 , the other parameters are given by T = 3 , κ = 5 , R (1) = 15 2 , R (2) = 9 2 , R (3) = 3 2 , x 0 = 9 , A = 0 . To conclude theFigure 5: A three-period piece-wise constant reference strategy and its corresponding optimal trajectory.line chartThe image is a line graph with two lines, one solid and one dashed, plotted against a time axis from 0 to 30. The solid line starts at 9, decreases to 0, and then increases to 1. The dashed line starts at 7.5, decreases to 4.5, and then increases to 7.5.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000004_d89ddef110f1480b5768203c4024ce99bcda21795cea21a4dd5226436eb40fcd.png)section, we show in Theorem 3 a stability theorem for optimal strategies subject to general, smooth enough, reference strategies.Theorem 3. For any given reference strategies R t and ˜ R t satisfying$$\\int _ { 0 } ^ { T } \\left ( R _ { t } - \\widetilde { R } _ { t } \\right ) ^ { 2 } d t < \\varepsilon ,$$for some ε &gt; 0 . Let x t and ˜ x t be the corresponding optimal trajectories for R t and ˜ R t , respectively. We have that, for any 0 ≤ t ≤ T ,$$| x _ { t } - \\widetilde { x } _ { t } | < \\frac { 1 } { 2 } \\sqrt { \\kappa \\varepsilon } .$$The above theorem basically states the fact that when two reference strategies are close in the L 2 sense, their corresponding optimal strategies remain close in the sup norm sense. Note that for any ε &gt; 0 and an almost everywhere continuous bounded RS R t , there exists a piece-wise constant RS ˜ R t such that$$\\int _ { 0 } ^ { T } \\left ( R _ { t } - \\widetilde { R } _ { t } \\right ) ^ { 2 } d t < \\varepsilon ,$$It follows that the optimal trajectory in (13) for ˜ R t , which is an affine transformation of unit IS and unit TC orders in each sub-interval, can be applied as an approximation to the optimal trajectory under reference strategy R t , with error estimate given in Theorem 3.## 5 Numerical examplesWe conduct in this section numerical experiments on the implementation of optimal strategy obtained in Theorem 1 and stress testing the strategy against various parameters. Monte Carlo simulations are implemented to illustrate sample trading trajectories and the performances criteria of the optimal strategy versus those of a related TWAP strategy. The performance of the optimal and TWAP strategies are then stress tested against certain extreme parameters. We remark that the parameters chosen for the numerical examples in this section are for convenience only. In practice, parameters are supposedly calibrated to market data prior to implementation, causing possible issues associated with estimation risk.## 5.1 Sample trading trajectoriesWe present in Figures (6) through (11) sample trading trajectories under the optimal and a related TWAP strategies in various parameters. Parameters chosen as base case are ( m 0 , η, ρ, θ, σ, β ) = (0 . 05 , 10 , 0 , 0 . 002 , 200 , 1000) . Table 1 summarizes the parameters that vary, while the others are held fixed, in each case and their corresponding figures. The reference strategy R t is selected as a block trade at t = 0 , as shown in Section 4.2.Figure (6) illustrates sample trading trajectories for the optimal and the TWAP strategies in different values of m 0 . We note that, since m 0 quantifies the execution risk, as m 0 increases both trajectories become more volatile and fluctuating.Table 1: Sample trading trajectory in varying parameters| Figure number   | Parameter   |   Case 1 |   Case 2 |    Case 3 ||-----------------|"
  },
  {
    "hash_code": "dfe16a627cc533303e285dadb6ebd158",
    "text": "are held fixed, in each case and their corresponding figures. The reference strategy R t is selected as a block trade at t = 0 , as shown in Section 4.2.Figure (6) illustrates sample trading trajectories for the optimal and the TWAP strategies in different values of m 0 . We note that, since m 0 quantifies the execution risk, as m 0 increases both trajectories become more volatile and fluctuating.Table 1: Sample trading trajectory in varying parameters| Figure number   | Parameter   |   Case 1 |   Case 2 |    Case 3 ||-----------------|-------------|----------|----------|-----------|| Figure 6        | m 0         |    0     |    0.05  |     0.1   || Figure 7        | η           |    5     |   10     |    20     || Figure 8        | ρ           |   -0.9   |    0     |     0.9   || Figure 9        | θ           |    0.001 |    0.002 |     0.004 || Figure 10       | σ           |  100     |  200     |   400     || Figure 11       | β           |  100     | 1000     | 10000     |Figure 6: Sample trading trajectories in different m 0line chartThe image is a line graph that compares the performance of two different algorithms, \"Optimal\" and \"TWAP\", over three different values of a parameter \"m0\". The x-axis represents \"m0\" and the y-axis represents the performance of the algorithms. The \"Optimal\" algorithm consistently outperforms the \"TWAP\" algorithm across all three values of \"m0\". The \"TWAP\" algorithm shows a significant decrease in performance as \"m0\" increases, while the \"Optimal\" algorithm shows a more gradual decrease.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000005_f2bb3fbffbd97b24220e90a4f0ce0248dafab2b26107e68c43a588de6a4b797c.png)Figure (7) exhibits sample trading trajectories in varying values of η . It is worth mentioning that, since η reflects the level of transaction costs, as the value of η increases, transaction cost becomes more significant in the determination of optimal strategies. Consequently, the optimal strategy tends to align more closely with the classical TWAP strategy. In other words, a higher η places greater emphasis on minimizing transaction costs, motivating the adoption of a strategy that mirrors the TWAP approach, which aims for consistent execution over time.Figure (8) shows optimal strategies in the values of ρ = -0 . 5 , 0 , 0 . 5 . Recall that ρ denotes the correlation between the stock price and the execution risk. We note that there seems no significant dependence of the optimal strategies on ρ within this set of parameters.Figure 7: Sample trading trajectories in different ηline chartThe image is a line graph with three different lines, each representing a different variable. The x-axis is labeled with numbers ranging from 0 to 100. The y-axis is labeled with numbers ranging from 0 to 1.0. The lines are labeled as \"optimal\", \"TWAP\", and \"TWAP-optimal\". The \"optimal\" line is the highest and the \"TWAP\" and \"TWAP-optimal\" lines are the lowest. The \"TWAP\" line is the most stable and the \"TWAP-optimal\" line is the most variable.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000006_5e0e1c6189245346e2601afb17e1e5b0e5251b4101a8ab4de43575a3b6378bfd.png)Figure 8: Sample trading trajectories in different ρline chartThe image shows three graphs with different trends.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000007_3caddd071accdceffb7079ad026d836ccd19917c07cce1469997b47c50e16a8d.png)Figure (9) illustrates the optimal strategies in the values of θ = 0 . 001 , 0 . 002 , 0 . 004 , which represents"
  },
  {
    "hash_code": "51eccf4c2dd5cb6127c7bb04ec817a20",
    "text": "trading trajectories in different ρline chartThe image shows three graphs with different trends.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000007_3caddd071accdceffb7079ad026d836ccd19917c07cce1469997b47c50e16a8d.png)Figure (9) illustrates the optimal strategies in the values of θ = 0 . 001 , 0 . 002 , 0 . 004 , which represents the broker's taste of risk aversion. The larger the value of θ , the more risk averse the broker tends to be. It follows that the sample trading trajectory under optimal strategy gradually converges towards the reference strategy as θ increases, indicating the broker's inclination to adopt a more conservative and risk-averse approach.Figure 9: Sample trading trajectories in different θline chartThe image is a line graph with three different lines, each representing a different variable. The x-axis is labeled with values ranging from 0 to 100, and the y-axis is labeled with values ranging from 0 to 1.0. The lines are labeled as \"optimal\", \"TWAP\", and \"TWAP-optimal\". The \"TWAP\" line is the highest and the \"TWAP-optimal\" line is the lowest.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000008_1aa5cf6acf7099e006dfbeb30e0195a944b77cd61506eac25974cbb66b7915cf.png)Figure (10) shows sample trading trajectories under optimal strategies with respect to the stock volatility σ = 100 , 200 , 400 . As σ increases, the price risk increases. Thus, to mitigate the risk incurred from the price volatility during execution, the optimal strategy tracks more closely to the reference strategy.Figure 10: Sample trading trajectories in different σline chartThe image is a line graph that compares the performance of two different algorithms, Optimal and TWAP, over three different parameter values (σ=100, σ=200, and σ=400). The x-axis represents the parameter values, while the y-axis represents the performance of the algorithms.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000009_55bd8a0bb0447784abf51d79d7285cf55daf8e9e7de363981d75524be69e04b8.png)Figure (11) presents the optimal strategies in the values of β = 100 , 1000 , 10000 , which quantifies the penalty of block trade at terminal time. Large β indicates that any remaining inventory at terminal time is unfavorable. Therefore, it is seen in the figure that the terminal inventory x ∗ T of the sample trading trajectories under optimal strategy gradually approaches zero. It is thus fathomable that, as β approaches infinity, a final block trade at terminal time turns into strictly prohibited.Figure 11: Sample trading trajectories in different βline chartThe image shows a comparison of two functions, \"optimal\" and \"TWAP\", over a range of x-values.![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000010_626cf181b3415010ba39fe8053e261b19c5c443b53f3578f73f1ca265279a7f0.png)## 5.2 Performance analysis and stress testTo demonstrate the performance of the optimal strategy against TWAP, we implement the strategies using Monte Carlo simulations and present the resulting histograms and boxplots of the terminal wealths as well as the utilities at the investment horizon. This performance analysis serves to illustrate the optimality and robustness of the optimal strategy.Figure 12: Histograms and boxplots of terminal wealths and utilities under the optimal and the TWAP strategies. Terminal wealth on the left, utility on the right. Vertical dashed lines indicate sample means.bar chartThe image consists of two graphs, each showing the distribution of a certain variable, with the x-axis representing the variable and the y-axis representing the frequency or density of the variable. The graphs are identical in terms of the x-axis, but differ in the shape of the y-axis. The first graph has a linear scale, while the second graph has a logarithmic scale. The first graph has a peak at the maximum value of the variable, while the second graph has a peak at"
  },
  {
    "hash_code": "4a2f52b97fa8ad6ee4bc7d171fd46347",
    "text": "utilities under the optimal and the TWAP strategies. Terminal wealth on the left, utility on the right. Vertical dashed lines indicate sample means.bar chartThe image consists of two graphs, each showing the distribution of a certain variable, with the x-axis representing the variable and the y-axis representing the frequency or density of the variable. The graphs are identical in terms of the x-axis, but differ in the shape of the y-axis. The first graph has a linear scale, while the second graph has a logarithmic scale. The first graph has a peak at the maximum value of the variable, while the second graph has a peak at the minimum value of the variable. The first graph has a bell-shaped curve, while the second graph has a flat line. The first graph has a sharp decline, while the second graph has a gentle decline. The first graph has a sharp decline, while the second graph has a gentle decline. The first graph has a sharp decline, while the second graph has a gentle decline. The first graph has![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000011_c7ea17caff469101fb9793bbe94fe6d1425238672bcc21a9473e7a91df615d59.png)Figure (12) exhibits the histograms and boxplots of the terminal P&amp;L (on the left) and the utility (on the left) at investment horizon for the optimal and the TWAP strategies. We observe that the optimalTable 2: Stress testing parameters|                         |   m 0 |    η |   ρ |     θ |    σ |     β ||-------------------------|-------|------|-----|-------|------|-------|| Baseline                |  0.05 | 10   |   0 | 0.002 |  200 | 1e+06 || Scenario 1 (large σ )   |  0.05 | 10   |   0 | 0.002 | 2000 | 1e+06 || Scenario 2 (large β )   |  0.05 | 10   |   0 | 0.002 |  200 | 1e+07 || Scenario 3 (large m 0 ) |  5    | 10   |   0 | 0.002 |  200 | 1e+06 || Scenario 4 (small η )   |  0.05 |  0.1 |   0 | 0.002 |  200 | 1e+06 |strategy not only consistently achieves higher average terminal wealth and utility than those under TWAP but also bears lower variability. Furthermore, an intriguing observation is made regarding the tail ends of the distribution. Specifically, the TWAP strategy demonstrates a greater density in these tail regions, indicating a higher likelihood of incurring significant losses when compared to our optimal strategy. This discrepancy in density at the tail end reinforces the notion that the TWAP strategy may accentuate the potential for unfavorable outcomes, thus reflecting a subpar approach to risk management.Finally, we stress test the optimal and the TWAP strategies against extreme parameters. While the remaining parameters are held the same as in the benchmark case, the stock volatility σ is scaled up by ten folds in Scenario 1, the terminal penalty coefficient β up by ten folds in Scenario 2, the execution risk m 0 up by a hundred folds in Scenario 3, and lastly in Scenario 4 down by 0.1 hundred folds the temporary cost η . Table 2 summarizes the parameters that are stress tested in each scenario and Figure 13 shows the histograms and boxplots of P&amp;Ls at the terminal time under the optimal and the TWAP strategies.Apparently, in all the scenarios the histograms under optimal strategies are concentrated around zero whereas those under TWAP strategies are much more widely spreading. Also, the sample means under optimal strategies are higher than those of TWAP strategies. We conclude that, even under extreme situations, the optimal strategies outperform TWAP strategies not only in higher averaged P&amp;L but also with lower volatility risk, which shows the robustness of our results.Figure 13: Histograms and boxplots of terminal P&amp;L under stress tests. Scenarios 1 on the top left, 2 on the top right, 3 on the bottom left, and 4 on the bottom right. Vertical dashed lines indicate sample means.bar chartThe image is a collection of four graphs, each depicting a different type of data. The first graph shows a line graph with a title \"TWAP optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal"
  },
  {
    "hash_code": "439989d4b4caf847f610149141f8a832",
    "text": "of our results.Figure 13: Histograms and boxplots of terminal P&amp;L under stress tests. Scenarios 1 on the top left, 2 on the top right, 3 on the bottom left, and 4 on the bottom right. Vertical dashed lines indicate sample means.bar chartThe image is a collection of four graphs, each depicting a different type of data. The first graph shows a line graph with a title \"TWAP optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal optimal![Image](C:\\Users\\KITES\\Desktop\\Projekt2025\\data\\OpenRAGBench\\out\\2401.03305v2_artifacts\\image_000012_64e42194784f001cc9adab5d7d23ddd52a8db39085e99c1ac93a328e68454be8.png)## 6 ConclusionIn this article, we showed how the broker's order execution problem under execution risk subject to client's reference strategy can be recast as a utility maximization problem. The optimal solution to the utility maximization problem was obtained by solving an associated Riccati differential equation and was presented in feedback form. When execution risk vanishes, the optimal strategies become deterministic and were given in closed form. We showed that trading trajectories subject to the unit IS and the unit TC orders form a basis of an affine structure for trading trajectories under optimal strategies subject to general piece-wise constant reference strategies. As for general continuous reference strategies, we proved an approximation and stability theorem for the trading trajectory under the corresponding optimal strategy via the trajectories from piece-wise constant reference strategies.Numerical experiments showed that the optimal strategies not only achieve higher expected values but also bear lower variabilities as opposed to those under TWAP strategies. Numerical performance analysis and stress tests confirmed that optimal strategies outperform TWAP strategies, by a wide margin in certain cases. Possible extensions of the framework considered in the paper include (a) a stochastic component to the price evolution reflecting the overall market activity such macroeconomics factors or news; (b) the price impact, rather than permanent, being transient with proper decay kernel. We leave these considerations to a future study.## AcknowledgementThis research is supported by the National Natural Science Foundation of China (Grants No.11971040).## A Appendix## A.1 Proof to Proposition 1By Itô's formula,$$( x _ { T } - A ) S _ { T } & = ( x _ { 0 } - A ) S _ { 0 } + \\int _ { 0 } ^ { T } S _ { t } d x _ { t } + \\iint _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - A ) + \\rho \\sigma m ( v _ { t } ) - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\\\ & + \\int _ { 0 } ^ { T } \\sigma ( x _ { t } - A ) d W _ { t } + \\int _ { 0 } ^ { T } \\gamma ( x _ { t } - A ) m ( v _ { t } ) d Z _ { t } .$$Furthermore, note that ˜ S t = S t + ηv t ,$$\\int _ { 0 } ^ { T } \\left ( - \\tilde { S } _ { t } \\right ) \\text {d} x _ { t } = - \\int _ { 0 } ^ { T } S _ { t } \\text {d} x _ { t } - \\int _ { 0 } ^ { T } \\eta v _ { t } ^ { 2 } \\text {d} t + \\int _ { 0 } ^ { T } \\eta v _ { t } m ( v _ { t } ) \\text {d} Z _ { t } ,$$in summary,$$\\Pi = & ( x _ { T } - A ) S _ { T } - \\beta ("
  },
  {
    "hash_code": "3701eb83b2e4f048ad1c6c87c92db043",
    "text": "_ { t } = - \\int _ { 0 } ^ { T } S _ { t } \\text {d} x _ { t } - \\int _ { 0 } ^ { T } \\eta v _ { t } ^ { 2 } \\text {d} t + \\int _ { 0 } ^ { T } \\eta v _ { t } m ( v _ { t } ) \\text {d} Z _ { t } ,$$in summary,$$\\Pi = & ( x _ { T } - A ) S _ { T } - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( - \\tilde { S } _ { T } \\right ) d x _ { t } \\\\ = & ( x _ { 0 } - A ) S _ { 0 } - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - A ) + \\rho \\sigma m ( v _ { t } ) - \\eta v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\\\ & + \\int _ { 0 } ^ { T } \\sigma ( x _ { t } - A ) d W _ { t } + \\int _ { 0 } ^ { T } ( \\eta v _ { t } + \\gamma ( x _ { t } - A ) ) m ( v _ { t } ) d Z _ { t } .$$## A.2 Risk-Neutral Case of Theorem 1In Section (3) we recast the optimal execution problem as a utility maximization problem with the CARA preference u ( x ) = 1 θ (1 -e -θx ) with parameter θ &gt; 0 , which reflects the broker's risk aversion. As θ → 0 , the utility function u ( x ) = lim θ → 0 1 θ (1 -e -θx ) = x describes a risk-neutral preference. With the admissible set A and the excess P&amp;L ˜ Π given in Section (2.2) and Section (3), the risk-neutral expected utility is given by$$\\mathbb { E } \\left [ u \\left ( \\tilde { \\Pi } \\right ) \\right ] = \\mathbb { E } \\left [ \\left [ - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - R _ { t } ) + \\rho \\sigma ( m _ { t } ) - \\nu p _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\\\ + \\int _ { 0 } ^ { T } \\sigma ( x _ { t } - R _ { t } ) d W _ { t } + \\int _ { 0 } ^ { T } \\left ( \\mu v _ { t } + \\gamma ( x _ { t } - A ) m ( v _ { t } ) d Z _ { t } \\right ] \\\\ = \\mathbb { E } \\left [ - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - R _ { t } ) + \\rho \\sigma ( m _ { t } ) - \\nu p _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\right ] .$$When the execution risk is constant (i.e. m ( · ) ≡ m 0 for some m 0 ≥ 0 ), the utility maximization problem reduces to the following Stochastic Linear Quadratic (SLQ) control problem [22]:$$\\begin{cases} \\sup _ { v \\in A } \\mathbb { E } \\left [ - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - R _ { t } ) + \\rho \\sigma m _ { 0 } - \\eta v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\right ] , \\\\"
  },
  {
    "hash_code": "3070231f79f8e9de9c95fee462fcd96b",
    "text": ") control problem [22]:$$\\begin{cases} \\sup _ { v \\in A } \\mathbb { E } \\left [ - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { 0 } ^ { T } \\left ( \\mu ( x _ { t } - R _ { t } ) + \\rho \\sigma m _ { 0 } - \\eta v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t \\right ] , \\\\ s . t . \\ x _ { t } = x _ { 0 } - \\int _ { 0 } ^ { t } v _ { s } d s + m _ { 0 } Z _ { t } . \\end{cases}$$Define the value function V ( t, x ) as$$V ( t , x ) = \\sup _ { v \\in \\mathcal { A } _ { t } } \\mathbb { E } \\left [ - \\beta ( x _ { T } - A ) ^ { 2 } + \\int _ { t } ^ { T } \\left ( \\mu ( x _ { s } - R _ { s } ) + \\rho \\sigma m _ { 0 } - \\eta v _ { s } ^ { 2 } - \\gamma v _ { s } ( x _ { s } - A ) \\right ) d s \\Big | x _ { t } = x \\right ] ,$$where A t := { v is progressively measurable in [ t, T ] and ∫ T t E [ v 2 s ]d s &lt; ∞ } . Note that our problem satisfies assumption (L1) given in Section 3.1 of [22] due to the boundedness of the coefficients, so the problem is solvable, and by the dynamic programming principle, the value function V is unique satisfying the HamiltonJacobi-Bellman (HJB hereafter) equation$$V _ { t } + \\frac { 1 } { 2 } m _ { 0 } ^ { 2 } V _ { x x } + \\sup _ { v \\in \\mathbb { R } } \\left \\{ - \\eta v ^ { 2 } - v \\left ( \\gamma ( x - A ) + V _ { x } \\right ) \\right \\} + \\mu ( x - R _ { t } ) + \\rho \\sigma m _ { 0 } = 0$$□with terminal condition v ( T, x ) = -β ( x -A ) 2 . With first order criterion, the optimal feedback control is given by v ∗ = -1 2 η ( γ ( x -A ) + V x ) , hence the HJB equation reduces to$$V _ { t } + \\frac { 1 } { 2 } m _ { 0 } ^ { 2 } V _ { x x } + \\frac { 1 } { 4 \\eta } \\left ( \\gamma ( x - A ) + V _ { x } \\right ) ^ { 2 } + \\mu ( x - R _ { t } ) + \\rho \\sigma m _ { 0 } = 0 , \\ \\ v ( T , x ) = - \\beta ( x - A ) ^ { 2 } .$$Assume the Ansarz for value function V$$V ( t , x ) = a ( t ) x ^ { 2 } + b ( t ) x + c ( t ) .$$Plug the Ansarz into the HJB equation yields the Riccati ODE system$$\\begin{cases} a ^ { \\prime } ( t ) + \\frac { 1 } { 4 \\eta } ( 2 a ( t ) + \\gamma ) ^ { 2 } = 0 , & a ( T ) = - \\beta , \\\\ b ^ { \\prime } ( t ) + \\frac { 1 } { 2 \\eta } ( 2 a ( t ) + \\gamma ) ( b ( t ) - \\gamma A ) + \\mu = 0 , & b ( T ) = 2 \\beta A , \\\\ c ^ { \\prime } ( t ) + m _ { 0 } ^ { 2 } a ( t ) + \\frac { 1 } { 4 \\eta } ( b ( t ) - \\gamma A ) ^ { 2 } - \\mu R _ { t } + \\rho \\sigma m _ { 0 } = 0 , & c ( T ) = - \\beta A ^ { 2 } . \\end{cases"
  },
  {
    "hash_code": "8e1662a620803960fdd1ac4952f5a0e0",
    "text": "gamma ) ( b ( t ) - \\gamma A ) + \\mu = 0 , & b ( T ) = 2 \\beta A , \\\\ c ^ { \\prime } ( t ) + m _ { 0 } ^ { 2 } a ( t ) + \\frac { 1 } { 4 \\eta } ( b ( t ) - \\gamma A ) ^ { 2 } - \\mu R _ { t } + \\rho \\sigma m _ { 0 } = 0 , & c ( T ) = - \\beta A ^ { 2 } . \\end{cases}$$By solving the ODE system, the deterministic functions a, b, c of time t are given by$$a ( t ) & = - \\ \\frac { \\eta } { T - t + \\alpha } - \\frac { \\gamma } { 2 } , \\\\ b ( t ) & = \\frac { \\mu } { 2 } ( T - t + \\alpha ) - \\frac { \\frac { 1 } { 2 } \\mu \\alpha ^ { 2 } - 2 A \\eta } { T - t + \\alpha } + \\gamma A , \\\\ c ( t ) & = - \\ m _ { 2 } \\eta \\log \\frac { T - t + \\alpha } { \\alpha } + \\left ( \\rho ( \\sigma _ { 0 } \\eta - \\frac { \\gamma } { 2 } \\eta _ { 0 } ^ { 2 } ) \\left ( T - t \\right ) } \\\\ & - \\frac { \\mu ^ { 2 \\alpha } \\alpha ^ { 3 } } { 1 6 \\eta } \\left ( \\frac { \\alpha } { T - t + \\alpha } + \\frac { 2 ( T - t + \\alpha ) } { \\alpha } - \\frac { ( T - t + \\alpha ) ^ { 3 } } { 3 \\alpha ^ { 3 } } - \\frac { 8 } { 3 } \\right ) \\\\ & - \\frac { A ^ { 2 \\eta } - \\frac { 1 } { 2 } \\mu A \\alpha ^ { 2 } } { T - t + \\alpha } - \\frac { \\gamma A ^ { 2 } + \\mu A ( T - t + \\alpha ) } { 2 } + \\int _ { t } ^ { T } \\mu ( A - R _ { s } ) d s \\\\ \\vdots & = \\frac { 2 \\eta } { 2 } \\geq 0 , \\, \\text {Note that} \\, V ( t , x ) \\equiv a ( t ) x ^ { 2 } + b ( t ) x + c ( t ) \\text { is continuous on } [ 0 , T ] \\times \\mathbb { R } \\text { and } \\text {cont}$$where α := 2 η 2 β -γ &gt; 0 . Note that V ( t, x ) = a ( t ) x 2 + b ( t ) x + c ( t ) is continuous on [0 , T ] × R and continuously differentiable on (0 , T ) × R satisfying the HJB equation, therefore it must be the value function by the uniqueness of solutions to the HJB equation. Then the verification theorem gives rise to the optimal feedback control$$v _ { t } ^ { * * } = & \\frac { x _ { t } - A } { T - t + \\alpha } - \\frac { \\mu } { 4 \\eta } \\left ( ( T - t + \\alpha ) - \\frac { \\alpha ^ { 2 } } { T - t + \\alpha } \\right ) .$$Due to the boundedness of 1 T -t + α and -A T -t + α -µ 4 η ( ( T -t + α ) -α 2 T -t + α ) , v ∗∗ ∈ A , which implies that the optimal control v ∗∗ lies in the admissible set. Moreover, the maximal risk-neutral expected utility is given by$$\\sup _ { v \\in A } \\mathbb { E } \\left [ \\tilde { \\Pi } \\right ] & = \\mathbb { E } \\left [ \\tilde { \\Pi } \\right ] \\Big | _ { v _ { 0 } = v ^ { * } } = V ( 0 , x _ { 0 } ) = a ( 0 ) x _ { 0 } ^ { 2 } + b ( 0 ) x _ { 0 } + c ("
  },
  {
    "hash_code": "344f183a71f2acffaca8bcf05caacf1e",
    "text": "missible set. Moreover, the maximal risk-neutral expected utility is given by$$\\sup _ { v \\in A } \\mathbb { E } \\left [ \\tilde { \\Pi } \\right ] & = \\mathbb { E } \\left [ \\tilde { \\Pi } \\right ] \\Big | _ { v _ { 0 } = v ^ { * } } = V ( 0 , x _ { 0 } ) = a ( 0 ) x _ { 0 } ^ { 2 } + b ( 0 ) x _ { 0 } + c ( 0 ) \\\\ & = \\left ( - \\frac { \\eta } { T + \\alpha } - \\frac { \\gamma } { 2 } \\right ) x _ { 0 } ^ { 2 } + \\left ( \\frac { \\mu } { 2 } ( T + \\alpha ) - \\frac { \\mu \\rho ^ { 2 } } { 2 } \\frac { - 2 A \\eta } { T + \\alpha } + \\gamma A \\right ) x _ { 0 } \\\\ & \\quad - m _ { 0 } ^ { 2 } \\eta \\log \\frac { T + \\alpha } { \\alpha } + \\left ( \\rho \\sigma m _ { 0 } - \\frac { \\gamma } { 2 } m _ { 0 } ^ { 2 } \\right ) T - \\frac { \\mu ^ { 2 } \\alpha ^ { 3 } } { 1 6 \\eta } \\left ( \\frac { \\alpha } { T + \\alpha } + \\frac { 2 ( T + \\alpha ) } { \\alpha } - \\frac { ( T + \\alpha ) ^ { 3 } } { 3 \\alpha ^ { 3 } } - \\frac { 8 } { 3 } \\right ) \\\\ & \\quad - \\frac { A \\eta - \\frac { 1 } { 2 } \\mu A \\alpha ^ { 2 } } { T + \\alpha } - \\frac { \\gamma A ^ { 2 } + \\mu A ( T + \\alpha ) } { 2 } + \\int _ { 0 } ^ { T } \\mu ( A - R _ { s } ) d s < \\infty .$$## A.3 Proof to Theorem 1Note that x t = x 0 -∫ t 0 v s d s + m 0 d Z t , by Itô's formula,$$( x _ { T } - A ) ^ { 2 } = ( x _ { t } - A ) ^ { 2 } + \\int _ { t } ^ { T } \\left ( - 2 ( x _ { s } - A ) v _ { s } + m _ { 0 } ^ { 2 } \\right ) \\, d s + \\int _ { t } ^ { T } \\left ( 2 m _ { 0 } ( x _ { s } - A ) \\right ) d Z _ { s } .$$Furthermore, note that standard Brownian motions W t and Z t have constant correlation ρ , denote standard Brownian motion W 1 t = Z t -ρW t √ 1 -ρ 2 , then W 1 t and W t are independent. By extracting the constants and deterministic parts out of the expectation, the value function is given by$$V ( t , x ) = \\frac { 1 } { \\theta } - \\frac { 1 } { \\theta } \\cdot \\exp \\left ( - \\int _ { t } ^ { T } \\theta \\left ( \\mu ( A - R _ { s } ) + \\rho \\sigma m _ { 0 } - \\beta m _ { 0 } ^ { 2 } \\right ) d s + \\theta \\beta ( x - A ) ^ { 2 } \\right ) \\cdot V _ { 0 } ( t , x ) ,$$where we observe the equivalent optimization problem$$\\text {where we observe this problem on optimization problem} \\\\ V _ { 0 } ( t , x ) & = \\inf _ { v \\in \\mathcal { A } } \\, \\mathbb { E } \\left [ \\exp \\left \\{ \\int _ { t } ^ { T } \\left [ - \\theta \\mu ( x _ { s } - A ) + \\theta \\eta v _ { s } ^ { 2 } - \\theta ( 2 \\beta - \\gamma ) v _ { s } ( x _ { s } - A ) \\right ] d s \\\\ & + \\int _"
  },
  {
    "hash_code": "bdcb66b5fd288de7d74f2fc932bbf113",
    "text": "on optimization problem} \\\\ V _ { 0 } ( t , x ) & = \\inf _ { v \\in \\mathcal { A } } \\, \\mathbb { E } \\left [ \\exp \\left \\{ \\int _ { t } ^ { T } \\left [ - \\theta \\mu ( x _ { s } - A ) + \\theta \\eta v _ { s } ^ { 2 } - \\theta ( 2 \\beta - \\gamma ) v _ { s } ( x _ { s } - A ) \\right ] d s \\\\ & + \\int _ { t } ^ { T } \\left [ ( - \\theta \\sigma - \\theta ( \\gamma - 2 \\beta ) m _ { 0 } \\rho ) \\left ( x _ { s } - A \\right ) - \\theta \\eta m _ { 0 } \\rho v _ { s } + \\theta \\sigma ( R _ { s } - A ) \\right ] d W _ { s } \\\\ & + \\int _ { t } ^ { T } \\left [ - \\theta ( \\gamma - 2 \\beta ) m _ { 0 } \\sqrt { 1 - \\rho ^ { 2 } } ( x _ { s } - A ) - \\theta \\eta m _ { 0 } \\sqrt { 1 - \\rho ^ { 2 } } v \\right ] d W _ { s } \\right \\} \\Big | x _ { t } = x \\right ] . \\\\ = & \\colon \\inf _ { v \\in \\mathcal { A } } \\, \\mathbb { E } \\left [ \\exp \\left \\{ J ( t , T ) \\right \\} | x _ { t } = x \\right ] \\\\ \\intertext { u r s t o c h a t i c r o n t l o b r a g h a s } \\text { our stochastic control problem has a similar form as stochastic linear exponential} \\text { quadratic (SI) EQ} \\text { control}$$Our stochastic control problem has a similar form as stochastic linear exponential quadratic (SLEQ) control problems given in [23] and [24], however, due to the constants and stochastic integrals appearing in our exponential function, these existing results can't be used directly. Instead, we follow the 'completing the square' technique used in [23], which is used to transform the integrand into a square form by introducing a Radon-Nikodym derivative. In this paper, we eliminate the stochastic integrals and 'complete the square' simultaneously by Girsanov's theorem. More precisely, we manage to find some probability measure Q ≪ P such that$$\\mathbb { E } ^ { \\mathbb { P } } \\left [ \\exp \\left \\{ J ( t , T ) \\right \\} | x _ { t } = x \\right ] = \\mathbb { E } ^ { \\mathbb { Q } } \\left [ \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\left \\{ J ( t , T ) \\right \\} \\right | x _ { t } = x \\right ] ,$$where 1 L ( t,T ) · exp { J ( t, T ) } can be transformed into a square form without Itô integrals. Concretely, suppose the Radon-Nikodym derivative is given by$$\\frac { d \\mathbf Q } { d \\mathbf P } \\Big | _ { \\mathcal { F } _ { t } } & = L ( t , T ) \\coloneqq \\exp \\left \\{ - \\int _ { t } ^ { T } \\Psi _ { 1 } ( s , x _ { s } , v _ { s } ) d W _ { s } - \\int _ { t } ^ { T } \\Psi _ { 2 } ( s , x _ { s } , v _ { s } ) d W _ { s } ^ { 1 } \\\\ & - \\frac { 1 } { 2 } \\int _ { t } ^ { T } \\left ( \\Psi _ { 1 } ^ { 2 } ( s , x _ { s } , v _ { s } ) + \\Psi _ { 2 } ^ { 2 } ( s , x _ { s } , v _ { s } ) \\right ) d s \\right \\} ,$$where Ψ 1 and Ψ 2 are L 2 -functions to be determined. At the same time, for some deterministic function G 1 ( t ) and G 2 ( t ) such that G 1 ( T ) ="
  },
  {
    "hash_code": "b67828d7b607aca0d44b41db7354bc46",
    "text": "{ t } ^ { T } \\left ( \\Psi _ { 1 } ^ { 2 } ( s , x _ { s } , v _ { s } ) + \\Psi _ { 2 } ^ { 2 } ( s , x _ { s } , v _ { s } ) \\right ) d s \\right \\} ,$$where Ψ 1 and Ψ 2 are L 2 -functions to be determined. At the same time, for some deterministic function G 1 ( t ) and G 2 ( t ) such that G 1 ( T ) = G 2 ( T ) = 0 , by Ito's lemma,$$- G _ { 1 } ( t ) ( x _ { t } - A ) ^ { 2 } - 2 G _ { 2 } ( t ) ( x _ { t } - A ) \\\\ = & ( G _ { 1 } ( T ) ( x _ { T } - A ) ^ { 2 } + 2 G _ { 2 } ( T ) ( x _ { T } - A ) ) - ( G _ { 1 } ( t ) ( x _ { t } - A ) ^ { 2 } + 2 G _ { 2 } ( t ) ( x _ { t } - A ) ) \\\\ = & \\int _ { t } ^ { T } \\left ( G _ { 1 } ^ { \\prime } ( s ) ( x _ { s } - A ) ^ { 2 } + 2 G _ { 2 } ^ { \\prime } ( s ) ( x _ { s } - A ) - 2 ( G _ { 1 } ( s ) ( x _ { s } - A ) + G _ { 2 } ( s ) ) v _ { s } + m _ { 0 } ^ { 2 } G _ { 1 } ( s ) ) \\, d s \\\\ & + \\int _ { t } ^ { T } 2 \\rho m _ { 0 } ( G _ { 1 } ( s ) ( x _ { s } - A ) + G _ { 2 } ( s ) ) d W _ { s } + \\int _ { t } ^ { T } 2 \\sqrt { 1 - \\rho ^ { 2 } } m _ { 0 } ( G _ { 1 } ( s ) ( x _ { s } - A ) + G _ { 2 } ( s ) ) d W _ { s } ^ { 1 } .$$Hence,$$H e n c , \\\\ \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\{ J ( t , T ) \\} \\\\ = \\exp \\{ J ( t , T ) - \\log L ( t , T ) + ( - G _ { 1 } ( t ) ( x _ { t } - A ) ^ { 2 } - 2 G _ { 2 } ( t ) ( x _ { t } - A ) ) + ( G _ { 1 } ( t ) ( x _ { t } - A ) ^ { 2 } + 2 G _ { 2 } ( t ) ( x _ { t } - A ) ) \\} \\\\ = \\exp \\{ \\int _ { J } \\left [ \\int _ { \\partial t } ^ { t } \\left [ - \\theta \\mu ( x _ { t } - A ) + \\theta \\nu _ { s } ^ { 2 } - \\theta ( 2 ^ { 3 } - \\gamma ) \\nu _ { s } ( x _ { s } - A ) + \\frac { 1 } { 2 } \\Psi _ { 1 } ^ { 2 } ( x _ { s } , x _ { s } ) + \\frac { 1 } { 2 } \\Psi _ { 2 } ^ { 2 } ( x _ { s } , x _ { s } , \\theta _ { s } ) \\right ] \\\\ + ( G _ { 1 } ^ { 1 } ( x _ { s } - A ) ^ { 2 } + 2 G _ { 2 } ^ { \\prime } ( s ) ( x _ { s } - A ) - 2 ( G _ { 1 } ( s ) ( x _ { s } - A ) + G _ { 2 } ( s ) ) v _ { s } + m _ { 0 } ^ { 2 } G _ { 1 } ^ { \\prime } ( ) ] d s \\\\ + \\int _ { t } ^ { T } \\left [ ( - \\theta \\sigma -"
  },
  {
    "hash_code": "df022119787a17cfbfc8d476abd0f445",
    "text": "( x _ { s } - A ) ^ { 2 } + 2 G _ { 2 } ^ { \\prime } ( s ) ( x _ { s } - A ) - 2 ( G _ { 1 } ( s ) ( x _ { s } - A ) + G _ { 2 } ( s ) ) v _ { s } + m _ { 0 } ^ { 2 } G _ { 1 } ^ { \\prime } ( ) ] d s \\\\ + \\int _ { t } ^ { T } \\left [ ( - \\theta \\sigma - \\theta ( \\gamma - 2 ) \\beta ) m _ { 0 } \\rho ( x _ { s } - A ) - \\theta m _ { 0 } \\rho v _ { s } + \\theta \\sigma ( R _ { s } - A ) + \\Psi _ { 1 } ( s , x _ { s } , v _ { s } ) \\\\ + 2 \\rho m _ { 0 } ( G _ { 1 } ( s ) ( x _ { s } - A ) + G _ { 2 } ( s ) ) \\right ] d W _ { s } \\\\ + \\int _ { t } ^ { T } \\left [ ( - \\theta ( \\gamma - 2 ) \\beta ) m _ { 0 } \\sqrt { 1 - \\rho ^ { 2 } } \\beta \\sqrt { ( x _ { s } - A ) } - \\theta \\eta m _ { 0 } \\sqrt { 1 - \\rho ^ { 2 } } v _ { s } + \\Psi _ { 2 } ( s , x _ { s } , v _ { s } ) \\\\ + 2 \\sqrt { 1 - \\rho ^ { 2 } } m _ { 0 } ( G _ { 1 } ( s ) ( x _ { s } - A ) + G _ { 2 } ( s ) ) \\right ] d W _ { s } \\\\ + ( G _ { 1 } ( t ) ( x _ { t } - A ) ^ { 2 } + 2 G _ { 2 } ( t ) ( x _ { t } - A ) ) \\right ]$$By setting$$\\begin{cases} \\Psi _ { 1 } ( s , x _ { s } , v _ { s } ) = ( \\theta \\sigma + m _ { 0 } \\rho ( \\theta ( \\gamma - 2 \\beta ) - 2 G _ { 1 } ( s ) ) ) & ( x _ { s } - A ) + \\quad \\theta \\eta _ { 0 } \\rho v _ { s } - 2 m _ { 0 } G _ { 2 } ( s ) - \\theta \\sigma ( R _ { s } - A ) , \\\\ \\Psi _ { 2 } ( s , x _ { s } , v _ { s } ) = ( \\theta ( \\gamma - 2 \\beta ) - 2 G _ { 1 } ( s ) ) \\, m _ { 0 } \\sqrt { 1 - \\rho ^ { 2 } } & ( x _ { s } - A ) + \\quad \\theta \\eta _ { 0 } \\sqrt { 1 - \\rho ^ { 2 } } v _ { s } - 2 m _ { 0 } \\sqrt { 1 - \\rho ^ { 2 } } G _ { 2 } ( s ) , \\end{cases}$$the stochastic integrals ∫ T t · · · d W s and ∫ T t · · · d W 1 s appear to be zero. Moreover,$$& \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\{ \\mathcal { J } ( t , T ) \\} \\\\ = & \\exp \\left \\{ \\int _ { t } ^ { T } \\left ( A ( s ) v _ { s } ^ { 2 } + B ( s ) ( x _ { s } - A ) v _ { s } + C ( s ) ( x _ { s } - A ) ^ { 2 } + D ( s ) v _ { s } + E ( s ) ( x _ { s } - A ) + F ( s ) \\right ) d t , \\right \\} \\\\ & \\quad + \\left ( G _ { 1 } ( t ) ( x _ { t } - A ) ^ { 2 } + 2 G _ { 2 } ( t ) ( x _ { t } - A ) \\right ) \\left \\{ \\end{"
  },
  {
    "hash_code": "d9cd8c19491b19eaa0d8d9d695f57aba",
    "text": "- A ) v _ { s } + C ( s ) ( x _ { s } - A ) ^ { 2 } + D ( s ) v _ { s } + E ( s ) ( x _ { s } - A ) + F ( s ) \\right ) d t , \\right \\} \\\\ & \\quad + \\left ( G _ { 1 } ( t ) ( x _ { t } - A ) ^ { 2 } + 2 G _ { 2 } ( t ) ( x _ { t } - A ) \\right ) \\left \\{ \\end{array}$$where the coefficients are given by$$\\text {where the coefficient are given by} \\\\ H & \\equiv A ( s ) \\div \\theta + \\frac { 1 } { 2 } \\theta ^ { 2 } \\eta ^ { 2 } m _ { 0 } ^ { 2 } > 0 , \\\\ & \\quad B ( s ) = - \\theta ( 2 \\beta - \\gamma ) - 2 G _ { 1 } ( s ) + m _ { 0 } \\theta ^ { 2 } \\rho + \\sigma \\, \\eta m _ { 0 } ^ { 2 } ( \\theta ( \\gamma - 2 \\beta ) - 2 G _ { 1 } ( s ) ) , \\\\ & \\quad C ( s ) = G _ { 1 } ^ { \\prime } ( s ) + \\frac { 1 } { m _ { 0 } ^ { 2 } } 2 \\left ( \\theta ( \\gamma - 2 \\beta ) - 2 G _ { 1 } ( s ) \\right ) ^ { 2 } + \\frac { 1 } { 2 } \\theta ^ { 2 } \\sigma ^ { 2 } + \\theta \\sigma p m _ { 0 } \\left ( \\theta ( \\gamma - 2 \\beta ) - 2 G _ { 1 } ( s ) \\right ) , \\\\ & \\quad D ( s ) = - 2 G _ { 2 } ( s ) - 2 m _ { 0 } \\theta \\rho G _ { 2 } ^ { \\prime } ( s ) - m _ { 0 } \\theta \\rho ^ { 2 } \\sigma ( R _ { s } - A ) , \\\\ & \\quad E ( s ) = - \\theta + \\mu 2 ^ { \\prime } G _ { 2 } ( s ) - 2 m _ { 0 } ^ { 2 } ( \\theta ( \\gamma - 2 \\beta ) - 2 G _ { 1 } ( s ) ) G _ { 2 } ( s ) - \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) - 2 m _ { 0 } \\theta \\rho G _ { 2 } ( s ) \\\\ & \\quad F ( s ) = m _ { 0 } \\rho \\theta \\sigma ( \\theta ( \\gamma - 2 \\beta ) - 2 G _ { 1 } ( s ) ) ( 1 3 - A ) \\\\ F ( s ) = m _ { 0 } ^ { 2 } G _ { 1 } ( s ) + 2 m _ { 0 } ^ { 2 } G _ { 2 } ^ { 2 } ( s ) + 2 \\rho m _ { 0 } \\theta \\sigma ( R _ { s } - A ) G _ { 2 } ( s ) + \\frac { 1 } { 2 } \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) ^ { 2 } . \\\\ \\intertext { To make sure the integral in ( 14 ) behaves as a square form (plus constants), we rewrite it as a quadratic }$$To make sure the integrand in (14) behaves as a square form (plus constants), we rewrite it as a quadratic function of v s :$$A ( s ) v _ { s } ^ { 2 } + ( B ( s ) ( x _ { s } - A ) + D ( s ) ) \\, v _ { s } + ( C ( s ) ( x _ { s } - A ) ^ { 2 } + E ( s ) ( x _ { s } - A ) + F _ { 0 } ( s ) ) + ( F ( s ) - F _ { 0 } ( s ) ) \\, ,$$of which the discriminant is set to be zero:$$0 = \\Delta = ( B ( s ) ( x _ { s"
  },
  {
    "hash_code": "5fc827def1dc0b5427d3fb1fb5a704f2",
    "text": "} ^ { 2 } + ( B ( s ) ( x _ { s } - A ) + D ( s ) ) \\, v _ { s } + ( C ( s ) ( x _ { s } - A ) ^ { 2 } + E ( s ) ( x _ { s } - A ) + F _ { 0 } ( s ) ) + ( F ( s ) - F _ { 0 } ( s ) ) \\, ,$$of which the discriminant is set to be zero:$$0 = \\Delta = ( B ( s ) ( x _ { s } - A ) + D ( s ) ) ^ { 2 } - 4 A ( s ) \\left ( C ( s ) ( x _ { s } - A ) ^ { 2 } + E ( s ) ( x _ { s } - A ) + F _ { 0 } ( s ) \\right ) ,$$hence,$$\\begin{cases} 0 = & B ^ { 2 } ( s ) - 4 A ( s ) C ( s ) , \\\\ 0 = & B ( s ) D ( s ) - 2 A ( s ) E ( s ) , \\\\ 0 = & D ^ { 2 } ( s ) - 4 A ( s ) F _ { 0 } ( s ) \\implies F _ { 0 } ( s ) = \\frac { D ^ { 2 } ( s ) } { 4 A ( s ) } . \\end{cases}$$Denote b 2 ( s ) := G 1 ( s ) + θ 2 (2 β -γ ) and b 1 ( s ) := 2 G 2 ( s ) , we may solve the ODEs b 2 ( · ) and b 1 ( · ) satisfy:$$b _ { 2 } ^ { \\prime } = \\frac { 1 } { H } ( ( b _ { 2 } + l _ { 3 } ) ^ { 2 } - l _ { 1 } ) , \\ \\ b _ { 2 } ( T ) = \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) ,$$where l 3 = 1 2 m 0 ηθ 2 ρσ and l 1 = θ 2 σ 2 2 H . We may solve that$$b _ { 2 } ( t ) = \\sqrt { l _ { 1 } } \\coth \\left ( A _ { 0 } + \\frac { \\sqrt { l _ { 1 } } } { H } ( T - t ) \\right ) - l _ { 3 } ,$$$$A _ { 0 } \\colon = \\coth ^ { - 1 } \\left ( \\frac { l _ { 3 } + \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) } { \\sqrt { l _ { 1 } } } \\right ) .$$whereMoreover, we have b 1 solves the ODE$$b _ { 1 } ^ { \\prime } = \\theta \\mu + \\frac { 1 } { H } b ( b _ { 2 } + l _ { 3 } ) + \\frac { \\eta \\theta ^ { 2 } \\sigma } { H } ( R _ { t } - A ) \\left ( \\theta \\sigma + \\frac { 1 } { 2 } m _ { 0 } ^ { 2 } \\theta ^ { 2 } \\eta \\sigma ( 1 - \\rho ^ { 2 } ) - \\rho m _ { 0 } b _ { 2 } \\right ) , \\quad b _ { 1 } ( T ) = 0 .$$With b 1 and b 2 solved, we may rewrite the integrand in (14) as$$A ( s ) v _ { s } ^ { 2 } + B ( s ) ( x _ { s } - A ) v _ { s } + C ( s ) ( x _ { s } - A ) ^ { 2 } + D ( s ) v _ { s } + E ( s ) ( x _ { s } - A ) + F ( s ) \\\\ = & H \\left ( v _ { s } - g _ { 1 } ( s ) ( x _ { s } - A ) - g _ { 0 } ( s ) \\right ) ^ { 2 } + \\left ( F ( s ) - \\frac { D ^ { 2 } ( s ) } { 4 H } \\right ) \\\\ = & H \\"
  },
  {
    "hash_code": "12e1052ae8e09d6a3b41827ba8ed3cc7",
    "text": ") ( x _ { s } - A ) ^ { 2 } + D ( s ) v _ { s } + E ( s ) ( x _ { s } - A ) + F ( s ) \\\\ = & H \\left ( v _ { s } - g _ { 1 } ( s ) ( x _ { s } - A ) - g _ { 0 } ( s ) \\right ) ^ { 2 } + \\left ( F ( s ) - \\frac { D ^ { 2 } ( s ) } { 4 H } \\right ) \\\\ = & H \\left ( v _ { s } - g _ { 1 } ( s ) ( x _ { s } - A ) - g _ { 0 } ( s ) \\right ) ^ { 2 } + \\left ( m _ { 0 } ^ { 2 } \\left ( b _ { 2 } - \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) \\right ) + \\frac { \\theta ^ { 2 } \\sigma ^ { 2 } } { 2 } ( R _ { s } - A ) ^ { 2 } - \\frac { 1 } { 4 H } \\left ( b _ { 1 } - \\rho m _ { 0 } \\theta _ { \\sigma } ^ { 2 } \\sigma \\eta ( R _ { s } - A ) \\right ) ^ { 2 } \\right ) ,$$whereDenote we have$$g _ { 1 } ( t ) \\coloneqq \\frac { ( 1 + m _ { 0 } ^ { 2 } \\theta \\eta ) b _ { 2 } - l _ { 3 } } { H } , \\ g _ { 0 } ( t ) \\coloneqq \\frac { ( 1 + \\theta \\eta m _ { 0 } ^ { 2 } ) b _ { 1 } + 2 l _ { 3 } ( R _ { u } - A ) } { 2 H } .$$$$c ( s ) \\colon = m _ { 0 } ^ { 2 } \\left ( b _ { 2 } - \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) \\right ) + \\frac { \\theta ^ { 2 } \\sigma ^ { 2 } } { 2 } ( R _ { s } - A ) ^ { 2 } - \\frac { 1 } { 4 H } \\left ( b _ { 1 } - \\rho m _ { 0 } \\theta ^ { 2 } \\sigma \\eta ( R _ { s } - A ) \\right ) ^ { 2 } , \\ 0 \\leq s \\leq T ,$$$$& \\quad \\text {we have} & \\mathbb { E } ^ { \\mathbb { P } } [ \\exp \\{ J ( t , T ) \\} | x _ { t } = x ] = \\mathbb { E } ^ { \\mathbb { Q } } \\left [ \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\{ J ( t , T ) \\} \\Big | x _ { t } = x \\right ] \\\\ & \\quad = \\mathbb { E } ^ { \\mathbb { Q } } \\Big [ \\exp \\left \\{ \\int _ { t } ^ { T } H ( v _ { s } - g _ { 1 } ( s ) ( x _ { s } - A ) - g _ { 0 } ( s ) ) ^ { 2 } d s \\right \\} \\\\ & \\quad \\cdot \\exp \\left \\{ \\left ( b _ { 2 } ( t ) - \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) \\right ) ( x _ { t } - A ) ^ { 2 } + b _ { 1 } ( t ) ( x _ { t } - A ) + \\int _ { t } ^ { T } c ( s ) d s \\right \\} \\Big | x _ { t } = x \\right ] \\\\ & \\quad = \\mathbb { E } ^ { \\mathbb { Q } } \\Big [ \\exp \\left \\{ \\int _ { t } ^ { T } H ( v _ { s } - g _ { 1 } ( s ) ( x _ { s } - A ) - g _ { 0 } ( s ) ) ^ { 2 } d s \\right \\} \\Big | x _ { t } = x"
  },
  {
    "hash_code": "3f92ba740314c69c28c9c7f2219f297b",
    "text": "_ { t } ^ { T } c ( s ) d s \\right \\} \\Big | x _ { t } = x \\right ] \\\\ & \\quad = \\mathbb { E } ^ { \\mathbb { Q } } \\Big [ \\exp \\left \\{ \\int _ { t } ^ { T } H ( v _ { s } - g _ { 1 } ( s ) ( x _ { s } - A ) - g _ { 0 } ( s ) ) ^ { 2 } d s \\right \\} \\Big | x _ { t } = x \\right ] \\\\ & \\quad \\cdot \\exp \\left \\{ \\left ( b _ { 2 } ( t ) - \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) \\right ) ( x - A ) ^ { 2 } + b _ { 1 } ( t ) ( x - A ) + \\int _ { t } ^ { T } c ( s ) d s \\right \\} \\\\ & \\quad \\geq \\exp \\left \\{ \\left ( b _ { 2 } ( t ) - \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) \\right ) ( x - A ) ^ { 2 } + b _ { 1 } ( t ) ( x - A ) + \\int _ { t } ^ { T } c ( s ) d s \\right \\} , \\\\ \\intertext { where the equation holds when } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext"
  },
  {
    "hash_code": "e90f70b01af5a4593c1042c9b9d674b4",
    "text": "{ o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext"
  },
  {
    "hash_code": "e90f70b01af5a4593c1042c9b9d674b4",
    "text": "{ o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext"
  },
  {
    "hash_code": "cc210a142b371e85d677a953eac16d52",
    "text": "{ o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } &"
  },
  {
    "hash_code": "ef9c59482acc4734815ee69d6e5993a6",
    "text": "{ o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertextuser { \\intertext { o n t i n g u s } } \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertextuser { \\intertext { o n t i n g u s } } \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext"
  },
  {
    "hash_code": "e90f70b01af5a4593c1042c9b9d674b4",
    "text": "{ o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext"
  },
  {
    "hash_code": "e90f70b01af5a4593c1042c9b9d674b4",
    "text": "{ o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext"
  },
  {
    "hash_code": "4931b82f8395eb3b46b020e0e5a2f9fc",
    "text": "{ o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s } & \\quad \\intertext { o n t i n g u s$$where the equation holds when v s = v ∗ s := g 1 ( s )( x s -A ) + g 0 ( s ) . Note that g 1 and g 0 are bounded and continuous on [0 , T ] , so v ∗ ∈ A t , hence,$$V _ { 0 } ( t , x ) & = \\inf _ { v \\in \\mathbb { A } } \\mathbb { E } ^ { \\mathbb { P } } \\left [ \\exp \\left \\{ J ( t , T ) \\right \\} | x _ { t } = x \\right ] = \\inf _ { v \\in \\mathbb { A } } \\mathbb { Q } \\left [ \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\left \\{ J ( t , T ) \\right \\} \\right | x _ { t } = x \\right ] \\\\ = & \\mathbb { Q } \\left [ \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\left \\{ J ( t , T ) \\right \\} \\right | x _ { t } = x \\right ]"
  },
  {
    "hash_code": "cc226d4f8fc1c2552b5fc2b007ffaf77",
    "text": "right ] = \\inf _ { v \\in \\mathbb { A } } \\mathbb { Q } \\left [ \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\left \\{ J ( t , T ) \\right \\} \\right | x _ { t } = x \\right ] \\\\ = & \\mathbb { Q } \\left [ \\frac { 1 } { L ( t , T ) } \\cdot \\exp \\left \\{ J ( t , T ) \\right \\} \\right | x _ { t } = x \\right ] \\Big | \\Big | _ { v = v ^ { * } } \\\\ = & \\exp \\left \\{ \\left \\{ \\left ( b _ { 2 } ( t ) - \\frac { \\theta } { 2 } ( 2 \\beta - \\gamma ) \\right ) ( x - A ) ^ { 2 } + b _ { 1 } ( t ) ( x - A ) + \\int _ { t } ^ { T } c ( s ) d s \\right \\} .$$Furthermore, the value function is given by$$\\text {For more} ; \\text { the value function is given by} \\\\ V ( t , x ) = & \\frac { 1 } { \\theta } - \\frac { 1 } { \\theta } \\cdot \\exp \\left \\{ - \\int _ { t } ^ { T } \\theta \\left ( \\mu ( A - R _ { s } ) + \\rho \\sigma m _ { 0 } - \\beta m _ { 0 } ^ { 2 } \\right ) d s + \\theta \\beta ( x - A ) ^ { 2 } \\right \\} V _ { 0 } ( t , x ) \\\\ = & \\frac { 1 } { \\theta } - \\frac { 1 } { \\theta } \\cdot \\exp \\left \\{ \\left ( b _ { 2 } ( t ) + \\frac { \\theta \\gamma } { 2 } \\right ) ( x - A ) ^ { 2 } + b _ { 1 } ( t ) ( x - A ) \\\\ & + \\int _ { t } ^ { T } \\left [ c ( s ) - \\theta \\mu ( A - R _ { s } ) - \\theta \\rho \\sigma m _ { 0 } + \\theta \\beta m _ { 0 } ^ { 2 } \\right ] d s \\right \\} . \\\\ = & \\frac { 1 } { \\theta } \\left ( 1 - \\exp \\left \\{ \\left ( b _ { 2 } ( t ) + \\frac { \\theta \\gamma } { 2 } \\right ) ( x - A ) ^ { 2 } + b _ { 1 } ( t ) ( x - A ) + b _ { 0 } ( t ) \\right \\} \\right ) , \\\\ \\intertext { w h e r e } \\text {where}$$where$$b _ { 0 } ( t ) & \\colon = \\int _ { t } ^ { T } \\left [ c ( s ) - \\theta \\mu ( A - R _ { s } ) - \\theta \\rho \\sigma m _ { 0 } + \\theta \\beta m _ { 0 } ^ { 2 } \\right ] d s \\\\ & = \\int _ { t } ^ { T } \\left [ \\frac { l _ { 1 } - l _ { 3 } ^ { 2 } } { H } ( R _ { s } - A ) ^ { 2 } + \\left ( \\theta \\mu + \\frac { l _ { 3 } } { H } b _ { 1 } \\right ) ( R _ { s } - A ) + \\left ( - \\frac { b _ { 1 } ^ { 2 } } { 4 H } + m _ { 0 } ^ { 2 } \\left ( b _ { 2 } + \\frac { \\theta \\gamma } { 2 } \\right ) - \\rho m _ { 0 } \\theta \\sigma \\right ) \\right ] d s .$$In summary, the optimal feedback control is given by$$x = y$$$$v _ { t } ^ { * } = & g _ { 1 } ( t ) ( x _ { t } - A ) + g _ { 0 } ( t ) \\\\ = & \\frac { ( 1 + m _ { 0 } ^ { 2 } \\theta ) \\eta _ { 2 } - l _ { 3 } } {"
  },
  {
    "hash_code": "1a3fd1f01d5add3ebc3c19fcce083776",
    "text": "{ 2 } \\right ) - \\rho m _ { 0 } \\theta \\sigma \\right ) \\right ] d s .$$In summary, the optimal feedback control is given by$$x = y$$$$v _ { t } ^ { * } = & g _ { 1 } ( t ) ( x _ { t } - A ) + g _ { 0 } ( t ) \\\\ = & \\frac { ( 1 + m _ { 0 } ^ { 2 } \\theta ) \\eta _ { 2 } - l _ { 3 } } { H } \\cdot ( x _ { t } - A ) + \\frac { l _ { 3 } } { H } \\cdot ( R _ { t } - A ) + \\frac { ( 1 + m _ { 0 } ^ { 2 } \\theta ) \\eta _ { 1 } } { 2 H } . \\\\$$## A.4 Proof to Theorem 2When m ( v ) = m 0 = 0 and β = 0 , d x t = -v t d t , according to Theorem 1, H = θη , l 3 = 0 , l 1 = θ 3 σ 2 η 2 , A 0 = 0 , and$$b _ { 2 } ( t ) = \\sqrt { \\frac { \\theta ^ { 3 } \\sigma ^ { 2 } \\eta } { 2 } } \\cdot \\coth \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) .$$Furthermore, b 1 solves the backward ODE$$b _ { 1 } ^ { \\prime } = \\theta \\mu + \\frac { 1 } { H } b _ { 1 } b _ { 2 } + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { t } - A ) , \\ b _ { 1 } ( T ) = 0 .$$Note that$$\\left ( b _ { 1 } \\cdot \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) \\right ) ^ { \\prime } & = \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) \\cdot b _ { 1 } ^ { \\prime } - b _ { 1 } \\cdot \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } \\cdot \\cosh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) \\\\ & = \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) \\cdot \\left ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { t } - A ) \\right ) .$$Integrate on both sides,$$- b _ { 1 } ( t ) \\cdot \\sinh \\left ( \\sqrt { - \\frac { \\theta \\sigma ^ { 2 } } { 2 } } ( T - t ) \\right ) = & \\int _ { t } ^ { T } \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) \\cdot ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) \\, d s . \\\\ & \\Rightarrow b _ { 1 } ( t ) = - \\, \\frac { 1 } { \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) } \\cdot \\int _ { t } ^ { T } \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - s ) \\right ) \\cdot ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) \\, d s .$$Note that now hence,$$\\left ( \\frac {"
  },
  {
    "hash_code": "18742f0c330f1b3ff0fe2e7b1e8ca2cc",
    "text": "\\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) } \\cdot \\int _ { t } ^ { T } \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - s ) \\right ) \\cdot ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) \\, d s .$$Note that now hence,$$\\left ( \\frac { x _ { t } - A } { \\sinh \\left ( \\sqrt { \\frac { \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) } \\right ) ^ { \\prime } = & \\frac { ( x _ { t } - A ) ^ { \\prime } \\cdot \\sinh \\left ( \\sqrt { \\frac { \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) + ( x _ { t } - A ) \\cdot \\sqrt { \\frac { \\sigma ^ { 2 } } { 2 \\eta } } \\cdot \\cosh \\left ( \\sqrt { \\frac { \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) } { \\sinh ^ { 2 } \\left ( \\sqrt { \\frac { \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) } \\\\ = & \\frac { T } { 2 H \\sinh ^ { 2 } \\left ( \\sqrt { \\frac { \\sigma ^ { 2 } } { 2 \\eta } } ( T - t ) \\right ) } .$$Integrate on both sides then integrate by parts, we have$$\\begin{array} { r l } & { \\frac { x _ { t } - A } { \\sinh \\left ( \\sqrt { \\frac { \\partial } { 2 n } } ( T - t ) \\right ) } - \\frac { x _ { 0 } - A } { \\sinh \\left ( \\sqrt { \\frac { \\partial } { 2 n } } T \\right ) } = \\int _ { 0 } ^ { T } \\frac { \\sinh \\left ( \\sqrt { \\frac { \\partial } { 2 n } } ( T - u ) \\right ) } { 2 H \\sinh ^ { 2 } \\left ( \\sqrt { \\frac { \\partial } { 2 n } } ( T - s ) \\right ) } \\cdot ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { u } - A ) \\, d u s } \\\\ & { = \\frac { 1 } { \\sinh \\left ( \\sqrt { \\frac { \\partial } { 2 n } } T \\right ) } \\int _ { 0 } ^ { t } \\frac { 1 } { \\sqrt { 2 3 \\sigma ^ { 2 } } \\eta } \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 n } } s \\right ) \\cdot ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) \\, d s + \\frac { \\sinh \\left ( \\sqrt { \\frac { \\partial } { 2 n } } t \\right ) } { \\sinh \\left ( \\sqrt { \\frac { \\partial } { 2 n } } T \\right ) } \\cdot \\frac { 1 } { \\sinh \\left ( \\sqrt { \\frac { \\partial } { 2 n } } ( T - t ) \\right ) } } \\\\ & { \\cdot \\int _ { t } ^ { T } \\frac { 1 } { \\sqrt { 2 3 \\sigma ^ { 2 } } \\eta } \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - s ) \\right ) \\cdot ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) \\, d s } . } \\end{array}$$Denote we can solve that$$x _ { t } = & \\frac { \\kappa } { \\sinh \\kappa T }"
  },
  {
    "hash_code": "546e88235947378d12e375d0e2c5bebd",
    "text": "2 3 \\sigma ^ { 2 } } \\eta } \\sinh \\left ( \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } ( T - s ) \\right ) \\cdot ( \\theta \\mu + \\theta ^ { 2 } \\sigma ^ { 2 } ( R _ { s } - A ) \\, d s } . } \\end{array}$$Denote we can solve that$$x _ { t } = & \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( R _ { s } \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) \\text {d} s \\\\ & + \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + A + \\left ( - A + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } .$$Furthermore,$$x _ { t } = & \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + \\left ( A - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa t } { \\sinh \\kappa T } \\\\ & + \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } \\cdot \\int _ { 0 } ^ { t } \\kappa \\sinh \\kappa R _ { \\ } d s + \\frac { \\sinh \\kappa t } { \\sinh \\kappa T } \\cdot \\int _ { t } ^ { t } \\kappa \\sinh \\kappa ( T - S ) R _ { \\ } d s \\\\ = & \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } \\cdot \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + \\int _ { 0 } ^ { t } \\kappa \\sinh \\kappa R _ { \\ } d s \\right ) + \\frac { \\sinh \\kappa } { \\sinh \\kappa T } \\cdot \\left ( A - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + \\int _ { t } ^ { T } \\kappa \\sinh \\kappa ( T - s ) R _ { \\ } d s \\right )$$$$\\frac { \\theta ^ { 2 } \\sigma ^ { 2 } } { \\sqrt { 2 \\theta ^ { 3 } \\sigma ^ { 2 } \\eta } } = \\sqrt { \\frac { \\theta \\sigma ^ { 2 } } { 2 \\eta } } = \\colon \\kappa ,$$$$v _ { t } = - x _ { t } ^ { \\prime } = - ( x _ { t } - A ) ^ { \\prime } = \\frac { b _ { 2 } } { H } ( x _ { t } - A ) + \\frac { b _ { 1 } } { 2 H } ,$$## A.5 Proof to Lemma 1$$\\begin{array} { r l } & { A . 5 } & { \\text {Proof to Lemma } 1 } \\\\ & { \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) d s } \\\\ & { = \\frac { \\kappa } { \\sinh \\kappa T } \\"
  },
  {
    "hash_code": "815c11cb28925ea496b97ab7346c0820",
    "text": "to Lemma 1$$\\begin{array} { r l } & { A . 5 } & { \\text {Proof to Lemma } 1 } \\\\ & { \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) d s } \\\\ & { = \\frac { \\kappa } { \\sinh \\kappa T } \\cdot \\int _ { 0 } ^ { t } \\sinh \\kappa s d s \\cdot \\sinh ( \\kappa ( T - t ) ) + \\frac { \\kappa } { \\sinh \\kappa T } \\cdot \\int _ { t } ^ { T } \\sinh \\kappa ( T - s ) d s \\cdot \\sinh \\kappa t } \\\\ & { = \\frac { \\sinh ( \\kappa ( T - t ) ) } { \\sinh \\kappa T } \\cdot ( \\cosh \\kappa t - 1 ) + \\frac { \\sinh \\kappa t } { \\sinh \\kappa T } \\cdot ( \\cosh \\kappa ( T - t ) - 1 ) } \\\\ & { = \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } - \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } = T C _ { t } - I S _ { t } . } \\end{array}$$## A.6 Proof to Proposition 2By Theorem 2 and Lemma 1,$$x _ { t } = & \\frac { \\kappa } { \\sinh \\kappa T } , \\int _ { 0 } ^ { T } \\left ( R \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) \\cos \\\\ & + \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + A + \\left ( - A + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } \\\\ = & \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } - R \\right ) \\cdot IS _ { t } + \\left ( - A + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } + R \\right ) \\cdot T C _ { t } + A .$$## A.7 Proof to Proposition 3Note that R + µ θσ 2 appears as a whole, so WLOG, we may assume that µ ≡ 0 . When the reference strategy is R , the optimal strategy is given by$$x _ { t } = A + ( R - A ) \\cdot \\mathbb { T } C _ { t } + ( x _ { 0 } - R ) \\cdot \\mathbb { S } _ { t } ,$$$$v _ { t } & = - x _ { t } ^ { \\prime } = - \\left ( R - A \\right ) \\cdot T C _ { t } ^ { \\prime } - ( x _ { 0 } - R ) \\cdot \\mathbb { I } _ { t } ^ { \\prime } \\\\ & = - \\left ( R - A \\right ) \\cdot \\frac { - \\kappa \\cosh \\kappa t } { \\sinh \\kappa T } - ( x _ { 0 } - R ) \\cdot \\frac { - \\kappa \\cosh \\kappa ( T - t ) } { \\sinh \\kappa T } ,$$note that 0 ≤ t ≤ T , then cosh κt ≥ 0 and cosh κ ( T -t ) ≥ 0 , so when A ≤ R ≤ x 0 , v t ≥ 0 , which represents that the optimal strategy does not contain overshooting. Note that$$x _ { t } ^ { \\prime \\prime } = - v _ { t } ^ {"
  },
  {
    "hash_code": "7b5522846239b932696e848f92cab619",
    "text": "} - ( x _ { 0 } - R ) \\cdot \\frac { - \\kappa \\cosh \\kappa ( T - t ) } { \\sinh \\kappa T } ,$$note that 0 ≤ t ≤ T , then cosh κt ≥ 0 and cosh κ ( T -t ) ≥ 0 , so when A ≤ R ≤ x 0 , v t ≥ 0 , which represents that the optimal strategy does not contain overshooting. Note that$$x _ { t } ^ { \\prime \\prime } = - v _ { t } ^ { \\prime } = ( R - A ) \\cdot \\frac { - \\kappa ^ { 2 } \\sinh \\kappa t } { \\sinh \\kappa T } + ( x _ { 0 } - R ) \\cdot \\frac { \\kappa ^ { 2 } \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } ,$$so when R &gt; x 0 &gt; A , x ′′ t = -v ′ t ≤ 0 , which implies that the optimal strategy is a concave function, so$$\\min _ { 0 \\leq t \\leq T } v _ { t } & = v _ { 0 } = - ( R - A ) \\cdot \\frac { - \\kappa } { \\sinh \\kappa T } - ( x _ { 0 } - R ) \\cdot \\frac { - \\kappa \\cosh \\kappa T } { \\sinh \\kappa T } \\\\ & = \\frac { \\kappa } { \\sinh \\kappa T } \\cdot [ ( 1 - \\cosh \\kappa T ) R + ( x _ { 0 } \\cosh \\kappa T - A ) ] \\, ,$$then□□□so when so when$$R > \\frac { x _ { 0 } \\cosh \\kappa T - A } { \\cosh \\kappa T - 1 } = x _ { 0 } + \\frac { 1 } { \\cosh \\kappa T - 1 } \\cdot ( x _ { 0 } - A ) ,$$the optimal strategy contains overshooting, and when x 0 &lt; R ≤ x 0 + 1 cosh κT -1 · ( x 0 -A ) , the optimal strategy does not contains overshooting. Similarly, when R &lt; A &lt; x 0 , q ′′ t = -v ′ t ≥ 0 , which implies that the optimal strategy is a convex function, so$$\\min _ { 0 \\leq t \\leq T } v _ { t } & = v _ { T } = - ( R - A ) \\cdot \\frac { - \\kappa \\cosh \\kappa T } { \\sinh \\kappa T } - ( x _ { 0 } - R ) \\cdot \\frac { - \\kappa } { \\sinh \\kappa T } \\\\ & = \\frac { \\kappa } { \\sinh \\kappa T } \\cdot [ ( \\cosh \\kappa T - 1 ) R - ( x _ { 0 } - A \\cosh \\kappa T ) ] \\, ,$$$$R < \\frac { A \\cosh \\kappa T - x _ { 0 } } { \\cosh \\kappa T - 1 } = A - \\frac { 1 } { \\cosh \\kappa T - 1 } \\cdot ( x _ { 0 } - A ) ,$$the optimal strategy contains overshooting.## A.8 Proof to Proposition 4By the results given in Theorem 2, the optimal strategy is a deterministic one. In other words, when µ = m 0 = 0 ,$$\\sup _ { v \\in \\mathcal { A } } \\mathbb { E } \\left [ u \\left ( \\tilde { \\Pi } \\right ) \\right ] = \\sup _ { v \\in \\mathcal { A } _ { d e t } } \\mathbb { E } \\left [ u \\left ( \\tilde { \\Pi } \\right ) \\right ] ,$$where A det = { v ∈ A : v is deterministic } . When v ∈ A det , x t = x 0 -∫ t 0 v s d s is also deterministic, hence,$$\\mathbb { E } \\left [ u \\left ( \\tilde { \\Pi } \\right ) \\right ] & = \\math"
  },
  {
    "hash_code": "d4f6acb1bafaa93398f78881eaf0d715",
    "text": "right ) \\right ] = \\sup _ { v \\in \\mathcal { A } _ { d e t } } \\mathbb { E } \\left [ u \\left ( \\tilde { \\Pi } \\right ) \\right ] ,$$where A det = { v ∈ A : v is deterministic } . When v ∈ A det , x t = x 0 -∫ t 0 v s d s is also deterministic, hence,$$\\mathbb { E } \\left [ u \\left ( \\tilde { \\Pi } \\right ) \\right ] & = \\mathbb { E } \\left [ \\frac { 1 } { 6 } \\left ( 1 - \\exp \\left \\{ \\theta \\beta ( x _ { T } - A ) ^ { 2 } - \\int _ { 0 } ^ { T } \\theta \\left ( - \\eta v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) d t - \\int _ { 0 } ^ { T } \\theta \\sigma ( x _ { t } - R _ { t } ) d W _ { t } \\right \\} \\right ) \\right ] \\\\ & = \\frac { 1 } { \\theta } - \\frac { 1 } { \\theta } \\cdot \\exp \\left \\{ \\theta \\beta ( x _ { T } - A ) ^ { 2 } - \\int _ { 0 } ^ { T } \\left [ \\theta \\left ( - \\eta v _ { t } ^ { 2 } - \\gamma v _ { t } ( x _ { t } - A ) \\right ) - \\frac { 1 } { 2 } \\theta ^ { 2 } \\sigma ^ { 2 } ( x _ { t } - R _ { t } ) ^ { 2 } \\right ] d t \\right \\} .$$Subject to x T = A (or β → + ∞ ) , ∫ T 0 v t ( x t -A )d t = 1 2 ( x 0 -A ) 2 is a constant, hence, maximizing the expected utility is equivalent to a typical variational problem$$\\begin{cases} \\inf _ { v \\in \\mathcal { A } _ { d e t } } \\int _ { 0 } ^ { T } \\left [ \\eta v _ { t } ^ { 2 } + \\frac { \\theta \\sigma ^ { 2 } } { 2 } ( x _ { t } - R _ { t } ) ^ { 2 } \\right ] d t \\\\ \\quad s . t . \\ x _ { t } = x _ { 0 } - \\int _ { 0 } ^ { t } v _ { s } d s , \\ x _ { T } = A , \\end{cases}$$Note that the objective can be divided into n parts, each with a constant RS:$$\\int _ { 0 } ^ { T } \\left ( \\eta v _ { t } ^ { 2 } + \\frac { \\gamma } { 2 } \\sigma ^ { 2 } ( x _ { t } - R _ { t } ) ^ { 2 } \\right ) d t = \\sum _ { k = 1 } ^ { n } \\int _ { \\frac { ( k - 1 ) T } { n } } ^ { \\frac { k T } { n } } \\left ( \\eta v _ { t } ^ { 2 } + \\frac { \\gamma } { 2 } \\sigma ^ { 2 } \\left ( x _ { t } - R ^ { ( k ) } \\right ) ^ { 2 } \\right ) d t .$$We divide the optimization problem into three steps:- Find the optimal value of x ∗ kT n , 1 ≤ k ≤ n -1 . We can directly use Theorem 2: suppose the optimal□strategy is x ∗ t , note that µ = 0 ,$$x _ { t } ^ { * } = & \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( R _ { s } \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) d s \\\\ & + x _ { 0 } \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + A - A \\"
  },
  {
    "hash_code": "f382169169b705eb6ad1d74d880c041d",
    "text": "0 ,$$x _ { t } ^ { * } = & \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( R _ { s } \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) d s \\\\ & + x _ { 0 } \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + A - A \\cdot \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } ,$$where R is the given piece-wise constant RS. For 1 ≤ k ≤ n -1 ,$$where R is the given piece-wise constant RS. For 1 \\leq k \\leq n - 1 , \\\\ \\times _ { k \\mathbb { T } } ^ { \\ast } = & \\sinh \\kappa ( T - \\frac { k T } { n } ) + A \\cdot \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } + \\frac { \\sinh \\kappa \\left ( T - \\frac { k T } { n } \\right ) } { \\sinh \\kappa T } \\cdot \\sum _ { i = 1 } ^ { k } R ^ { ( i ) } \\cdot \\int _ { ( i - 1 ) T } ^ { \\frac { k T } { n } } \\sinh \\kappa T \\\\ & + \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } \\cdot \\sum _ { i = k + 1 } ^ { n } R ^ { ( i ) } \\cdot \\int _ { ( i - 1 ) T } ^ { \\frac { k T } { n } } \\sinh \\left ( \\kappa ( T - s ) \\right ) d s \\\\ = & \\sinh \\kappa ( T - \\frac { k T } { n } ) + A \\cdot \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } \\\\ & + \\frac { \\sinh \\kappa ( T - \\frac { k T } { n } ) } { \\sinh \\kappa T } \\cdot \\sum _ { i = 1 } ^ { k } R ^ { ( i ) } \\cdot \\left ( \\cosh \\left ( \\kappa \\frac { i T } { n } \\right ) - \\cosh \\left ( \\kappa \\frac { ( i - 1 ) T } { n } \\right ) \\right ) \\\\ & + \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } \\cdot \\sum _ { i = k + 1 } ^ { n } R ^ { ( i ) } \\cdot \\left ( \\cosh \\left ( \\kappa \\frac { ( n - i + 1 ) T } { n } \\right ) - \\cosh \\left ( \\kappa \\frac { ( n - i ) T } { n } \\right ) \\right ) . \\\\ \\\\ \\text {Denote } R ^ { ( 0 ) } \\colon = & \\, x _ { 0 } , R ^ { ( n + 1 ) } \\colon = A , \\, \\text {and}$$Denote R (0) := x 0 , R ( n +1) := A , and$$b _ { i } = \\begin{cases} 1 , & i = 0 , \\\\ \\cosh \\left ( \\kappa \\frac { i T } { n } \\right ) - \\cosh \\left ( \\kappa \\frac { ( i - 1 ) T } { n } \\right ) , & 1 \\leq i \\leq n - 1 , \\end{cases}$$we can rewrite$$x _ { \\frac { k T } { n } } ^ { * } = \\frac { \\sinh \\zeta ( T - \\frac { k T } { n } ) } { \\sinh \\kappa T } \\cdot \\sum _ { i = 0 } ^ { k } b _ { i } R ^ { ( i ) } + \\frac { \\sinh \\kappa \\frac { k T } { n } }"
  },
  {
    "hash_code": "9388c1028bb22606a42eb195c04a3862",
    "text": "\\right ) , & 1 \\leq i \\leq n - 1 , \\end{cases}$$we can rewrite$$x _ { \\frac { k T } { n } } ^ { * } = \\frac { \\sinh \\zeta ( T - \\frac { k T } { n } ) } { \\sinh \\kappa T } \\cdot \\sum _ { i = 0 } ^ { k } b _ { i } R ^ { ( i ) } + \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } \\cdot \\sum _ { i = k + 1 } ^ { n + 1 } b _ { n - i + 1 } R ^ { ( i ) } , \\ 1 \\leq k \\leq n - 1 .$$Furthermore, we denote the optimal values a k := x ∗ kT n , 1 ≤ k ≤ n -1 . Note that the sum of coefficients in a k is$$\\frac { \\sinh \\kappa \\left ( T - \\frac { k T } { n } \\right ) } { \\sinh \\kappa T } \\cdot \\sum _ { i = 0 } ^ { k } b _ { i } + \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } \\cdot \\sum _ { i = k + 1 } ^ { n + 1 } b _ { n - i + 1 } \\\\ = & \\frac { \\sinh \\kappa \\left ( T - \\frac { k T } { n } \\right ) } { \\sinh \\kappa T } \\cdot \\cosh \\kappa \\frac { k T } { n } + \\frac { \\sinh \\kappa \\frac { k T } { n } } { \\sinh \\kappa T } \\cdot \\cosh \\kappa \\frac { ( n - k ) T } { n } = 1 ,$$so a k is a weighted average of R ( i ) 's.- Note that we already know the value of optimal strategy x kT n , so the optimization problem is equivalent to the following optimization problem with extra constraints:$$\\inf _ { \\substack { v \\in A _ { \\det } \\\\ s . t . \\ x _ { t } = x _ { 0 } - \\int _ { 0 } ^ { T } v _ { s } d s , \\ x _ { T } = A , \\\\ x _ { \\frac { k T } { n } } = a _ { k } , \\ 1 \\leq k \\leq n - 1 . } } \\frac { t } { n } \\inf _ { \\substack { v \\in A _ { \\det } \\\\ v _ { s } = 1 } } \\sum _ { \\substack { ( \\frac { k T } { n } - 1 ) T } } ^ { n } \\int _ { \\frac { k T } { n } } ^ { \\frac { k T } { n } } \\left ( \\eta v _ { t } ^ { 2 } + \\frac { \\gamma } { 2 } \\sigma ^ { 2 } \\left ( x _ { t } - R ^ { ( k ) } \\right ) ^ { 2 } \\right ) \\\\ \\intertext { f o r } x _ { t } = x _ { 0 } - \\int _ { 0 } ^ { T } v _ { s } d s , \\ x _ { T } = A , \\\\ x _ { \\frac { k T } { n } } = a _ { k } , \\ 1 \\leq k \\leq n - 1 . }$$Denote a 0 = x 0 and a n = A , inheriting from the spirit of dynamic programming, we can further divide the optimization problem into n parts: for 1 ≤ k ≤ n , where the k -th sub-problem is given by$$\\begin{cases} \\inf _ { v \\in \\mathcal { A } _ { \\det } } \\int _ { \\Omega _ { n } ^ { T } } ^ { \\frac { k _ { T } } { n } } \\left ( \\eta v _ { t } ^ { 2 } + \\frac { \\theta \\sigma ^ { 2 } } { 2 } \\left ( x _ { t } - R ^ { ( k ) } \\right ) ^ { 2 } \\right ) \\mathrm d t \\\\ s . t"
  },
  {
    "hash_code": "d4ce4dcae4b331b3d187c6f2be990ab2",
    "text": "blem is given by$$\\begin{cases} \\inf _ { v \\in \\mathcal { A } _ { \\det } } \\int _ { \\Omega _ { n } ^ { T } } ^ { \\frac { k _ { T } } { n } } \\left ( \\eta v _ { t } ^ { 2 } + \\frac { \\theta \\sigma ^ { 2 } } { 2 } \\left ( x _ { t } - R ^ { ( k ) } \\right ) ^ { 2 } \\right ) \\mathrm d t \\\\ s . t . \\ x _ { t } = a _ { k - 1 } - \\int _ { \\frac { ( k - 1 ) T } { n } } ^ { t } v _ { \\ } d s , \\, \\frac { ( k - 1 ) T } { n } \\leq t \\leq \\frac { k T } { n } , \\, x _ { \\frac { k T } { n } } = a _ { k } , \\end{cases}$$which is a constant RS problem with time period [ ( k -1) T n , kT n ] , of which the optimal strategy is given by$$x _ { t } ^ { ( k ) } = a _ { k } + \\left ( R ^ { ( k ) } - a _ { k } \\right ) \\, \\text {TC} _ { t - \\frac { ( k - 1 ) T } { n } } + \\left ( a _ { k - 1 } - R ^ { ( k ) } \\right ) \\, \\text {IS} _ { t - \\frac { ( k - 1 ) T } { n } } , \\frac { ( k - 1 ) T } { n } \\leq t \\leq \\frac { k T } { n } ,$$where κ := √ θσ 2 2 η and$$IS _ { t } \\coloneqq \\frac { \\sinh \\kappa \\left ( \\frac { T } { n } - t \\right ) } { \\sinh \\kappa \\frac { T } { n } } , \\ \\ T C _ { t } \\coloneqq \\frac { \\sinh \\kappa \\frac { T } { n } - \\sinh \\kappa t } { \\sinh \\kappa \\frac { T } { n } } .$$- Connect n optimal strategies to obtain the optimal strategy of the original optimization problem. Denote$$x _ { t } ^ { * * } = \\sum _ { k = 1 } ^ { n } x _ { t } ^ { ( k ) } \\, \\mathbb { 1 } _ { \\{ \\frac { ( k - 1 ) T } { n } \\leq t < \\frac { k T } { n } \\} } + A \\, \\mathbb { 1 } _ { \\{ t = T \\} } ,$$then by the results of constant reference strategies, for any strategy x t ,$$\\int _ { 0 } ^ { T } \\left [ \\eta v _ { t } ^ { 2 } + \\frac { \\theta \\sigma ^ { 2 } } { 2 } ( x _ { t } - R _ { t } ) ^ { 2 } \\right ] d t & = \\sum _ { k = 1 } ^ { n } \\int _ { \\frac { ( k - n ) T } { n } } ^ { \\frac { K T } { n } T } \\left ( \\eta v _ { t } ^ { 2 } + \\frac { \\gamma } { 2 } \\sigma ^ { 2 } \\left ( x _ { t } - R ^ { ( k ) } \\right ) ^ { 2 } \\right ) \\\\ & \\geq \\sum _ { k = 1 } ^ { n } \\int _ { \\frac { ( k - n ) T } { n } } ^ { \\frac { K T } { n } T } \\left ( \\eta \\left ( - x _ { t } ^ { ( k ) } \\right ) ^ { 2 } + \\frac { \\gamma } { 2 } \\sigma ^ { 2 } \\left ( x _ { t } ^ { ( k ) } - R ^ { ( k ) } \\right ) ^ { 2 } \\right ) \\\\ & = \\int _ { 0 } ^ { T } \\left [ \\eta ( - \\dot { x } _ { t } ^ { * * } ) ^ { 2 }"
  },
  {
    "hash_code": "57f4ba80f7fa929ef11a64f507cc05ad",
    "text": "K T } { n } T } \\left ( \\eta \\left ( - x _ { t } ^ { ( k ) } \\right ) ^ { 2 } + \\frac { \\gamma } { 2 } \\sigma ^ { 2 } \\left ( x _ { t } ^ { ( k ) } - R ^ { ( k ) } \\right ) ^ { 2 } \\right ) \\\\ & = \\int _ { 0 } ^ { T } \\left [ \\eta ( - \\dot { x } _ { t } ^ { * * } ) ^ { 2 } + \\frac { \\theta \\sigma ^ { 2 } } { 2 } ( x _ { t } ^ { * * } - R _ { t } ) ^ { 2 } \\right ] d t ,$$where implies that the optimal value is obtained when x t = x ∗∗ t , in other words, x ∗∗ t is the optimal strategy.□## A.9 Proof to Theorem 3According to Theorem 2, the optimal strategies x t and ˜ x t can be given by$$x _ { t } = & \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( R _ { s } \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) \\text {d} s \\\\ & + \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + A + \\left ( - A + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } ,$$and$$\\tilde { x } _ { t } = & \\frac { \\kappa } { \\sinh \\kappa T } \\int _ { 0 } ^ { T } \\left ( \\tilde { R } _ { t } \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) \\text {d} s \\\\ & + \\left ( x _ { 0 } - \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa ( T - t ) } { \\sinh \\kappa T } + A + \\left ( - A + \\frac { \\mu } { \\theta \\sigma ^ { 2 } } \\right ) \\cdot \\frac { \\sinh \\kappa T - \\sinh \\kappa t } { \\sinh \\kappa T } ,$$where x t , ˜ x t are the optimal strategy with respect to reference strategies R t and ˜ R t respectively. Hence, by Jensen's inequality,$$\\text {Jensin's frequency} ; \\\\ ( x _ { t } - \\widetilde { x } _ { t } ) ^ { 2 } = & \\frac { \\kappa ^ { 2 } } { \\sinh ^ { 2 } \\kappa T } \\left ( \\int _ { 0 } ^ { T } \\left ( \\left ( R _ { s } ^ { ( 1 ) } - R _ { s } ^ { ( 2 ) } \\right ) \\sinh ( \\kappa \\min ( s , t ) ) \\sinh ( \\kappa ( T - \\max ( s , t ) ) ) \\right ) \\text {d} s \\right ) ^ { 2 } \\\\ \\leq & \\frac { \\kappa ^ { 2 } } { \\sinh ^ { 2 } \\kappa T } \\int _ { 0 } ^ { T } \\left ( R _ { s } ^ { ( 1 ) } - R _ { s } ^ { ( 2 ) } \\right ) ^ { 2 } \\text {d} s \\cdot \\int _ { 0 } ^ { T } \\sinh ^ { 2 } ( \\kappa \\min ( s , t ) ) \\sinh ^ { 2 } ( \\kappa ( T - \\max ( s , t ) ) ) \\text {d} s \\\\ < & \\frac"
  },
  {
    "hash_code": "0e33170652de24fa93a2968a03eb7f20",
    "text": "2 } \\kappa T } \\int _ { 0 } ^ { T } \\left ( R _ { s } ^ { ( 1 ) } - R _ { s } ^ { ( 2 ) } \\right ) ^ { 2 } \\text {d} s \\cdot \\int _ { 0 } ^ { T } \\sinh ^ { 2 } ( \\kappa \\min ( s , t ) ) \\sinh ^ { 2 } ( \\kappa ( T - \\max ( s , t ) ) ) \\text {d} s \\\\ < & \\frac { \\kappa ^ { 2 } } { \\sinh ^ { 2 } \\kappa T } \\cdot \\varepsilon \\cdot \\frac { 1 } { 4 \\kappa } \\left ( \\sinh ^ { 2 } ( \\kappa t ) \\left ( \\sinh ( 2 \\kappa ( T - t ) ) - 2 \\kappa ( T - t ) \\right ) + \\sinh ^ { 2 } ( \\kappa ( T - t ) ) \\left ( \\sinh ( 2 \\kappa t ) - 2 \\kappa t \\right ) \\right ) \\\\ \\leq & \\frac { \\kappa ^ { 2 } } { \\sinh ^ { 2 } \\kappa T } \\cdot \\varepsilon \\cdot \\frac { 1 } { 2 \\kappa } \\sinh ^ { 2 } \\left ( \\kappa \\frac { T } { 2 } \\right ) \\left ( \\sinh \\kappa T - \\kappa T \\right ) = \\frac { \\kappa \\left ( \\sinh \\kappa T - \\kappa T \\right ) } { 4 \\left ( \\cosh \\kappa T + 1 \\right ) } \\cdot \\varepsilon < \\frac { \\kappa } { 4 } \\\\ \\text {which implies the fact that when } \\varepsilon \\geq 0 \\ \\widetilde { \\kappa } \\to \\kappa \\text {, } \\text {so} \\text {wey may use the outimal strategy by a device-wise constant}$$which implies the fact that when ε → 0 , ˜ x t → x t . So we may use the optimal strategy by a piece-wise constant RS to approximate the general RS, with small error$$| x _ { t } - \\widetilde { x } _ { t } | < \\frac { 1 } { 2 } \\sqrt { \\kappa \\varepsilon } .$$## References- [1] R. Almgren and N. Chriss, 'Optimal execution of portfolio transactions,' Journal of Risk , vol. 3, pp. 5-40, 2001.- [2] C. Institute, CFA Program Curriculum 2018 Level II . John Wiley &amp; Sons, 2017.- [3] O. Guéant, 'Target close execution strategies.' [Online]. Available: https://www.oliviergueant.com/ uploads/4/3/0/9/4309511/targetclose.pdf- [4] R. Almgren and N. Chriss, 'Value under liquidation,' Risk , vol. 12, no. 12, pp. 61-63, 1999.- [5] R. F. Almgren, 'Optimal execution with nonlinear impact functions and trading-enhanced risk,' Applied Mathematical Finance , vol. 10, no. 1, pp. 1-18, 2003.- [6] D. Bertsimas and A. W. Lo, 'Optimal control of execution costs,' Journal of Financial Markets , vol. 1, no. 1, pp. 1-50, 1998.- [7] J. Gatheral, 'No-dynamic-arbitrage and market impact,' Quantitative finance , vol. 10, no. 7, pp. 749-759, 2010.- [8] J. Gatheral, A. Schied, and A. Slynko, 'Transient linear price impact and fredholm integral equations,' Mathematical Finance: An International Journal of Mathematics, Statistics and Financial Economics , vol. 22, no. 3, pp. 445-474, 2012.- [9] S. Tse, P. Forsyth, J. Kennedy, and H. Windcliff, 'Comparison between the mean-variance optimal and the mean-quadratic-variation optimal trading strategies,' Applied Mathematical Finance , vol. 20, no. 5, pp. 415-449"
  },
  {
    "hash_code": "7c80052f1680dd510b5ed7bf0d0cd92a",
    "text": "] J. Gatheral, A. Schied, and A. Slynko, 'Transient linear price impact and fredholm integral equations,' Mathematical Finance: An International Journal of Mathematics, Statistics and Financial Economics , vol. 22, no. 3, pp. 445-474, 2012.- [9] S. Tse, P. Forsyth, J. Kennedy, and H. Windcliff, 'Comparison between the mean-variance optimal and the mean-quadratic-variation optimal trading strategies,' Applied Mathematical Finance , vol. 20, no. 5, pp. 415-449, 2013.- [10] X. Cheng, M. Di Giacinto, and T.-H. Wang, 'Optimal execution with dynamic risk adjustment,' Journal of the Operational Research Society , vol. 70, no. 10, pp. 1662-1677, 2019.- [11] C. Frei and N. Westray, 'Optimal execution of a vwap order: a stochastic control approach,' Mathematical Finance , vol. 25, no. 3, pp. 612-639, 2015.- [12] Á. Cartea and S. Jaimungal, 'A closed-form execution strategy to target volume weighted average price,' SIAM Journal on Financial Mathematics , vol. 7, no. 1, pp. 760-785, 2016.- [13] O. Guéant and G. Royer, 'Vwap execution and guaranteed vwap,' SIAM Journal on Financial Mathematics , vol. 5, no. 1, pp. 445-471, 2014.- [14] R. Almgren and J. Lorenz, 'Adaptive arrival price,' Trading , vol. 2007, no. 1, pp. 59-66, 2007.- [15] C. Frei and N. Westray, 'Optimal execution in hong kong given a market-on-close benchmark,' Quantitative Finance , vol. 18, no. 4, pp. 655-671, 2018.- [16] O. Guéant, J. Pu, and G. Royer, 'Accelerated share repurchase: pricing and execution strategy,' International Journal of Theoretical and Applied Finance , vol. 18, no. 03, p. 1550019, 2015.- [17] L. Bargeron, M. Kulchania, and S. Thomas, 'Accelerated share repurchases,' Journal of Financial Economics , vol. 101, no. 1, pp. 69-89, 2011.- [18] X. Cheng, M. Di Giacinto, and T.-H. Wang, 'Optimal execution with uncertain order fills in almgrenchriss framework,' Quantitative Finance , vol. 17, no. 1, pp. 55-69, 2017.- [19] R. Carmona and L. Leal, 'Optimal execution with quadratic variation inventories,' SIAM Journal on Financial Mathematics , vol. 14, no. 3, pp. 751-776, 2023.- [20] M. Nutz, K. Webster, and L. Zhao, 'Unwinding stochastic order flow: When to warehouse trades,' arXiv preprint arXiv:2310.14144 , 2023.- [21] Á. Cartea and L. Sánchez-Betancourt, 'Brokers and informed traders: dealing with toxic flow and extracting trading signals,' Available at SSRN 4265814 , 2022.- [22] J. Yong and X. Y. Zhou, Stochastic controls: Hamiltonian systems and HJB equations . Springer Science &amp; Business Media, 2012, vol. 43.- [23] T. E. Duncan, 'Linear-exponential-quadratic gaussian control,' IEEE Transactions on Automatic Control , vol. 58, no. 11, pp. 2910-2911, 2013.- [24] A. E. Lim and X. Y. Zhou, 'A new risk-sensitive maximum principle,' IEEE transactions on automatic control , vol. 50, no. 7, pp. 958-966, 2005.- [25] R. Carmona and K. Webster, 'The self-financing equation in limit order book markets,' Finance and Stochastics , vol. 23, no. 3, pp. 729-759, Jul. 2019.- [26] A. Mazzolo, 'Constraint Ornstein-Uhlenbeck bridges,' Journal of Mathematical Physics , vol. 58, no. 9, p. 093302, 09 201"
  },
  {
    "hash_code": "e1234ff3d132773cec528b6fd94d40c8",
    "text": "principle,' IEEE transactions on automatic control , vol. 50, no. 7, pp. 958-966, 2005.- [25] R. Carmona and K. Webster, 'The self-financing equation in limit order book markets,' Finance and Stochastics , vol. 23, no. 3, pp. 729-759, Jul. 2019.- [26] A. Mazzolo, 'Constraint Ornstein-Uhlenbeck bridges,' Journal of Mathematical Physics , vol. 58, no. 9, p. 093302, 09 2017.- [27] O. Guéant, The Financial Mathematics of Market Liquidity: From optimal execution to market making . CRC Press, 2016, vol. 33.- [28] R. C. Merton, 'Optimum consumption and portfolio rules in a continuous-time model,' in Stochastic optimization models in finance . Elsevier, 1975, pp. 621-661."
  }
]