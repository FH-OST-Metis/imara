<doctag><page_header><loc_15><loc_125><loc_31><loc_362>arXiv:2003.08005v1  [cs.CV]  18 Mar 2020</page_header>
<section_header_level_1><loc_42><loc_33><loc_458><loc_62>ScanSSD: Scanning Single Shot Detector for Mathematical Formulas in PDF Document Images</section_header_level_1>
<text><loc_109><loc_73><loc_391><loc_79>Parag Mali, Puneeth Kukkadapu, Mahshad Mahdavi, and, Richard Zanibbi</text>
<text><loc_132><loc_81><loc_368><loc_101>Department of Computer Science, Rochester Institute of Technology Rochester, NY 14623, USA Email: { psm2208, pxk8301, mxm7832, rxzvcs } @rit.edu</text>
<text><loc_34><loc_117><loc_245><loc_222>Abstract -We introduce the Scanning Single Shot Detector (ScanSSD) for locating math formulas offset from text and embedded in textlines. ScanSSD uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results. For our experiments we use the TFD-ICDAR2019v2 dataset, a modification of the GTDB scanned math article collection. ScanSSD detects characters in formulas with high accuracy, obtaining a 0.926 f-score, and detects formulas with high recall overall. Detection errors are largely minor, such as splitting formulas at large whitespace gaps (e.g., for variable constraints) and merging formulas on adjacent textlines. Formula detection f-scores of 0.796 (IOU ≥ 0 . 5 ) and 0.733 (IOU ≥ 0 . 75 ) are obtained. Our data, evaluation tools, and code are publicly available.</text>
<section_header_level_1><loc_107><loc_229><loc_172><loc_235>I. INTRODUCTION</section_header_level_1>
<text><loc_34><loc_239><loc_245><loc_337>The PDF format is used ubiquitously for sharing and printing documents. Unfortunately, while the latest PDF specification supports embedding structural information for graphical elements (e.g., figures, tables, and footnotes 1 ), most born-digital PDF documents contain only rendering-level information such as characters, lines, and images. These low-level objects can be recovered by parsing the document [1], but graphic regions must be located using detection algorithms. For example, PDFFigures [2] extracts figures and tables from born-digital Computer Science research papers for Semantic Scholar. 2 Unfortunately, older PDF documents may contain only scanned images of document pages, providing no information about characters or other graphical objects in the page images.</text>
<text><loc_34><loc_339><loc_245><loc_408>We present a new image-based detector for mathematical formulas in in both born-digital and scanned PDF documents. Math expressions may be displayed and offset from the main text, or appear embedded directly in text lines (see Figure 1). Displayed expressions are generally easier to detect due to indentation and vertical gaps, whereas embedded expressions are more challenging. Embedded equations may differ in font, e.g., for the italicized variable t in Figure 1, but italicized words and variations in text fonts make fonts unreliable for detecting embedded formulas in general.</text>
<footnote><loc_41><loc_416><loc_153><loc_421>1 https://www.iso.org/standard/63534.html</footnote>
<footnote><loc_41><loc_422><loc_131><loc_426>2 https://www.semanticscholar.org</footnote>
<picture><loc_262><loc_115><loc_458><loc_172><other><caption><loc_268><loc_177><loc_453><loc_182>Fig. 1: Embedded (blue) vs. displayed (red) formulas.</caption></picture>
<text><loc_255><loc_197><loc_466><loc_280>Our work makes two main contributions. First, we introduce the ScanSSD architecture for detecting formulas using only visual features. A deep neural-network Single Shot Detector (SSD [3]) locates formulas at multiple scales using a sliding window in a 600 dpi page image. Page-level formula detections are obtained by pooling SSD region detections using a simple voting and thresholding procedure. ScanSSD detects characters in formulas with high accuracy (92.6% f-score), and detects formula regions accurately enough to be used as a baseline for indexing mathematical formulas in born-digital and scanned PDF documents. The ScanSSD code is publicly available. 3</text>
<text><loc_255><loc_282><loc_466><loc_330>Our second contribution is a new benchmark for formula detection comprised of a dataset and evaluation tools. The dataset is a modification of the GTDB database of Suzuki et al [4]. Our data and evaluation tools were developed for the ICDAR 2019 TFD competition [5], and the dataset (TFD-ICDAR2019v2) and evaluation tools are publicly available (see Section III).</text>
<text><loc_255><loc_332><loc_466><loc_358>In the next section we provide an overview of related work, followed by our dataset (Section III), ScanSSD (Sections IV and V), our results (Section VI), and finally our conclusions and plans for future work (Section VII).</text>
<section_header_level_1><loc_324><loc_365><loc_397><loc_370>II. RELATED WORK</section_header_level_1>
<text><loc_255><loc_374><loc_466><loc_415>Existing methods for formula detection in PDF documents use formatting information, such as page layout, character labels, character locations, font sizes, etc. However, PDF documents are generated by many different tools, and the quality of their character information varies. Lin et al. [6] point out math formulas may be composed of several object</text>
<footnote><loc_262><loc_422><loc_370><loc_426>3 https://github.com/MaliParag/ScanSSD</footnote>
<page_break>
<text><loc_34><loc_31><loc_245><loc_65>types (e.g. text, image, graph). For example, the square root sign in a PDF generated from L A T E X contains the text object representing a radical sign and a graphical object for the horizontal line. As a result, some symbols must be identified from multiple drawing elements.</text>
<text><loc_34><loc_67><loc_245><loc_129>Given characters and formula locations, the visual structure of each formula (i.e., spatial arrangement of symbols on writing lines) can be recovered with high accuracy using existing techniques [7]-[12]. For formula retrieval, flexible matching of sub-expressions requires that formula structure (i.e., visual syntax and/or semantics) be available -however, there has been recent work using CNN-based embeddings for purely appearance-based retrieval [13].</text>
<text><loc_34><loc_131><loc_245><loc_228>Displayed expression detection is relatively easy, as offset formulas differ in height and width of the line, character size, and, symbol layout [14]. Embedded mathematical expressions are more challenging: Iwatsuki et al. [15] conclude that distinguishing dictionary words that appear in italics and embedded mathematical expressions is a non-trivial task as embedded formulas at times can contain complex mathematical structures such as summations or integrals. However, many embedded math expressions are very small, often just a single symbol in a definition such as 'where w is the set of words'. Some approaches have been proposed specifically for embedded math expression detection [15], [16] and others specifically for displayed math expressions [17], [18].</text>
<text><loc_34><loc_230><loc_245><loc_320>Lin et al. classify formula detection methods into three categories based on the features used [6]. These categories are character-based, image-based, and layout-based. Character-based methods use OCR engines to identify characters, and characters not recognized by the engine are considered candidates for math expression elements. The second category of methods uses image segmentation. Most traditional methods require segmentation thresholds. Setting threshold values can be difficult, especially for the unknown documents. Layout-based methods detect math expressions using features such as line height, line spacing, alignment, etc. Many published methods use a combination of character features, layout features, and context features.</text>
<section_header_level_1><loc_34><loc_327><loc_115><loc_332>A. Traditional Methods</section_header_level_1>
<text><loc_34><loc_336><loc_245><loc_427><loc_255><loc_31><loc_466><loc_86>Garain and Chaudhari did a survey of over 10,000 document pages and found the frequency of each mathematical character in formulas [19]. They used this information found to develop a detector for embedded mathematical expressions [20]. They scan each text line and decide if the line contains one of the 25 most frequent mathematical symbols. After finding the leftmost word containing a mathematical symbol, they grow the region around the word on the left and right using rules to identify the formula region. For detection of displayed expressions they use two features: first, white space around math expressions. Second, the standard deviation of the left lowermost pixels of symbols on the text line. They base this feature on the observation that for a math expression, the leftmost pixels of each symbol are often not on the same line, while for text they often are. A disadvantage of their method for embedded formula detection is that it requires symbol recognition, which adds complexity to the system. Another approach based on locating mathematical symbols and then growing formula regions around symbols was proposed by Kacem et al., but using fuzzy logic [21].</text>
<text><loc_255><loc_88><loc_466><loc_214>Lin et al. [6] proposed a four-step detection process. In the first step, they extract the locations, bounding boxes, baselines, fonts, etc., and use them for character and layout features in the following steps. They also process math symbols comprised of multiple objects: for example, a vertical delimiter may be made up of multiple short vertical line objects. They detect named mathematical functions such as 'sin,' 'cos,' etc., and numbers. In the next step, they distinguish text lines from non-text lines. They find displayed math expressions in nontext lines using geometric layout features (e.g., line-height), character features (e.g., is it the character part of a named math function like 'sin'), and context features (e.g., whether the preceding and the following character is a math element). In the last step, they classify characters into math and non-math characters. They find embedded math expressions by merging characters tagged as math characters. SVM classification was used for both isolated math expression detection, and character classification into math and non-math.</text>
<section_header_level_1><loc_255><loc_220><loc_415><loc_226>B. CRF and Deep Learning-Based Techniques</section_header_level_1>
<text><loc_255><loc_230><loc_466><loc_320>For born-digital PDF papers, Iwatsuki et al. [15] created a manually annotated dataset and applied conditional random fields (CRF) for math-zone identification using both layout features (e.g. font types) and linguistic features (e.g. n-grams) extracted from PDF documents. For each word, they used three labels: the beginning of a math expression, inside a math expression, and at the end of a math expression. They concluded that words and fonts are important for distinguishing math from the text. This method has limitations, as it requires a specially annotated dataset that has each word annotated with either beginning, inside or end of the math expression label. Their method works only for born-digital PDF documents with layout information.</text>
<text><loc_255><loc_322><loc_466><loc_405>Gao et al. [18] used a combination of CNN and RNN for formula detection. They first extract text, graph and image streams from the PDF document. Next, they perform top-down layout analysis based on XY-cutting [22], and bottom-up layout analysis based on connected components to generate candidate expression regions. Features are then extracted using neural networks from each candidate region, and they classify candidate regions. Finally, they adjust and refine the incomplete math expression areas. Similar to their method, we use a CNN model (VGG16 [23]) for feature extraction. In contrast to their method, we do not depend on the layout analysis of the page.</text>
<text><loc_255><loc_407><loc_466><loc_427>Recently Ohyama et al. [4] used a U-net to detect characters in formulas. The U-net acts as a pixel-level image filter, and does not produce regions for symbols or formulas.</text>
<page_break>
<text><loc_34><loc_31><loc_245><loc_93>Detection is evaluated based on pixel-level agreement between detected and ground-truth symbols; formula detection is estimated based on the number of formulas with at least half of their characters detected. In contrast, our method produces bounding boxes for mathematical expressions of one or more symbols. As we wanted to propose specific regions (bounding boxes) for formulas, we decided to explore modern object detection methods employing deep neural networks.</text>
<text><loc_34><loc_95><loc_245><loc_107>We next we discuss different object detection methods, and our selection of SSD as the underlying detector for our model.</text>
<section_header_level_1><loc_34><loc_113><loc_104><loc_118>C. Object Detection</section_header_level_1>
<text><loc_34><loc_122><loc_245><loc_262>The first deep learning algorithm that achieved noticably stronger results for object detection task was the R-CNN [24] (Region proposal with CNN). Unlike R-CNN, which feeds ≈ 2 k regions to a CNN for each image, in Fast R-CNN [25] only the original input image is used as input. Faster R-CNN [26] introduced a different architecture, the region-proposal network. In contrast to R-CNN, Fast R-CNN, and Faster R-CNN which use region proposals, the YOLO [27] and Single Shot MultiBox Detectors (SSD) [3] perform detection in a single-stage network. Both YOLO and SSD divide the input image into a grid, where each grid point has an associated set of 'default' bounding boxes. Unlike YOLO, SSD uses multiple grids with different scales instead of a single grid. This allows an SSD detector to divide the responsibility for detecting objects across scales. The SSD network learns to predict offsets and size modifications for each default bounding box. Just like R-CNN, SSD uses the VGG16 [23] architecture for feature extraction. SSD does not require selective search, region proposals, or multi-stage networks like R-CNN, Fast R-CNN, and, Faster R-CNN.</text>
<text><loc_34><loc_264><loc_245><loc_319>Among the CNN-based object detectors, SSD is a simple single stage model that obtains accuracy comparable to models with region proposal steps such as Faster R-CNN [3], [28]. Liao et al. with their TextBoxes architecture have shown that a modified SSD can detect wide regions [29]. Formulas are often quite wide, and so we use an SSD modified in a manner similar to TextBoxes as the basis for our formula detector. Details for our detector are presented in Section IV.</text>
<section_header_level_1><loc_48><loc_324><loc_231><loc_329>III. CREATING THE TFD-ICDAR2019V2 DATASET</section_header_level_1>
<text><loc_34><loc_333><loc_245><loc_409>For typeset formula detection, we modified ground truth for the GTDB1 and GTDB2 datasets 4 created by Suzuki et al. [30]. TFD-ICDAR2019v2 represents 'Typeset Formula Detection task for ICDAR 2019,' version 2. The first version was used for the CROHME math recognition competition at ICDAR 2019 [5]: version two (v2) adds formulas to ground truth that were missing in the original. The dataset is available online, and we provide scripts to compile and render the dataset PDFs at 600 dpi, and evaluation scripts with region matching using thresholded intersection-over-union (IOU) measures. 5</text>
<footnote><loc_41><loc_416><loc_205><loc_421>4 available from https://github.com/uchidalab/GTDB-Dataset</footnote>
<footnote><loc_41><loc_422><loc_175><loc_426>5 https://github.com/MaliParag/TFD-ICDAR2019</footnote>
<otsl><loc_258><loc_38><loc_459><loc_69><ecel><ched>Docs (Pages)<ched>1 symbol<ched>Formulas > 1 symbol<ched>Total<nl><rhed>Training<fcel>36 (569)<fcel>7506<fcel>18947<fcel>26453<nl><rhed>Test<fcel>10 (236)<fcel>2556<fcel>9350<fcel>11906<nl><caption><loc_271><loc_29><loc_450><loc_34>TABLE I: TFD-ICDAR2019v2 Collection Statistics.</caption></otsl>
<text><loc_255><loc_83><loc_466><loc_131>The GTDB collection provides annotations for 48 PDF documents from scientific journals and textbooks using a variety of font faces and notation styles. It also provides ground truth at the character level in CSV format, including spatial relationships between math characters (e.g., subscript, superscript). Character labels, and an indication of whether a character belongs to a formula region are also provided.</text>
<text><loc_255><loc_133><loc_466><loc_209>At the time we created our dataset in early 2019, we were unable to locate two PDFs from GTDB1, and so omitted them in TFD-ICDAR2019v2. 6 From the remaining 46 documents, 10 PDFs from GTDB2 serve as the test set (see Figure 7). We developed image processing tools for modifying the GTDB ground-truth to reflect scale and translation differences found in the publicly available versions of the PDF documents. GTDB also does not provide bounding boxes for math expressions directly: we used character bounding boxes and spatial relationships to generate math regions in our ground truth files.</text>
<text><loc_255><loc_211><loc_466><loc_231>Metrics for TFD-ICDAR2019v2 may be found in Table I. It is worth noting that over 25% of formulas in the collection contain a single symbol (e.g., ' λ ').</text>
<section_header_level_1><loc_280><loc_237><loc_441><loc_242>IV. SCANSSD: WINDOW-LEVEL DETECTION</section_header_level_1>
<text><loc_255><loc_246><loc_466><loc_323>Figure 2 illustrates the ScanSSD architecture. First, we use a sliding window to sample overlapping sub-images from the document page image. We then pass each window to a Single-Shot Detector (SSD [3]) to locate formula regions. SSD simultaneously evaluates multiple formula region candidates laid out in a grid (see Figure 3), and then applies non-maximal suppression (NMS) to select the window-level detections. NMS is a greedy strategy that keeps one detection per group of overlapping detections. Formulas detected within each window have associated confidences, shown using colour in the 3rd stage of Figure 2.</text>
<text><loc_255><loc_325><loc_466><loc_365>As seen with the purple boxes in Figure 2, many formulas are repeated and/or split across the sampled windows. To obtain page-level formula detections, we first stitch the window-level SSD detections together on the page. A voting-based pooling method in then used to obtain final detection results (shown as green boxes in Figure 2).</text>
<text><loc_263><loc_367><loc_441><loc_372>Details of the ScanSSD system are provided below.</text>
<section_header_level_1><loc_255><loc_379><loc_323><loc_384>A. Sliding Windows</section_header_level_1>
<text><loc_255><loc_388><loc_466><loc_414>To produce sub-images for use in detection, starting from a 600 dpi page image we slide a 1200 × 1200 window with a vertical and horizontal stride (shift) of 120 pixels (10% of window size). Our windows are roughly 10 text lines in height,</text>
<page_footer><loc_262><loc_422><loc_383><loc_426>6 MA 1970 26 38, and MA 1977 275 292</page_footer>
<page_break>
<chart><loc_60><loc_32><loc_440><loc_106><bar_chart><caption><loc_34><loc_111><loc_466><loc_123>Fig. 2: ScanSSD architecture. Heatmaps illustrate detection confidences with gray ≈ 0 , red ≈ 0 . 5 , white ≈ 1 . 0 . Purple and green bounding boxes show formula regions after stitching window-level detections and pooling, respectively.</caption></chart>
<text><loc_34><loc_139><loc_245><loc_165>which makes math formulas large enough for SSD to detect them reliably. The SSD detector is trained using ground truth math regions cropped at the boundary of each window, after scaling and translating formula bounding boxes appropriately.</text>
<text><loc_34><loc_167><loc_245><loc_286>Advantages. There are four main advantages to using sliding windows. The first is data augmentation: only 569 page images are available in the training set, which is very small for training a deep neural network. Our sliding windows produce 656,717 sub-images. Second, converting the original page image directly to 300 × 300 or 512 × 512 loses a great deal of visual information, and when we tried to detect formulas using subsampled page images recall was extremely low. Third, as we maintain the overlap between windows, the network sees formulas multiple times, and has multiple chances to detect a formula. This helps increase recall, because formulas appear in more regions of detection windows. Finally, Liu et al. [3] mention that SSD is challenged when detecting small objects. Formulas with just one or two characters are common, but also small. Using high-resolution sub-images increases the relative size of math regions, which makes it easier for SSD to detect them.</text>
<text><loc_34><loc_287><loc_245><loc_385>Disadvantages. There are also a few disadvantages to using sliding windows versus detection within a single page image. The first is increased computational cost; this can be mitigated through parallelization, as each window may be processed independently. Secondly, windowing cuts formulas if they do not fit in a window. This means that a large expression may be split into multiple sub-images; this makes it impossible to train the SSD network to detect large math expressions directly. To mitigate this issue, we train the network to detect formulas across windows. Furthermore, windowing requires that we stitch (combine) results from individual windows to obtain detection results at the level of the original page. We discuss how we address these problems using pooling methods in section V.</text>
<section_header_level_1><loc_34><loc_391><loc_197><loc_396>B. Region Matching and Default Boxes in SSD</section_header_level_1>
<text><loc_34><loc_400><loc_245><loc_427><loc_255><loc_139><loc_466><loc_215>SSD defines a fixed space of candidate detection regions organized in a spatial grid at multiple resolutions ('default boxes'). Each default box may be resized and translated by the SSD network to fit target regions, and is associated with a confidence score. Figure 3 shows default boxes of different sizes and aspect ratios overlaid on a 512 × 512 image. In SSD, each feature map is a pixel grid, but the associated default boxes are defined in the original image coordinate space. The image is analyzed at multiple scales; here for illustration the 32 × 32 grid of default boxes is shown. In practice, if we used only the 32 × 32 default boxes, we might miss smaller objects. For the highlighted formula in Figure 3, the wider yellow box has the maximum intersection-over-union (IOU), and during training the wide yellow box will be matched with the highlighted ground truth.</text>
<text><loc_255><loc_218><loc_466><loc_280>Our metric for matching ground truth to candidate detection regions is the same as SSD [3]. Each ground truth box is matched to a default box with the highest IOU, and also with default boxes with an IOU greater than 0.5. Matching targets to more than one default box simplifies learning by allowing the network to predict higher scores for more boxes. The matched default boxes are considered positive examples (POS) and the remaining default boxes are considered negative examples (NEG).</text>
<picture><loc_289><loc_295><loc_428><loc_394><other><caption><loc_255><loc_398><loc_466><loc_425>Fig. 3: Default boxes for a 512 × 512 window. Box centers ( glyph[star] ) are in a 32 × 32 grid. Shown are six default boxes around one point with different sizes and aspect ratios (red, green, and yellow boxes) located near a target formula (pink highlight).</caption></picture>
<page_break>
<text><loc_34><loc_31><loc_245><loc_93>The original SSD [3] architecture uses aspect ratios (width/height) of { 1 , 2 , 3 , 1 / 2 , 1 / 3 } . However, as we see in Figure 4, there are many wide formulas with an aspect ratio greater than 3 in the dataset. As a result, wider default boxes will have a higher chance of matching wide formulas. So, in addition to the default boxes used in the original SSD, we also add the wider default boxes used in TextBoxes [29], with aspect ratios { 5 , 7 , 10 } . In our early experiments, these wider default boxes increased recall for large formulas.</text>
<section_header_level_1><loc_34><loc_105><loc_96><loc_110>C. Postprocessing</section_header_level_1>
<text><loc_34><loc_116><loc_245><loc_171>Figure 5 illustrates postprocessing in ScanSSD. We expand and/or shrink initial formula detections so that are cropped around the connected components they contain and touch at their border. The goal is to capture entire characters belonging to a detection region, without additional padding. This postprocessing is done at two stages: first, before stitching, and second, after pooling regions to obtain output formula detections.</text>
<section_header_level_1><loc_36><loc_183><loc_243><loc_188>V. SCANSSD: VOTING-BASED POOLING FROM WINDOWS</section_header_level_1>
<text><loc_34><loc_196><loc_245><loc_279>At inference time we send overlapping page windows to our modified SSD detector, and obtain formula bounding boxes with associated confidences in each window. As the SSD network sees the same page region multiple times, multiple bounding boxes are often predicted for a single formula (see Figure 2). Detections within windows are stitched together on the page, and then each detection region votes at the pixel level. Pixel-level votes are thresholded, and the bounding boxes of connected components in the resulting binary image are returned in the output as formula detections. Example formula detection results are provided in Figure 6.</text>
<text><loc_34><loc_282><loc_245><loc_309>Voting. Let B be the set of page-level bounding boxes for detected formulas, and C be set of confidences obtained for each. Let B i ∈ B be the i th bounding box with confidence C i ∈ C . Let each pixel in image I be represented by pixel</text>
<chart><loc_38><loc_323><loc_240><loc_408><bar_chart></chart>
<caption><loc_34><loc_412><loc_245><loc_425>Fig. 4: Formula aspect ratios. Most formulas in our dataset have more width than height, i.e., are oriented horizontally.</caption>
<picture><loc_284><loc_30><loc_442><loc_67><other><caption><loc_331><loc_71><loc_390><loc_76>(a) Initial detection</caption></picture>
<picture><loc_284><loc_80><loc_442><loc_117><other><caption><loc_332><loc_121><loc_388><loc_126>(b) After cropping</caption></picture>
<caption><loc_255><loc_131><loc_466><loc_144>Fig. 5: Postprocessing crops detection regions around connected components within or touching the initial detection.</caption>
<text><loc_255><loc_159><loc_466><loc_172>P ab . We say that a pixel P ab ∈ B i if it is inside the bounding box B i . Let us define</text>
<formula><loc_319><loc_175><loc_401><loc_192>L _ { a b } ^ { i } = \begin{cases} 1 & \text {if } P _ { a b } \in B _ { i } \\ 0 & \text {if } P _ { a b } \notin B _ { i } \end{cases}</formula>
<text><loc_255><loc_197><loc_466><loc_218>It is possible that ∑ i L i ab ≥ 1 , meaning that P ab belongs to more than one bounding box. We considered different vote scoring functions S ab for each pixel P ab :</text>
<formula><loc_349><loc_219><loc_359><loc_223>| B |</formula>
<formula><loc_277><loc_220><loc_429><loc_277>\text {scoring functions} \ S _ { a b } \text { for each pixel } P _ { a b } \colon \\ \quad \text {uniform} \ ( c o u n t ) \quad \sum _ { i \colon = 0 } ^ { | B | } L _ { a b } ^ { i } \\ \quad \max \quad \arg \max _ { i \in \{ 0 , \dots , | B | \} } L _ { a b } ^ { i } C _ { i } \\ \quad \\ \quad \sum _ { i = 0 } ^ { | B | } L _ { a b } ^ { i } C _ { i } \\ \quad \text {average} \quad \sum _ { i = 0 } ^ { | B | } L _ { a b } ^ { i } C _ { i } \ / \ \sum _ { i = 0 } ^ { | B | } L _ { a b } ^ { i } \\ \quad \\ \text {Threshold} \ \ W _ { a } \text { computes} \ \text {voting} \ \text {method} \ \text {and} \ \text {two} \ t</formula>
<text><loc_255><loc_285><loc_466><loc_361>Thresholding. We compare voting methods and tune their associated thresholds using the training data. A grid search was performed to maximize detection results for each voting method (f-score for IOU ≥ 0 . 75 ). Average scoring does not perform as well as the other methods. For uniform weighting and sum scores, we tried thresholds in { 0 , 1 , . . . , 55 } , and for max scoring we tried thresholds in { 0 , 1 , . . . , 100 } . The simplest method where each pixel for the number of detections it belongs to (uniform weighting) obtained the best detection results using a threshold value of 30, and so we use this in our experiments.</text>
<section_header_level_1><loc_304><loc_368><loc_417><loc_373>VI. RESULTS AND DISCUSSION</section_header_level_1>
<section_header_level_1><loc_255><loc_377><loc_295><loc_382>A. Training</section_header_level_1>
<text><loc_255><loc_386><loc_466><loc_427>We used a validation set to tune hyper-parameters for the ScanSSD detector. The TFD-ICDAR2019v2 training dataset was further divided into training (453 pages) and validation sets (116 pages). This produces 524,718 training and 131,999 testing sub-images, respectively. In our preliminary experiments, we observed that using a larger window size</text>
<page_break>
<picture><loc_38><loc_29><loc_462><loc_153><other><caption><loc_34><loc_157><loc_466><loc_177>Fig. 6: Detection results. Detected formulas are shown as blue bounding boxes. Split formulas are highlighted in pink (3rd panel), and merged formulas are highlighted in green (4th panel). A small number of false negatives (red) and false positives (yellow) are produced.</caption></picture>
<text><loc_34><loc_192><loc_245><loc_232>with SSD512 performs far better (+5% f-score) than SSD300, [31] and cross-entropy loss with hard-negative mining performs better than focal-loss [32] (with or without hard-negative mining). Focal-loss reshapes the standard cross entropy loss such that it down-weights the loss for well-classified examples.</text>
<text><loc_34><loc_234><loc_245><loc_339>We evaluated SSD models with different parameters 7 and found that our HBOXES512 model, which introduces additional default box aspect ratios (see Section IV-B) performs better than SSD512, and MATH512 performs better than HBOXES512. For HBOXES512 we used default boxes with aspect ratios { 1 , 2 , 3 , 5 , 7 , 10 } instead of default boxes with aspect ratios { 1 , 2 , 3 , 1 / 2 , 1 / 3 } for SSD512. MATH512 uses default boxes with aspect ratios { 1 , 2 , 3 , 5 , 7 , 10 } as well as rectangular kernels of size 1 × 5 rather than the square 3 × 3 kernel used in SSD512. From our experiments on the validation set, we observed that the MATH512 model consistently obtained the best detection results for the 512 × 512 inputs (by 0.5% to 1.0% f-score). So we use MATH512 for our evaluation. We then re-trained MATH512 using all TFD-ICDAR2019v2 training data.</text>
<text><loc_34><loc_342><loc_245><loc_361>ScanSSD was built starting from an existing PyTorch SSD implementation. 8 The VGG16 sub-network was pre-trained on ImageNet [33].</text>
<section_header_level_1><loc_34><loc_370><loc_114><loc_375>B. Quantitative Results</section_header_level_1>
<text><loc_34><loc_381><loc_245><loc_407>We used two evaluation methods, based on the ICDAR 2019 Typeset Formula Detection competition [5] (Table II), and the character-level detection metrics used by Ohyama et al. [4] (Table III).</text>
<footnote><loc_41><loc_416><loc_120><loc_421>7 Details are available in [31]</footnote>
<footnote><loc_41><loc_422><loc_155><loc_426>8 https://github.com/amdegroot/ssd.pytorch</footnote>
<otsl><loc_254><loc_199><loc_460><loc_241><ecel><ched>IOU ≥ 0 . 75<lcel><lcel><ched>IOU ≥ 0 . 5<lcel><lcel><nl><ecel><ched>Precision<ched>Recall<ched>F-score<ched>Precision<ched>Recall<ched>F-score<nl><rhed>ScanSSD *<fcel>0.781<fcel>0.690<fcel>0.733<fcel>0.848<fcel>0.749<fcel>0.796<nl><rhed>RIT 2<fcel>0.753<fcel>0.625<fcel>0.683<fcel>0.831<fcel>0.670<fcel>0.754<nl><rhed>RIT 1<fcel>0.632<fcel>0.582<fcel>0.606<fcel>0.744<fcel>0.685<fcel>0.713<nl><rhed>Mitchiking<fcel>0.191<fcel>0.139<fcel>0.161<fcel>0.369<fcel>0.270<fcel>0.312<nl><rhed>Samsung ‡<fcel>0.941<fcel>0.927<fcel>0.934<fcel>0.944<fcel>0.929<fcel>0.936<nl><caption><loc_290><loc_190><loc_431><loc_195>TABLE II: Results for TFD-ICDAR2019</caption></otsl>
<unordered_list><list_item><loc_264><loc_242><loc_349><loc_246>Used TFD-ICDAR2019v2 dataset</list_item>
<list_item><loc_264><loc_247><loc_433><loc_251>† Earlier ScanSSD, placed 2 nd in TFD-ICDAR 2019 competition [5]</list_item>
<list_item><loc_264><loc_251><loc_334><loc_256>‡ Used character information</list_item>
</unordered_list>
<text><loc_255><loc_271><loc_466><loc_304>Formula detection. An earlier version of ScanSSD placed second in the ICDAR 2019 competition on Typeset Formula Detection (TFD) [5]. 9 The new ScanSSD system outperforms the other systems from the competition that did not use character locations and labels from ground truth.</text>
<text><loc_255><loc_307><loc_466><loc_355>Figure 7 gives the document-level f-scores for each of the 10 testing documents, for matching constraints IOU ≥ 0 . 5 and IOU ≥ 0 . 75 . The highest and lowest f-scores for IOU ≥ 0 . 75 are 0.8518 for Erbe94, and 0.5898 for Emden76. We think this variance is due to document styles: we have more training documents with a style similar to Erbe94 than Emden76. With more diverse training data we expect better results.</text>
<text><loc_255><loc_357><loc_466><loc_412>Examining the effect of the IOU matching threshold on results demonstrates that the detection regions found by ScanSSD are highly precise: 70.9% of the ground-truth formulas are found at their exact location (i.e., IOU threshold of 1.0). Requiring this exact matching of detected and ground truth formulas also yields a precision of 62.67%, and an f-score of 66.5%. To obtain a more complete picture, we next look at the detection of math symbols.</text>
<text><loc_262><loc_422><loc_428><loc_426>9 The first place system used provided character information.</text>
<page_break>
<otsl><loc_56><loc_52><loc_226><loc_95><ched>System<ched>Math Symbol<lcel><lcel><nl><ched>System<ched>Precision<ched>Recall<ched>F-score<nl><rhed>ScanSSD †<fcel>0.889<fcel>0.965<fcel>0.925<nl><rhed>InftyReader *<fcel>0.971<fcel>0.946<fcel>0.958<nl><rhed>ME U-Net *<fcel>0.973<fcel>0.950<fcel>0.961<nl><caption><loc_34><loc_29><loc_245><loc_48>TABLE III: Benchmarking ScanSSD at the Character Level [4]. Note differences in data sets and evaluation techniques (see main text).</caption></otsl>
<footnote><loc_44><loc_97><loc_105><loc_102>* Used GTDB dataset</footnote>
<unordered_list><list_item><loc_44><loc_102><loc_142><loc_107>† Used TFD-ICDAR2019v2 dataset</list_item>
</unordered_list>
<chart><loc_40><loc_119><loc_243><loc_205><bar_chart><caption><loc_34><loc_214><loc_245><loc_219>Fig. 7: Document-level results, IOU ≥ 0 . 5 and IOU ≥ 0 . 75 .</caption></chart>
<text><loc_34><loc_235><loc_245><loc_325>Math symbol detection. To measure math detection at the symbol (character) level, we consider all characters located within formula detections as 'math' characters. Our method has 0.9652 recall and 0.889 precision at the character level, resulting in a 0.925 f-score. This benchmarks well against recent results on the GTDB dataset (see Table III). Note that the detection targets (formulas for ScanSSD vs. characters), datasets, and evaluation protocols are different (1000 regions per test page are randomly sampled in Ohayama et al. [4]), and so the measures are not directly comparable. The lower precision for character detection in ScanSSD may be an artifact of predicting formulas rather than individual characters.</text>
<text><loc_34><loc_327><loc_245><loc_396>The difference betweeen ScanSSD's math symbol detection f-score and formula detection f-score is primarily due to merging and splitting formula regions, which themselves are often valid subexpressions. Merging and splitting valid formula regions often produces regions too large or too small to satisfy the IOU matching criteria, leading to lower scores. Merging occurs in part because formula detections in neighboring text lines may overlap, and splitting may occur because large formulas have features similar to separate formulas within windowed sub-images.</text>
<section_header_level_1><loc_34><loc_405><loc_109><loc_410>C. Qualitative results</section_header_level_1>
<text><loc_34><loc_415><loc_245><loc_427><loc_255><loc_31><loc_466><loc_114>Figure 6 provides example ScanSSD detection results. ScanSSD can detect math regions of arbitrary size, from a single character to hundreds of characters. It also detects matrices and correctly rejects equation numbers, page numbers, and other numbers not belonging to formulas. Figure 6 shows some example of detection errors. When there is a large space between characters within a formula (e.g., for variable constraints shown in the third panel of Figure 6), ScanSSD may split the formula and generate multiple detections (shown with pink boxes). Second, when formulas are close to each other, our method may merge them (shown with green boxes in Figure 6). Another error not shown, was wide embedded graphs (visually similar to functions) being detected as math formulas.</text>
<text><loc_255><loc_117><loc_466><loc_157>On examination, it turns out that most detection 'failures' are because of valid detections merged or split in the manner described, and not spurious detections or false negatives. A small number of these are seen in Figure 6 using red and yellow boxes; note that all but one false negative are isolated symbols.</text>
<section_header_level_1><loc_327><loc_167><loc_394><loc_172>VII. CONCLUSION</section_header_level_1>
<text><loc_255><loc_179><loc_466><loc_248>In this paper we make two contributions: 1) modifying the GTDB datasets to compensate for differences in scale and translation found in the publicly available versions of PDFs in the collection, creating new bounding box annotations for math expressions, and 2) the ScanSSD architecture for detecting math expressions in document images without using page layout, font, or character information. The method is simple but effective, applying a Single-Shot Detector (SSD) using a sliding window, followed by voting-based pooling across windows and scales.</text>
<text><loc_255><loc_250><loc_466><loc_283>Through our experiments, we observed that 1) carefully selected default boxes improves formula detection, 2) kernels of size 1 × 5 yield rectangular receptive fields that better-fit wide math expressions with larger aspect ratios, and avoid noise that square-shaped receptive fields introduce.</text>
<text><loc_255><loc_286><loc_466><loc_362>A key difference between formula detection in typeset documents and object detection in natural scenes is that typeset documents avoid occlusion of content by design. This constraint may help us design a better algorithm for non-maximal suppression, as the original non-maximal suppression algorithm is designed to handle overlapping objects. Also, we would like to use a modified version of the pooling methods based on agglomerative clustering such as the fusion algorithm introduced by Yu et al. [34]. We believe improved pooling will reduce the number of over-merged and split detections, improving both precision and recall.</text>
<text><loc_255><loc_364><loc_466><loc_405>In our current architecture, we use a fixed pooling method; we plan to design an architecture where we can train the model end-to-end to learn pooling parameters directly from data. ScanSSD allows the use of multiple classes, and we would also like to explore detecting multiple page objects in a single framework.</text>
<text><loc_255><loc_414><loc_466><loc_427>Acknowledgements. This material is based upon work supported by the Alfred P. Sloan Foundation under Grant</text>
<page_break>
<text><loc_34><loc_31><loc_245><loc_43>No. G-2017-9827 and the National Science Foundation (USA) under Grant No. IIS-1717997.</text>
<section_header_level_1><loc_116><loc_54><loc_163><loc_59>REFERENCES</section_header_level_1>
<ordered_list><list_item><loc_37><loc_66><loc_245><loc_81>K. Davila, R. Joshi, S. Setlur, V . Govindaraju, and R. Zanibbi, 'Tangentv: Math formula image search using line-of-sight graphs,' in ECIR , ser. LNCS, vol. 11437, pp. 681-695.</list_item>
<list_item><loc_37><loc_82><loc_245><loc_97>C. Clark and S. Divvala, 'Pdffigures 2.0: Mining figures from research papers,' in 2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL) . IEEE, 2016, pp. 143-152.</list_item>
<list_item><loc_37><loc_99><loc_245><loc_113>W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, 'SSD: Single shot multibox detector,' in European conference on computer vision . Springer, 2016, pp. 21-37.</list_item>
<list_item><loc_37><loc_115><loc_245><loc_129>W. Ohyama, M. Suzuki, and S. Uchida, 'Detecting mathematical expressions in scientific document images using a u-net trained on a diverse dataset,' IEEE Access , vol. 7, pp. 144 030-144 042, 2019.</list_item>
<list_item><loc_37><loc_131><loc_245><loc_151>M. Mahdavi, R. Zanibbi, H. Mouchere, and U. Garain, 'ICDAR 2019 CROHME + TFD: Competition on recognition of handwritten mathematical expressions and typeset formula detection,' in ICDAR 2019) . IEEE.</list_item>
<list_item><loc_37><loc_153><loc_245><loc_167>X. Lin, L. Gao, Z. Tang, X. Lin, and X. Hu, 'Mathematical formula identification in pdf documents,' in 2011 International Conference on Document Analysis and Recognition . IEEE, 2011, pp. 1419-1423.</list_item>
<list_item><loc_37><loc_169><loc_245><loc_194>M. Mahdavi, M. Condon, K. Davila, and R. Zanibbi, 'LPGA: Line-of-sight parsing with graph-based attention for math formula recognition,' in Proc. International Conference on Document Analysis and Recognition . Sydney, Australia: IAPR, September 2019, pp. 647654.</list_item>
<list_item><loc_37><loc_196><loc_245><loc_210>M. Condon, 'Applying hierarchical contextual parsing with visual density and geometric features to typeset formula recognition,' Master's thesis, Rochester Institute of Technology, Rochester, NY, USA, 2017.</list_item>
<list_item><loc_37><loc_212><loc_245><loc_227>Y. Deng, A. Kanervisto, J. Ling, and A. M. Rush, 'Imageto-markup generation with coarse-to-fine attention,' arXiv preprint arXiv:1609.04938 , 2016.</list_item>
<list_item><loc_34><loc_228><loc_245><loc_248>J. Zhang, J. Du, and L. Dai, 'Track, attend, and parse (tap): An end-to-end framework for online handwritten mathematical expression recognition,' IEEE Transactions on Multimedia , vol. 21, no. 1, pp. 221233, 2018.</list_item>
<list_item><loc_34><loc_250><loc_245><loc_270>F. Alvaro and R. Zanibbi, 'A shape-based layout descriptor for classifying spatial relationships in handwritten math,' in Proceedings of the 2013 ACM symposium on Document engineering . ACM, 2013, pp. 123-126.</list_item>
<list_item><loc_34><loc_271><loc_245><loc_291>J. Zhang, J. Du, S. Zhang, D. Liu, Y. Hu, J. Hu, S. Wei, and L. Dai, 'Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition,' Pattern Recognition , vol. 71, pp. 196-206, 2017.</list_item>
<list_item><loc_34><loc_293><loc_245><loc_308>L. Pfahler, J. Schill, and K. Morik, 'The search for equations - learning to identify similarities between mathematical expressions,' in Proc. ECML-PKDD , 2019.</list_item>
<list_item><loc_34><loc_309><loc_245><loc_324>U. Garain and B. B. Chaudhuri, 'Ocr of printed mathematical expressions,' in Digital Document Processing . Springer, 2007, pp. 235-259.</list_item>
<list_item><loc_34><loc_325><loc_245><loc_346>K. Iwatsuki, T. Sagara, T. Hara, and A. Aizawa, 'Detecting in-line mathematical expressions in scientific documents,' in Proceedings of the 2017 ACM Symposium on Document Engineering . ACM, 2017, pp. 141-144.</list_item>
<list_item><loc_34><loc_347><loc_245><loc_367>X. Lin, L. Gao, Z. Tang, X. Hu, and X. Lin, 'Identification of embedded mathematical formulas in pdf documents using svm,' in Document Recognition and Retrieval XIX , vol. 8297. International Society for Optics and Photonics, 2012, p. 82970D.</list_item>
<list_item><loc_34><loc_369><loc_245><loc_389>D. M. Drake and H. S. Baird, 'Distinguishing mathematics notation from english text using computational geometry,' in Eighth International Conference on Document Analysis and Recognition (ICDAR'05) . IEEE, 2005, pp. 1270-1274.</list_item>
<list_item><loc_34><loc_390><loc_245><loc_410>L. Gao, X. Yi, Y. Liao, Z. Jiang, Z. Yan, and Z. Tang, 'A deep learningbased formula detection method for pdf documents,' in 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , vol. 1. IEEE, 2017, pp. 553-558.</list_item>
<list_item><loc_34><loc_412><loc_245><loc_426>B. Chaudhuri and U. Garain, 'An approach for processing mathematical expressions in printed document,' in International Workshop on Document Analysis Systems . Springer, 1998, pp. 310-321.</list_item>
<list_item><loc_255><loc_32><loc_466><loc_52>U. Garain and B. Chaudhuri, 'A syntactic approach for processing mathematical expressions in printed documents,' in Proceedings 15th International Conference on Pattern Recognition. ICPR-2000 , vol. 4. IEEE, 2000, pp. 523-526.</list_item>
<list_item><loc_255><loc_53><loc_466><loc_73>A. Kacem, A. Bela¨ ıd, and M. B. Ahmed, 'Automatic extraction of printed mathematical formulas using fuzzy logic and propagation of context,' International Journal on Document Analysis and Recognition , vol. 4, no. 2, pp. 97-108, 2001.</list_item>
<list_item><loc_255><loc_75><loc_466><loc_89>G. Nagy and S. Seth, 'Hierarchical representation of optically scanned documents,' in Proc. Seventh Int'l Conf. Pattern Recognition , Montreal, Canada, 1984, pp. 347-349.</list_item>
<list_item><loc_255><loc_90><loc_466><loc_100>K. Simonyan and A. Zisserman, 'Very deep convolutional networks for large-scale image recognition,' arXiv preprint arXiv:1409.1556 , 2014.</list_item>
<list_item><loc_255><loc_101><loc_466><loc_121>R. Girshick, J. Donahue, T. Darrell, and J. Malik, 'Rich feature hierarchies for accurate object detection and semantic segmentation,' in Proceedings of the IEEE conference on computer vision and pattern recognition , 2014, pp. 580-587.</list_item>
<list_item><loc_255><loc_122><loc_466><loc_132>R. Girshick, 'Fast r-cnn,' in Proceedings of the IEEE international conference on computer vision , 2015, pp. 1440-1448.</list_item>
<list_item><loc_255><loc_133><loc_466><loc_148>S. Ren, K. He, R. Girshick, and J. Sun, 'Faster r-cnn: Towards real-time object detection with region proposal networks,' in Advances in neural information processing systems , 2015, pp. 91-99.</list_item>
<list_item><loc_255><loc_149><loc_466><loc_169>J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, 'You only look once: Unified, real-time object detection,' in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 779788.</list_item>
<list_item><loc_255><loc_170><loc_466><loc_196>J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama et al. , 'Speed/accuracy trade-offs for modern convolutional object detectors,' in Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp. 73107311.</list_item>
<list_item><loc_255><loc_197><loc_466><loc_212>M. Liao, B. Shi, X. Bai, X. Wang, and W. Liu, 'Textboxes: A fast text detector with a single deep neural network,' in Thirty-First AAAI Conference on Artificial Intelligence , 2017.</list_item>
<list_item><loc_255><loc_213><loc_466><loc_233>M. Suzuki, F. Tamari, R. Fukuda, S. Uchida, and T. Kanahori, 'Infty: an integrated ocr system for mathematical documents,' in Proceedings of the 2003 ACM symposium on Document engineering . ACM, 2003, pp. 95-104.</list_item>
<list_item><loc_255><loc_234><loc_466><loc_249>P. Mali, 'Scanning single shot detector for math in document images,' Master's thesis, Rochester Institute of Technology, Rochester, NY, USA, August 2019.</list_item>
<list_item><loc_255><loc_250><loc_466><loc_265>T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ ar, 'Focal loss for dense object detection,' in Proceedings of the IEEE international conference on computer vision , 2017, pp. 2980-2988.</list_item>
<list_item><loc_255><loc_266><loc_466><loc_281>J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, 'Imagenet: A large-scale hierarchical image database,' in 2009 IEEE conference on computer vision and pattern recognition . IEEE, 2009, pp. 248-255.</list_item>
<list_item><loc_255><loc_282><loc_466><loc_297>Z. Yu, S. Lyu, Y. Lu, and P. S. Wang, 'A fusion strategy for the single shot text detector,' in 2018 24th International Conference on Pattern Recognition (ICPR) . IEEE, 2018, pp. 3687-3691.</list_item>
</ordered_list>
</doctag>