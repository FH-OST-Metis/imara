<doctag><page_header><loc_15><loc_116><loc_30><loc_344>arXiv:2401.06326v4  [math.ST]  4 Mar 2025</page_header>
<section_header_level_1><loc_117><loc_89><loc_383><loc_141>Optimal linear prediction with functional observations: Why you can use a simple post-dimension reduction estimator</section_header_level_1>
<section_header_level_1><loc_223><loc_149><loc_275><loc_155>Won-Ki Seo ∗</section_header_level_1>
<text><loc_217><loc_160><loc_281><loc_165>University of Sydney</text>
<text><loc_137><loc_170><loc_362><loc_241>Abstract: We study the optimal linear prediction of a random function that takes values in an infinite dimensional Hilbert space. We begin by characterizing the mean square prediction error (MSPE) associated with a linear predictor and discussing the minimal achievable MSPE. This analysis reveals that, in general, there are multiple non-unique linear predictors that minimize the MSPE, and even if a unique solution exists, consistently estimating it from finite samples is generally impossible. Nevertheless, we can define asymptotically optimal linear operators whose empirical MSPEs approach the minimal achievable level as the sample size increases. We show that, interestingly, standard post-dimension reduction estimators, which have been widely used in the literature, attain such asymptotic optimality under minimal conditions.</text>
<text><loc_137><loc_246><loc_362><loc_263>MSC2020 subject classifications: Primary 60G25; secondary 62J99. Keywords and phrases: linear prediction, functional data, functional linear models, regularization.</text>
<section_header_level_1><loc_109><loc_276><loc_146><loc_282>Contents</section_header_level_1>
<otsl><loc_109><loc_291><loc_390><loc_402><fcel>1<fcel>Introduction . .<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2<nl><fcel>2<fcel>Optimal linear prediction in Hilbert space . . . . . . . . . . . .<fcel>. . . . 3<nl><ecel><fcel>2.1<fcel>Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . 3<nl><ecel><fcel>2.2<fcel>. . . . Linear prediction in H . . . . . . . . . . . . . . . . . . .<nl><fcel>3<fcel>. . . . 4 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6<lcel><nl><fcel>4<fcel>Discussions and extensions . . . . . . . . . . . . . . . . . . . . . . . . . 9<lcel><nl><ecel><fcel>4.1 A<fcel>more general result . . . . . . . . . . . . . . . . . . . . . . . . 9<nl><ecel><fcel>4.2<fcel>Misspecified functional linear models and OLPO . . . . . . . . . 9<nl><ecel><fcel>4.3<fcel>Requirement of sufficient dimension reduction . . . . . . . . . . . 10<nl><ecel><fcel>4.4<fcel>Dimension reduction of the target variable . . . . . . . . . . . . . 10<nl><fcel>5<fcel>Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<fcel>11<nl><fcel>6<fcel>Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13<lcel><nl><fcel>A<fcel>Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15<lcel><nl><ecel><fcel>A.1 Useful<fcel>lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15<nl></otsl>
<footnote><loc_119><loc_407><loc_246><loc_412>∗ Won-Ki Seo is the corresponding author.</footnote>
<page_footer><loc_248><loc_423><loc_251><loc_428>1</page_footer>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>2</page_header>
<otsl><loc_109><loc_73><loc_390><loc_86><fcel>B<fcel>Additional simulation results for alternative estimators . . . . . . . . . 20<nl><fcel>References<fcel>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23<nl></otsl>
<section_header_level_1><loc_109><loc_101><loc_173><loc_107>1. Introduction</section_header_level_1>
<text><loc_109><loc_116><loc_390><loc_160>We study the optimal linear prediction in an arbitrary Hilbert space H , and how to estimate the optimal linear predictor. Given recent developments in functional data analysis, studies on this subject hold significant importance and relevance to many empirical applications; see e.g., [1], [2], [17], [18] and [23] to name only a few recent papers. The reader is also referred to [8], [9] and [22] containing an earlier mathematical exploration of this subject.</text>
<text><loc_109><loc_161><loc_391><loc_191>Let { Y t } t ≥ 1 and { X t } t ≥ 1 be stationary sequences of mean-zero random elements, both taking values in H . Suppose that ˜ Y t = A o X t , where A o is a continuous linear operator, and it satisfies the following: for any arbitrary continuous linear operator B ,</text>
<formula><loc_164><loc_197><loc_391><loc_213></formula>
<text><loc_109><loc_212><loc_392><loc_344>where ‖ · ‖ is the norm defined on H . We refer to ˜ Y t = A o X t as an optimal linear predictor (OLP) and A o as an optimal linear prediction operator (OLPO). Given the observations { Y t , X t } T t =1 , practitioners are often interested in constructing a predictor ̂ Y t that converges (in probability) to an OLP as T increases. This is straightforward when H = R , and Y t and X t are real-valued mean-zero random variables with positive variances. In this case, it is well known that the unique solution to (1.1) is achieved by A o = E [ Y t X t ] / E [ X 2 t ] and the minimal achievable mean squared prediction error (MSPE) is E ‖ Y t -A o X t ‖ 2 = E [ Y 2 t ] -( E [ Y t X t ]) 2 / E [ X 2 t ]. The conventional plug-in-type estimator (or OLS estimator) of A o may be defined by ̂ A = T -1 ∑ T t =1 Y t X t /T -1 ∑ T t =1 X 2 t . When the weak law of large numbers holds for { Y 2 t } t ≥ 1 , { X 2 t } t ≥ 1 and { X t Y t } t ≥ 1 , we find that the following two results hold: (i) ̂ A → p A o = E [ X t Y t ] / E [ X 2 t ] and (ii) T -1 ∑ T t =1 ( Y t -̂ AX t ) 2 → p E [ Y 2 t ] -( E [ X t Y t ]) 2 / E [ X 2 t ], where and hereafter → p denotes the convergence in probability with respect to the norm of H . That is, a consistent estimator of the OLPO can be constructed from the given observations. Furthermore, the empirical MSPE obtained from the estimator converges to the minimal achievable MSPE.</text>
<text><loc_109><loc_346><loc_391><loc_379>However, in a more general situation where Y t and X t take values in a possibly infinite dimensional H , it is generally impossible to obtain parallel results. To see this with a simple example, suppose that { Y t , X t } t ≥ 1 satisfies the following: for t ≥ 1,</text>
<formula><loc_128><loc_377><loc_391><loc_396></formula>
<text><loc_109><loc_404><loc_390><loc_417>where ε t is independent of X t and { f j } j ≥ 1 is an orthonormal basis of H . A o specified in (1.2) is obviously the OLPO, but A o is not consistently estimable in</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>3</page_header>
<text><loc_109><loc_73><loc_390><loc_133>general. As will be detailed in Example 1, this is because, it is not possible to estimate a j for j > T from T observations unless (i) a simplifying condition on A , such as a j = a for all j ≥ m for some finite m , holds and (ii) researchers are aware of this condition and use it appropriately. Even in the simple case where Y t = ¯ A o X t + ε t with ¯ A o = aI ( I denotes the identity map) for a ∈ R , consistent estimation of ¯ A o is impossible for the same reason if we do not know such a simple structure of ¯ A o and hence allow a more general case given in (1.2) (note that, ¯ A o = aI is a special case of A o in (1.2) with a j = a ).</text>
<text><loc_109><loc_134><loc_392><loc_284>Does this mean that it is impossible to statistically solve the optimal linear prediction problem in this general setting? The answer is no. We will demonstrate that, under mild conditions, there exists the minimum MSPE achievable by a linear predictor and it is feasible to construct a possibly inconsistent estimator ̂ A such that the empirical MSPE, computed as T -1 ∑ T t =1 ‖ Y t -̂ AX t ‖ 2 , converges to the minimum MSPE. Particularly, we show that a standard post dimension-reduction estimator, obtained by (i) reducing the dimensionality of the predictive variable X t using the principal directions of its sample covariance and then (ii) applying the least squares method to estimate the linear relationship between the resulting lower dimensional predictive variable and Y t , is effective for linear prediction. This post dimension-reduction estimator has been widely used due to its simplicity. Its statistical properties have been studied under technical assumptions, which are challenging to verify, such as those concerning the eigenstructure of the covariance of X t ; the reader is referred to, e.g., [13] and [26], where the assumptions of [12] are adopted for function-onfunction regression models. We show that, without such assumptions, a naive use of this simple post dimension-reduction estimator can be justified as a way to obtain a solution which asymptotically minimizes the MSPE. We also extend this finding to show that similar estimators, which involve further dimension reduction of Y t , also possess this desirable property under mild conditions.</text>
<text><loc_109><loc_285><loc_392><loc_336>The paper proceeds as follows: Section 2 characterizes the minimal achievable MSPE by a linear predictor, followed by a discussion on the estimation of an asymptotically optimal predictor in Section 3. Further discussions and extensions are given in Section 4. Section 5 provides simulation evidence of our theoretical findings, and Section 6 contains concluding remarks. The appendix includes mathematical proofs of the theoretical results and some additional simulation results.</text>
<section_header_level_1><loc_109><loc_349><loc_294><loc_355>2. Optimal linear prediction in Hilbert space</section_header_level_1>
<section_header_level_1><loc_109><loc_364><loc_185><loc_370>2.1. Preliminaries</section_header_level_1>
<text><loc_109><loc_379><loc_392><loc_420>For the subsequent discussion, we introduce notation. Let H be a separable Hilbert space with inner product 〈· , ·〉 and norm ‖ · ‖ . For V ⊂ H , let V ⊥ be the orthogonal complement to V . We let L ∞ be the set of continuous linear operators, and let ‖T ‖ ∞ , T ∗ , ran T , and ker T denote the operator norm, adjoint, range, and kernel of T , respectively. T is self-adjoint if T = T ∗ . T is</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>4</page_header>
<text><loc_109><loc_73><loc_392><loc_185>called nonnegative if 〈T x, x 〉 ≥ 0 for any x ∈ H , and positive if also 〈T x, x 〉 /negationslash = 0 for any x ∈ H \ { 0 } . For x, y ∈ H , we let x ⊗ y be the operator given by z ↦→ 〈 x, z 〉 y for z ∈ H . T ∈ L ∞ is compact if T = ∑ j ≥ 1 a j v j ⊗ w j for some orthonormal bases { v j } j ≥ 1 and { w j } j ≥ 1 and a sequence of nonnegative numbers { a j } j ≥ 1 tending to zero; if T is also self-adjoint and nonnegative (see [7], p. 35), we may assume that v j = w j . For any compact T ∈ L ∞ and p ∈ N , let ‖T ‖ S p be defined by ‖T ‖ p S p = ∑ ∞ j =1 a p j and let S p be the set of compact operators T with ‖T ‖ S p < ∞ ; S p is called the Schatten p -class. S 1 (resp. S 2 ) is also referred to the trace (resp. Hilbert-Schmidt) class. It is known that the following hold: ‖T ‖ ∞ ≤ ‖T ‖ S 2 ≤ ‖T ‖ S 1 and ‖T ‖ 2 S 2 = ∑ ∞ j =1 ‖T w j ‖ 2 for any orthonormal basis { w j } j ≥ 1 . For any H -valued mean-zero random elements Z and ˜ Z with E ‖ Z ‖ 2 < ∞ and E ‖ ˜ Z ‖ 2 < ∞ , their cross-covariance C Z ˜ Z = E [ Z ⊗ ˜ Z ] is a Schatten 1-class operator; if Z = ˜ Z , it reduces to the covariance of Z and E ‖ Z ‖ 2 = ‖ C ZZ ‖ S 1 holds.</text>
<section_header_level_1><loc_109><loc_193><loc_223><loc_199>2.2. Linear prediction in H</section_header_level_1>
<text><loc_109><loc_208><loc_390><loc_237>Consider a weakly stationary sequence { Y t , X t } t ≥ 1 with nonzero covariances C Y Y = E [ Y t ⊗ Y t ] and C XX = E [ X t ⊗ X t ], along with the cross-covariance C XY = E [ X t ⊗ Y t ] (or equivalently C ∗ Y X ). We hereafter write C Y Y and C XX as their spectral representations as follows, if necessary:</text>
<formula><loc_164><loc_240><loc_391><loc_259></formula>
<text><loc_109><loc_266><loc_390><loc_303>where κ 1 ≥ κ 2 ≥ . . . ≥ 0, λ 1 ≥ λ 2 ≥ . . . ≥ 0, and { u j } j ≥ 1 and { v j } j ≥ 1 are orthonormal sets of H . Unless otherwise stated, we assume that C Y Y and C XX are not finite rank operators and thus there are infinitely many nonzero eigenvalues in(2.1), which is as usually assumed for covariances of Hilbert-valued random elements in the literature on functional data analysis.</text>
<text><loc_109><loc_303><loc_390><loc_322>Note first that, for any B ∈ L ∞ , E ‖ Y t -BX t ‖ 2 = ‖ E [( Y t -BX t ) ⊗ ( Y t -BX t )] ‖ S 1 and hence the MSPE associated with B can be written as follows:</text>
<formula><loc_143><loc_324><loc_391><loc_336></formula>
<text><loc_109><loc_339><loc_390><loc_361>Next, we provide the main result of this section, which not only gives us a useful characterization of the MSPE in (2.2), but also provides essential preliminary results for the subsequent discussion.</text>
<text><loc_109><loc_366><loc_390><loc_382>Proposition 2.1. For any B ∈ L ∞ , there exists a unique element R XY ∈ L ∞ such that C XY = C 1 / 2 Y Y R XY C 1 / 2 XX and</text>
<formula><loc_113><loc_388><loc_391><loc_400></formula>
<text><loc_109><loc_403><loc_391><loc_417>Proposition 2.1 shows that the MSPE associated with B ∈ L ∞ is the sum of the Schatten 1- and 2-norms of specific operators dependent on C Y Y , C XX ,</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>5</page_header>
<text><loc_109><loc_72><loc_391><loc_117>R XY and B ; notably, only the latter term ( ‖ BC 1 / 2 XX -C 1 / 2 Y Y R XY ‖ 2 S 2 ) in (2.3) depends on B . Thus, the former term ( ‖ C Y Y -C 1 / 2 Y Y R XY R ∗ XY C 1 / 2 Y Y ‖ S 1 ) represents the minimal achievable MSPE by a linear predictor, while the latter can be understood as a measure of the inadequacy of B as a linear predictor. If { w j } j ≥ 1 is an orthonormal basis of H , this inadequacy becomes zero if and only if</text>
<formula><loc_168><loc_118><loc_391><loc_130></formula>
<text><loc_109><loc_132><loc_391><loc_170>Based on these findings, we obtain the following two characterizations of an OLPO in Corollary 2.1: the first is a direct consequence of (2.4) and the HahnBanach extension theorem (see, e.g., Theorem 1.9.1 of [21]), which, in turn, implies the second due to the fact that C XY = C 1 / 2 Y Y R XY C 1 / 2 XX as observed in Proposition 2.1.</text>
<text><loc_109><loc_174><loc_390><loc_190>Corollary 2.1. A is an OLPO if and only if any of the following equivalent conditions holds: (a) AC 1 / 2 XX = C 1 / 2 Y Y R XY and (b) AC XX = C XY .</text>
<text><loc_109><loc_194><loc_391><loc_314>Condition ( b ) follows directly from condition ( a ) and Proposition 2.1, and it is notably align with the characterization of an OLPO provided by [8]; Remark 2.1 outlines the distinctions between our findings and the existing result in more detail. From Corollary 2.1, we find that the minimum MSPE is attained by A ∈ L ∞ satisfying AC XX = C XY (or AC 1 / 2 XX = C 1 / 2 Y Y R XY ). However, such an operator A is not uniquely determined; particularly, the equation does not specify how A acts on ker C XX , allowing A to agree with any arbitrary element in L ∞ on ker C XX (see Remark 2.2). When C XX is not injective, there are multiple choices of A that achieve the minimum MSPE. In infinite dimensional settings, the injectivity of C XX is a stringent assumption, and verifying this condition from finite observations is impractical. Thus, pursuing prediction under the existence of the unique OLPO, as in the standard univariate or multivariate prediction, is restrictive. Even if a different setup is considered with a different purpose, similar concerns about the requirements for unique identification were recently raised by [3], and the enthusiastic reader is referred to their paper for more detailed discussion on the topic.</text>
<text><loc_109><loc_319><loc_391><loc_386>Remark 2.1. Condition ( b ) in Proposition 2.1 was earlier obtained as the requirement for A ∈ L ∞ to be an OLPO by Propositions 2.2-2.3 of [8]. Compared with this earlier result, Proposition 2.1 not only provides more general results, such as the explicit expression of the gap between the minimal attainable MSPE and the MSPE associated with any B ∈ L ∞ , but it also employs a distinct approach. The result of [8] relies on the notion of a linearly closed subspace and an extension of the standard projection theorem, while Proposition 2.1 is established by an algebraic proof based on the representation of cross-covariance operators in [4].</text>
<text><loc_109><loc_390><loc_391><loc_427>Remark 2.2. By invoking the Hahn-Banach extension theorem (Theorem 1.9.1 of [21]), we may assume that A satisfying AC XX = C XY is a unique continuous linear map defined on the closure of ran C XX , which is not equal to H if C XX is not injective. Thus, if there exists another continuous linear operator ˜ A which</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>6</page_header>
<text><loc_109><loc_72><loc_390><loc_96>agrees with A on the closure of ran C XX but not on [ran C XX ] ⊥ , then ˜ A also satisfies that E ‖ Y t -˜ AX t ‖ 2 = ‖ C Y Y -C 1 / 2 Y Y R XY R ∗ XY C 1 / 2 Y Y ‖ S 1 .</text>
<text><loc_109><loc_94><loc_392><loc_145>The results given in Proposition 2.1 and Remark 2.2 imply that, particularly when the predictive variable is function-valued, there may be multiple OLPOs that satisfy (1.1). Furthermore, even if a unique OLPO exists, it may not be consistently estimable; a more detailed discussion is given in Example 1 below. This means that we are in a somewhat different situation from the previous simple univariate case discussed in Section 1, where we can estimate the OLP by consistently estimating the OLPO.</text>
<text><loc_109><loc_150><loc_392><loc_229>Example 1. Suppose that { Y t , X t } t ≥ 1 satisfies (1.2), X t has a positive covariance C XX , and ε t is serially independent and also independent of X s for any s . In this case, A o C XX = C XY , making A o an OLPO. Since C XX is injective, any continuous linear operator ˜ A agrees with A o on the closure of ran C XX also agrees with A o on H (see Remark 2.2 and note that the closure of ran C XX is H in this case). However, consistently estimating A o without any further assumptions is impossible. To illustrate this, we may consider the case where { f j } j ≥ 1 in (1.2) are known for simplicity. Even in this simplified scenario, there are infinitely many unknown parameters { a j } j ≥ 1 to be estimated from only T samples, necessitating additional assumptions on { a j } j ≥ 1 for consistent estimation.</text>
<section_header_level_1><loc_109><loc_237><loc_166><loc_243>3. Estimation</section_header_level_1>
<text><loc_109><loc_252><loc_392><loc_296>We observed that in a general Hilbert space setting, there can be not only multiple OLPOs but also instances where, even if a unique OLPO exists, consistent estimation of it is impossible. Nevertheless, under mild conditions, we may construct a predictor using a standard post dimension-reduction estimator in such a way that the associated empirical MSPE converges to the minimum MPSE as in the simple univariate case. To propose such a predictor, let</text>
<formula><loc_111><loc_303><loc_391><loc_331></formula>
<text><loc_109><loc_338><loc_390><loc_364>where k T is an integer satisfying the following assumption: below, we let a 1 ∧ a 2 = min { a 1 , a 2 } for a 1 , a 2 ∈ R and assume that max j ≥ 1 { j : E j } = 1 if the condition E j is not satisfied for all j ≥ 1.</text>
<text><loc_109><loc_364><loc_313><loc_371>Assumption 1 (Elbow-like rule) . k T in (3.1) is given by</text>
<formula><loc_181><loc_373><loc_318><loc_391></formula>
<text><loc_109><loc_397><loc_390><loc_423>where τ T and υ T are user-specific choices of positive constants decaying to 0 as T → ∞ , and both τ -1 T and υ -1 T are bounded above by γ 0 T γ 1 for some γ 0 > 0 and γ 1 ∈ (0 , 1 / 2) .</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>7</page_header>
<text><loc_109><loc_72><loc_392><loc_228>̂ C -1 XX,k T in(3.1)is understood as the inverse of ̂ C XX viewed as a map acting on the restricted subspace span { ˆ v j } k T j =1 and k T in Assumption 1 is a random integer by its construction (see Remark 3.1 below). By including υ T in Assumption 1, we ensure that k T becomes o p ( T 1 / 2 ), which facilitates our theoretical analysis. Even if the choice of k T depends on various contexts requiring a regularized inverse of ̂ C XX , it is commonly set to a much smaller number than T , and thus this condition does not impose any practical restrictions. A practically more meaningful decision is made by the first component, max j ≥ 1 { j : ˆ λ j ≥ ˆ λ j +1 + τ T } in Assumption 1. Firstly, given that ˆ λ k T +1 > 0, this condition implies that ˆ λ -1 k T < τ -1 T , and thus ‖ ̂ C -1 XX,k T ‖ ∞ ≤ τ -1 T , where τ -1 T diverges slowly compared to T ; clearly, this is one of the essential requirements for k T (as the rank of a regularized inverse of ̂ C XX ) to satisfy in the literature employing similar regularized inverses. Secondly, k T is determined near the point where the gap ˆ λ j -ˆ λ j +1 is no longer smaller than a specified threshold τ T for the last time. This approach is, in fact, analogous to the standard elbow rule used to determine the number of principal components in multivariate analysis based on the scree plot. Thus, even if Assumption 1 details some specific mathematical requirements necessary for our asymptotic analysis, these seem to closely align with existing practical rules for selecting k T , commonly employed in current practice; for example, see [10].</text>
<text><loc_109><loc_228><loc_390><loc_244>Using the regularized inverse ̂ C -1 XX,k T , the proposed predictor is constructed as follows:</text>
<formula><loc_115><loc_249><loc_391><loc_276></formula>
<text><loc_109><loc_283><loc_391><loc_335>The above predictor is standard in the literature on functional data analysis as the least squares predictor of Y t given the projection of X t onto the space spanned by the eigenvectors corresponding to the first k T largest eigenvalues; a similar estimator was earlier considered by [23]. The predictor described in (3.2) and its modifications (such as those that will be considered in Section 4.4) have been widely studied in the literature and adapted to various contexts; see e.g., [1], [2], [7], [17], [20], [27] and [28].</text>
<text><loc_109><loc_339><loc_391><loc_386>Remark 3.1. A significant difference in ̂ A compared to most of the existing estimators, lies in the choice of k T . In many earlier articles, k T is directly chosen by researchers and thus regarded as deterministic. However, as pointed out by [26], even in this case, it is generally not recommended to choose k T arbitrarily without taking the eigenvalues ˆ λ j into account. Therefore, it is natural to view k T as random from a practical point of view.</text>
<text><loc_109><loc_389><loc_391><loc_419>To establish consistency of ̂ A in(3.2), certain assumptions have been employed in the aforementioned literature, particularly concerning the eigenstructure of C XX and the unique identification of the target estimator A . However, as illustrated by Example 1 and Remark 2.2, these assumptions are not guaranteed to</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>8</page_header>
<text><loc_109><loc_73><loc_391><loc_117>hold even in cases where there exists a well defined OLPO. Thus, in this paper, we do not make such assumptions for consistency, but allow ̂ A to potentially be inconsistent. Our main result in this section is that, despite not relying on the typical assumptions for consistency, the predictor ̂ Y t asymptotically minimizes the MSPE, which only requires mild conditions on the sample (cross-)covariance operators.</text>
<text><loc_119><loc_119><loc_375><loc_129>For the subsequent discussion, we define the following: for any B ∈ L ∞ ,</text>
<formula><loc_178><loc_132><loc_321><loc_151></formula>
<text><loc_109><loc_154><loc_396><loc_188>Note that ‖ Σ( B ) ‖ S 1 is equivalent to the empirical MSPE, given by T -1 ∑ T t =1 ‖ Y t -BX t ‖ 2 . We then define an asymptotically OLPO as a random bounded linear operator producing a predictor that asymptotically minimizes the MSPE in the following sense:</text>
<text><loc_109><loc_191><loc_391><loc_207><loc_109><loc_218><loc_390><loc_234>Definition 1. Any random bounded linear operator ̂ B is called an asymptotically OLPO if where Σ min = ‖ C Y Y -C 1 / 2 Y Y R XY R ∗ XY C 1 / 2 Y Y ‖ S 1 , which is the minimum MSPE that can be achieved by a linear predictor, as defined in Proposition 2.1.</text>
<formula><loc_192><loc_206><loc_308><loc_222></formula>
<text><loc_119><loc_239><loc_268><loc_245>We will employ the following assumption:</text>
<text><loc_109><loc_249><loc_390><loc_273>Assumption 2 (Standard rate of convergence) . ‖ ̂ C Y Y -C Y Y ‖ ∞ , ‖ ̂ C XX -C XX ‖ ∞ , and ‖ ̂ C XY -C XY ‖ ∞ are O p ( T -1 / 2 ) .</text>
<text><loc_109><loc_270><loc_390><loc_344>We observe that { Y t ⊗ Y t -C Y Y } t ≥ 1 , { X t ⊗ X t -C XX } t ≥ 1 , and { X t ⊗ Y t -C XY } t ≥ 1 are stationary sequences of Schatten 2-class operators. These operator-valued sequences may also be understood as stationary sequences in a separable Hilbert space ([7], p. 34). Then, Assumption 2 is satisfied under some non-restrictive regularity conditions; see e.g., Theorems 2.16-2.18 of [7] concerning the central limit theorems for Hilbert-valued random elements. Given that we are dealing with a weakly stationary sequence { Y t , X t } t ≥ 1 , Assumption 2 appears to be standard. Furthermore, it can be relaxed to a weaker requirement by imposing stricter conditions on τ T and υ T in Assumption 1; this will be detailed in Section 4.1.</text>
<text><loc_119><loc_345><loc_283><loc_351>We now present the main result of this paper.</text>
<text><loc_109><loc_355><loc_391><loc_378>Theorem 3.1. Under Assumptions 1-2, ̂ A is an asymptotically OLPO, i.e., ‖ Σ( ̂ A ) ‖ S 1 → p Σ min .</text>
<text><loc_109><loc_375><loc_391><loc_427>As stated, an appropriate growth rate of k T , detailed in Assumption 1, and the standard rate of convergence of the sample (cross-)covariance operators (Assumption 2) are all that we need in order to demonstrate that the proposed predictor in (3.2) is an asymptotically OLPO. As discussed earlier, since the choice rule in Assumption 1 is practically similar to existing rules for selecting k T , a naive use of the simple post-dimension reduction estimator ̂ A in (3.2)</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_387><loc_59><loc_390><loc_64>9</page_header>
<text><loc_109><loc_73><loc_392><loc_118>without the usual assumptions for the unique identification and/or consistency, which are widely employed but challenging to verity, can still be justified as a way to asymptotically minimize the MSPE (see Section 4.2). Some discussions and extensions on the above theorem are given in the next section. We also consider the case where ̂ A is replaced by another estimator employing a different regularization scheme (Sections 4.3-4.4).</text>
<section_header_level_1><loc_109><loc_130><loc_231><loc_136>4. Discussions and extensions</section_header_level_1>
<section_header_level_1><loc_109><loc_145><loc_219><loc_151>4.1. A more general result</section_header_level_1>
<text><loc_109><loc_160><loc_390><loc_190>In Assumption 2, we assumed that the sample (cross-)covariances converge to the population counterparts with √ T -rate. However, the exact √ T -rate is not mandatory for the desired result and can be relaxed if we make appropriate adjustments on τ T and υ T in Assumption 1 as follows:</text>
<text><loc_108><loc_193><loc_390><loc_225>Corollary 4.1. Suppose that, for β ∈ (0 , 1 / 2] , ‖ ̂ C Y Y -C Y Y ‖ ∞ , ‖ ̂ C XX -C XX ‖ ∞ , and ‖ ̂ C XY -C XY ‖ ∞ are O p ( T -β ) . If Assumption 1 holds for γ 1 ∈ (0 , β ) , then ̂ A is an asymptotically OLPO.</text>
<text><loc_109><loc_222><loc_390><loc_268>That is, ̂ A is an asymptotically OLPO under weaker assumptions on the sample (cross-)covariance operators if we impose stricter conditions on the decay rates of τ T and υ T . τ T and υ T are user-specific choices dependent on T , so researchers can easily manipulate their decay rates. Corollary 4.1, thus, tells us that we can make ̂ A an asymptotically OLPO under more general scenarios by simply reducing the decay rates of τ T and υ T .</text>
<section_header_level_1><loc_109><loc_280><loc_329><loc_286>4.2. Misspecified functional linear models and OLPO</section_header_level_1>
<text><loc_109><loc_295><loc_274><loc_301>Consider the standard functional linear model</text>
<formula><loc_192><loc_309><loc_391><loc_320></formula>
<text><loc_108><loc_323><loc_392><loc_413>where A is typically assumed to satisfy the following two conditions: (i) A is Hilbert-Schmidt (i.e., A ∈ S 2 ) and (ii) A is uniquely identified (in S 2 ). A common assumption employed for the unique identification is that ran C XX is dense in H (see Remark 2.2). Under additional technical assumptions on the eigenstructure of C XX , such as those on the spectral gap ( λ j -λ j -1 ) as in [12], the proposed estimator ̂ A in (3.2) turns out to be consistent if k T grows appropriately. Of course, some alternative estimators, such as the least squares estimator with Tikhonov regularization (see, e.g., [6]), do not require any assumptions on the spectral gap, which is a well known advantage of such methods. However, they still require condition (i), the Hilbert-Schmidt property of A , with some additional regularity assumptions on A , and also impose assumptions for condition (ii) when discussing the consistency of those estimators.</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>10</page_header>
<text><loc_109><loc_73><loc_392><loc_140>It is crucial to have conditions (i) and (ii) together with the model (4.1), since a violation of either of these can easily lead to inconsistency (see Example 1). While these requirements are standard for estimation, they exclude many natural data generating mechanisms in H (see Examples 1-2). Consequently, the model may suffer from misspecification issues. However, even in such cases, our results show that ̂ A attains the minimum MSPE asymptotically if k T grows at an appropriate rate, and thus affirm the potential use of the standard postdimension reduction estimator in practice without a careful examination of various technical conditions.</text>
<text><loc_109><loc_145><loc_392><loc_227>Example 2. Similar to the example given in Section 5 of [5], suppose that X t = Y t -1 and Y t satisfies the functional AR(1) law of motion: Y t = AY t -1 + ε t with A = ∑ ∞ j =1 a j w j ⊗ w j for some orthonormal basis { w j } j ≥ 1 and iid sequence { ε t } t ∈ Z (see also [2]). This leads to the following pointwise AR(1) model: 〈 Y t , w j 〉 = a j 〈 Y t -1 , w j 〉 + 〈 ε t , w j 〉 . This time series is guaranteed to be stationary if sup j | a j | < 1 (this can be demonstrated with only a slight and obvious modification of Theorem 3.1 of [7] or Proposition 3.2 of [25]). On the other hand, for A to be a Hilbert-Schmidt operator, a much more stringent condition ∑ ∞ j =1 | a j | 2 < ∞ is required, and, as a consequence, 〈 Y t , w j 〉 and 〈 Y t -1 , w j 〉 need to be nearly uncorrelated for large j while they can be arbitrarily correlated under the aforementioned condition for weak stationarity.</text>
<section_header_level_1><loc_109><loc_240><loc_319><loc_245>4.3. Requirement of sufficient dimension reduction</section_header_level_1>
<text><loc_109><loc_255><loc_392><loc_352>In our proof of Theorem 3.1 (resp. Corollary 4.1), it is crucial to have a regularized inverse of ̂ C XX , denoted as ̂ C -1 XX,k T , whose rank k T grows at a sufficiently slower rate than T ; see e.g., (A.10) and (A.11) in Appendix A. Due to this requirement, our arguments for proving the main results (Theorem 3.1 and Corollary 4.1) are not straightforwardly extended to other popular estimators without dimension reduction, such as least squares-type estimators with ridge or Tikhonov regularization (see, e.g., [6]). Of course, this does not mean that these estimators cannot achieve prediction results similar to those in Theorem 3.1 and Corollary 4.1 without requiring the standard assumptions of HilbertSchmidtness and the unique identification of A . Rather, it simply suggests that, for estimators computed without dimension reduction, alternative approaches may be needed under different sets of assumptions. This could be a potential direction for future research.</text>
<section_header_level_1><loc_109><loc_365><loc_304><loc_371>4.4. Dimension reduction of the target variable</section_header_level_1>
<text><loc_109><loc_378><loc_391><loc_417>The proposed estimator ̂ A is commonly used as a standard estimator of the functional linear model (4.1). Note that, in our construction of ̂ A , the target variable Y t is used as is, without any dimension reduction. In the literature, estimators similar to ̂ A in (3.2), but where Y t is replaced with its version obtained through dimension reduction, also appear to be popular and are used in practice (see</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>11</page_header>
<text><loc_109><loc_73><loc_392><loc_102>e.g., [27]). As noted in recent articles, dimension reduction of the target variable not only is generally non-essential for establishing certain key asymptotic properties (such as consistency) of the estimator but may also lead to a less optimal estimator (see Remark 1 of [13]).</text>
<text><loc_109><loc_104><loc_390><loc_118>Consider the following predictor and estimator, constructed using a version of Y t with reduced dimension:</text>
<text><loc_109><loc_138><loc_130><loc_144>where</text>
<formula><loc_231><loc_123><loc_268><loc_139></formula>
<formula><loc_134><loc_150><loc_365><loc_170></formula>
<text><loc_109><loc_173><loc_390><loc_232>and ̂ Π Y,/lscript T = ∑ /lscript T j =1 ˆ w j ⊗ ˆ w j for some orthonormal basis { ˆ w j } j ≥ 1 and /lscript T growing as T increases; often, ˆ w j is set to the eigenvector of ̂ C Y Y or ̂ C XX corresponding to the j -th largest eigenvalue, but our subsequent analysis is not restricted to these specific cases. Even if the additional dimension reduction applied to Y t introduces some complications in our theoretical analysis, it can also be shown that ˜ A is an asymptotically OLPO under an additional mild condition.</text>
<text><loc_109><loc_255><loc_392><loc_348>Given that ̂ Π Y,/lscript T is the orthogonal projection with a growing rank, the condition given in Corollary 4.2 becomes easier to be satisfied if /lscript T grows more rapidly. Viewed in this light, the scenario in Theorem 3.1 can be seen as the limiting case where ̂ Π Y,/lscript T = I , indicating no dimension reduction applied to Y t . Corollary 4.2 tells us that estimators obtained by reducing the dimensionality of Y t tend to be asymptotically OLPOs under non-restrictive conditions. Given that C 1 / 2 Y Y and C Y Y share the same eigenvectors, one may conjecture that satisfying the requirement in Corollary 4.2 could be easier if ̂ w j is an eigenvector of ̂ C Y Y . Theoretical justification of this conjecture may require further assumptions on the eigenstructure of C Y Y . Given the focus on optimal linear prediction H under minimal conditions, we do not pursue this direction and leave it for future study.</text>
<text><loc_109><loc_229><loc_390><loc_260>Corollary 4.2. Suppose that Assumptions 1-2 hold and there exists a sequence m T tending to infinity as T →∞ such that m T ‖ ̂ Π Y,/lscript T C 1 / 2 Y Y -C 1 / 2 Y Y ‖ ∞ → p 0 and m T ≤ /lscript T eventually. Then, ˜ A is an asymptotically OLPO.</text>
<section_header_level_1><loc_109><loc_361><loc_165><loc_366>5. Simulation</section_header_level_1>
<text><loc_109><loc_375><loc_390><loc_404>We provide simulation evidence for our theoretical findings, focusing on cases that have not been sufficiently explored in the literature and where consistent estimation of the OLPO is impossible. In all simulation experiments, the number of replications is 1000.</text>
<text><loc_109><loc_405><loc_392><loc_424>Let { f j } j ≥ 1 be the Fourier basis of L 2 [0 , 1], the Hilbert space of squareintegrable functions on [0 , 1], i.e., for x ∈ [0 , 1], f 1 ( x ) = 1, and for j ≥ 2,</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>12</page_header>
<text><loc_109><loc_68><loc_390><loc_92>f j = √ 2 sin(2 πjx ) if j is even, and f j = √ 2cos(2 πjx ) if j is odd. We define X t and Y t as follows: for some real numbers { a j } 101 j =1 ,</text>
<formula><loc_167><loc_95><loc_332><loc_115></formula>
<text><loc_109><loc_122><loc_391><loc_151>where { e t } t ≥ 1 and { ε t } t ≥ 1 are assumed to be mutually and serially independent sequences of random elements. In this simulation setup, the minimal MSPE that can be achieved by a linear operator equals the Schatten 1-norm (trace norm) of the covariance operator C ε of ε t .</text>
<text><loc_109><loc_152><loc_391><loc_234>We conducted experiments in three different cases. In the first case (referred to as Case BB), e t and ε t are set to independent realizations of the standard Brownian bridge, and in the second case (referred to as Case CBM), they are set to independent realizations of the centered Brownian motion. In these first two cases, the j -th largest eigenvalue of C ε is given by π -2 j -2 ; see [14] (p. 86) and [16] (p. 1465). From a well known result on the Riemann zeta function (see e.g., [15]), we find that these eigenvalues add up to 1 / 6, which is the minimal achievable MSPE. In the last case (referred to as Case BM), e t and ε t are set as realizations of the standard Brownian motion multiplied by a constant, which is properly chosen to ensure that the minimal MSPE in this scenario matches that in the previous two cases.</text>
<text><loc_109><loc_235><loc_390><loc_253>We will subsequently consider the following four models, depending on the specification of a j and A for generating X t and Y t : for all j ≥ 1,</text>
<formula><loc_109><loc_256><loc_290><loc_262></formula>
<formula><loc_109><loc_266><loc_385><loc_272></formula>
<formula><loc_109><loc_275><loc_296><loc_282></formula>
<formula><loc_109><loc_284><loc_385><loc_291></formula>
<formula><loc_167><loc_256><loc_392><loc_295></formula>
<text><loc_109><loc_298><loc_392><loc_369>where b 0 ∼ U [ -2 . 5 , 2 . 5] and b j ∼ iid U [ -2 . 5 , 2 . 5] for j ≥ 1. Note that the parameters a j , b 0 and b j are generated differently in each simulation run; this allows us to assess the average performance of the proposed predictor across various parameter choices. In many empirical examples involving dependent sequences of functions X t , it is often expected that 〈 X t , v 〉 for any v ∈ H exhibits a positive lag-one autocorrelation, so we let a j tend to take positive values more frequently in our simulation settings. In any of the above cases, A is non-compact and hence cannot be consistently estimated without prior knowledge on the structure of { Af j } j ≥ 1 , which is as in the operator considered in Example 1.</text>
<text><loc_109><loc_365><loc_391><loc_427>We set τ T = 0 . 01 ‖ ̂ C XX ‖ S 1 T γ and υ T = 0 . 5 T γ for some γ > 0, where note that τ T is designed to reflect the scale of X t , as proposed by [26] in a similar context. We then computed the empirical MSPE associated with ̂ A , introduced in (3.2). Table 1 reports the excess MSPE (the empirical MSPE minus 1 / 6) for each of the considered cases. As expected from our main theoretical results, the excess MSPE associated with ̂ A approaches zero as the sample size T increases, even if ̂ A is not consistent. Notably, in Case BM, the excess MSPE tends to be</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>13</page_header>
<caption><loc_237><loc_72><loc_263><loc_75>Table 1</caption>
<caption><loc_190><loc_78><loc_309><loc_82>Excess MSPE of the proposed predictor</caption>
<otsl><loc_110><loc_96><loc_392><loc_141><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.043<fcel>0.035<fcel>0.023<fcel>0.018<fcel>0.012<fcel>0.072<fcel>0.051<fcel>0.031<fcel>0.022<fcel>0.014<nl><fcel>M2<fcel>0.041<fcel>0.033<fcel>0.022<fcel>0.017<fcel>0.011<fcel>0.068<fcel>0.049<fcel>0.029<fcel>0.021<fcel>0.013<nl><fcel>M3<fcel>0.056<fcel>0.046<fcel>0.031<fcel>0.023<fcel>0.016<fcel>0.094<fcel>0.066<fcel>0.040<fcel>0.028<fcel>0.019<nl><fcel>M4<fcel>0.054<fcel>0.044<fcel>0.029<fcel>0.022<fcel>0.015<fcel>0.089<fcel>0.063<fcel>0.038<fcel>0.027<fcel>0.018<nl><caption><loc_230><loc_86><loc_271><loc_92>(a) Case BB</caption></otsl>
<otsl><loc_110><loc_158><loc_392><loc_203><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.045<fcel>0.036<fcel>0.024<fcel>0.018<fcel>0.012<fcel>0.074<fcel>0.052<fcel>0.031<fcel>0.022<fcel>0.014<nl><fcel>M2<fcel>0.041<fcel>0.034<fcel>0.022<fcel>0.017<fcel>0.011<fcel>0.069<fcel>0.049<fcel>0.029<fcel>0.020<fcel>0.013<nl><fcel>M3<fcel>0.059<fcel>0.048<fcel>0.032<fcel>0.024<fcel>0.016<fcel>0.098<fcel>0.068<fcel>0.041<fcel>0.029<fcel>0.019<nl><fcel>M4<fcel>0.055<fcel>0.045<fcel>0.030<fcel>0.022<fcel>0.015<fcel>0.091<fcel>0.064<fcel>0.038<fcel>0.027<fcel>0.018<nl><caption><loc_226><loc_148><loc_275><loc_153>(b) Case CBM</caption></otsl>
<caption><loc_230><loc_209><loc_271><loc_214>(c) Case BM</caption>
<otsl><loc_110><loc_219><loc_392><loc_264><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.011<fcel>0.010<fcel>0.006<fcel>0.004<fcel>0.004<fcel>0.026<fcel>0.017<fcel>0.009<fcel>0.006<fcel>0.004<nl><fcel>M2<fcel>0.012<fcel>0.010<fcel>0.006<fcel>0.004<fcel>0.004<fcel>0.027<fcel>0.017<fcel>0.009<fcel>0.006<fcel>0.004<nl><fcel>M3<fcel>0.018<fcel>0.014<fcel>0.009<fcel>0.006<fcel>0.005<fcel>0.037<fcel>0.024<fcel>0.013<fcel>0.008<fcel>0.006<nl><fcel>M4<fcel>0.018<fcel>0.015<fcel>0.009<fcel>0.006<fcel>0.005<fcel>0.038<fcel>0.024<fcel>0.013<fcel>0.008<fcel>0.006<nl></otsl>
<text><loc_109><loc_267><loc_390><loc_306>Notes: The excess MSPE is calculated as the empirical MSPE minus 1 / 6, where 1 / 6 represents the minimal achievable MSPE by a linear predictor. τ T = 0 . 01 ‖ ̂ C XX ‖ S 1 T γ and υ T = 0 . 5 T γ for γ ∈ { 0 . 45 , 0 . 475 } . The reported MSPEs are approximately computed by (i) generating X t and Y t on a fine grid of [0 , 1] with 200 equally spaced grid points, and then (ii) representing those with 100 cubic B-spline functions. The results exhibit little change with varying numbers of grid points and B-spline functions.</text>
<text><loc_109><loc_320><loc_392><loc_379>significantly smaller than in the other two cases (Case BB and Case CMB); even with a moderately large number of observations, the empirical MSPE in Case BMtends to be close to the minimal MSPE. This suggests that the performance of the proposed predictor significantly depends on the specification of ε t . Overall, the simulation results reported in Table 1 support our theoretical finding in Section 3. We also experimented with an alternative estimator ˜ A which is introduced in Section 4.4 and obtained qualitatively similar supporting evidence; some of the simulation results are reported in Appendix B of the appendix.</text>
<section_header_level_1><loc_109><loc_391><loc_204><loc_397>6. Concluding remarks</section_header_level_1>
<text><loc_109><loc_406><loc_391><loc_419>This paper studies linear prediction in a Hilbert space, demonstrating that, under mild conditions, the empirical MSPEs associated with standard post-</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>14</page_header>
<text><loc_109><loc_73><loc_390><loc_110>dimension reduction estimators approach the minimal achievable MSPE. There is ample room for future research; for example, it would be intriguing to explore whether similar prediction results can be obtained from various alternatives or modifications of the simple post-dimension reduction estimators considered in the literature.</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>15</page_header>
<section_header_level_1><loc_109><loc_74><loc_193><loc_79>Appendix A: Proofs</section_header_level_1>
<section_header_level_1><loc_109><loc_88><loc_192><loc_94>A.1. Useful lemmas</section_header_level_1>
<text><loc_109><loc_102><loc_390><loc_120>Lemma A.1. Let Γ be a nonnegative self-adjoint Schatten 1-class operator. For any D ∈ L ∞ , the following hold.</text>
<unordered_list><list_item><loc_113><loc_134><loc_307><loc_141>(ii) Γ 1 / 2 D and D ∗ Γ 1 / 2 are Schatten 2-class operators.</list_item>
<list_item><loc_115><loc_119><loc_390><loc_138>(i) Γ 1 / 2 DD ∗ Γ 1 / 2 and D Γ D ∗ are Schatten 1-class operators and their Schatten 1-norms are bounded above by ‖ D ‖ 2 ∞ ‖ Γ ‖ S 1 .</list_item>
</unordered_list>
<text><loc_108><loc_142><loc_392><loc_204>Proof. Let Γ = ∑ ∞ j =1 c j w j ⊗ w j , where c 1 ≥ c 2 ≥ . . . ≥ 0 and c j may be zero. By allocating a proper vector to each zero eigenvalue, we may assume that { w j } j ≥ 1 is an orthonormal basis of H . To show (i), we note that 〈 Γ 1 / 2 DD ∗ Γ 1 / 2 w j , w j 〉 = c j 〈 w j , DD ∗ w j 〉 ≤ c j ‖ D ‖ 2 ∞ , from which ‖ Γ 1 / 2 DD ∗ Γ 1 / 2 ‖ S 1 ≤ ‖ D ‖ 2 ∞ ‖ Γ ‖ S 1 is established. The desired result for ‖ D Γ D ∗ ‖ S 1 is already well known, see e.g., [11] (p. 267). To show (ii), we observe that ‖ Γ 1 / 2 D ‖ 2 S 2 = ‖ D ∗ Γ 1 / 2 ‖ 2 S 2 ≤ ‖ D ‖ 2 ∞ ∑ ∞ j =1 ‖ ‖ D ‖ 2 ∞ ∑ ∞ j =1 c j < ∞ , which establishes the desired result.</text>
<text><loc_109><loc_204><loc_390><loc_230>Lemma A.2. Let { Γ j } j ≥ 1 be a sequence of random Schatten 1-class operators and let Γ be a self-adjoint Schatten 1-class operator. Then, for any orthonormal basis { w j } j ≥ 1 of H and m T ≥ 0 ,</text>
<text><loc_109><loc_226><loc_391><loc_279>‖ Γ j -Γ ‖ S 1 = O p ( ‖ Γ j ‖ S 1 -‖ Γ ‖ S 1 ) + O p   m T ‖ Γ j -Γ ‖ ∞ + ∞ ∑ j = m T +1 〈 Γ w j , w j 〉   . (A.1) Moreover, ‖ Γ j -Γ ‖ S 1 → p 0 if ‖ Γ j ‖ S 1 -‖ Γ ‖ S 1 → p 0 and m T ‖ Γ j -Γ ‖ ∞ → p 0 as m T →∞ and T →∞ .</text>
<text><loc_109><loc_280><loc_391><loc_323>Proof. Equation (A.1) directly follows from Lemma 2 (and also Theorem 2) of [19]. Moreover, if ‖ Γ j ‖ S 1 -‖ Γ ‖ S 1 → p 0 and m T ‖ Γ j -Γ ‖ ∞ → p 0 as m T → ∞ and T → ∞ , we find that ‖ Γ j -Γ ‖ S 1 = O p ( ∑ ∞ j = m T +1 〈 Γ w j , w j 〉 ). Since Γ is a self-adjoint Schatten 1-class operator, we have ∑ ∞ j =1 〈 Γ w j , w j 〉 → p ‖ Γ ‖ S 1 < ∞ , from which we conclude that ∑ ∞ j = m T +1 〈 Γ w j , w j 〉 → p 0 as m T →∞ .</text>
<section_header_level_1><loc_109><loc_330><loc_260><loc_335>A.2. Proofs of the theoretical results</section_header_level_1>
<text><loc_109><loc_343><loc_390><loc_369>Proof of Proposition 2.1. From Theorem 1 of [4], we find that C XY allows the following representation: for a unique bounded linear operator R XY satisfying ‖ R XY ‖ ∞ ≤ 1,</text>
<formula><loc_209><loc_370><loc_291><loc_379></formula>
<text><loc_109><loc_383><loc_391><loc_411>Let C min = C Y Y -C 1 / 2 Y Y R XY R ∗ XY C 1 / 2 Y Y , which is clearly self-adjoint. Moreover, C min is a nonnegative Schatten 1-class operator. To see this, note that for any w ∈ H ,</text>
<formula><loc_163><loc_411><loc_336><loc_424></formula>
<text><loc_392><loc_184><loc_397><loc_190>Γ</text>
<text><loc_397><loc_183><loc_401><loc_187>1</text>
<text><loc_401><loc_183><loc_404><loc_187>/</text>
<text><loc_404><loc_183><loc_407><loc_187>2</text>
<text><loc_408><loc_184><loc_413><loc_190>w</text>
<text><loc_413><loc_186><loc_416><loc_191>j</text>
<text><loc_417><loc_184><loc_421><loc_195>‖</text>
<text><loc_421><loc_183><loc_424><loc_187>2</text>
<text><loc_427><loc_184><loc_433><loc_190>=</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>16</page_header>
<formula><loc_204><loc_72><loc_391><loc_84></formula>
<text><loc_108><loc_88><loc_399><loc_126>(i.e., C min is nonnegative), which is because ‖ R ∗ XY C 1 / 2 Y Y w ‖ 2 ≤ ‖ R ∗ XY ‖ 2 ∞ ‖ C 1 / 2 Y Y w ‖ 2 ≤ ‖ C 1 / 2 Y Y w ‖ 2 . Moreover, from (A.2) and Lemma A.1(ii), we know that, for any orthonormal basis { w j } j ≥ 1 , ‖ C min ‖ S 1 = ∑ ∞ j =1 ( ‖ C 1 / 2 Y Y w j ‖ 2 - ‖ R ∗ XY C 1 / 2 Y Y w j ‖ 2 ) < ∞ .</text>
<text><loc_119><loc_123><loc_338><loc_133>We then find the following holds for any B ∈ L ∞ and w ∈ H :</text>
<formula><loc_143><loc_136><loc_391><loc_181></formula>
<text><loc_109><loc_184><loc_198><loc_191>We know from (A.3) that</text>
<formula><loc_129><loc_197><loc_391><loc_238></formula>
<text><loc_109><loc_246><loc_401><loc_280>where { w j } j ≥ 1 is any orthonormal basis of H . Note that C 1 / 2 XX B ∗ -R ∗ XY C 1 / 2 Y Y is a Schatten 2-class operator (Lemma A.1(ii)) and thus we find that ∑ ∞ j =1 ‖ C 1 / 2 XX B ∗ w j -R ∗ XY C 1 / 2 Y Y w j ‖ 2 = ‖ C 1 / 2 XX B ∗ -R ∗ XY C 1 / 2 Y Y ‖ 2 S 2 . From this result combined with(A.4), the desired result immediately follows.</text>
<text><loc_109><loc_285><loc_391><loc_324>Proofs of Theorem 3.1 and Corollary 4.1. To accommodate more general cases, which are considered in Corollary 4.1, we hereafter assume that τ -1 T = O p ( T γ 1 ) and υ -1 T = O p ( T γ 1 ) for some γ 1 ∈ (0 , β ), and ‖ ̂ C XX -C XX ‖ ∞ , ‖ ̂ C Y Y -C Y Y ‖ ∞ and ‖ ̂ C XY -C XY ‖ ∞ are all O p ( T β ) for some β ∈ (0 , 1 / 2]. Our proof of Theorem 3.1 corresponds to the particular case with β = 1 / 2.</text>
<text><loc_109><loc_324><loc_390><loc_346>Note that sup j ≥ 1 | ˆ λ j -λ j | ≤ ‖ ̂ C XX -C XX ‖ ∞ = O p ( T -β ) (Lemma 4.2 of [7]). Under Assumption 1, we have ˆ λ k T -ˆ λ k T +1 ≥ τ T and thus</text>
<formula><loc_109><loc_348><loc_391><loc_363></formula>
<text><loc_109><loc_364><loc_391><loc_412>If λ k T = λ k T +1 , (A.5) reduces to T β ( ˆ λ k T -ˆ λ k T +1 ) ≥ T β τ T . Moreover, since T β ( ˆ λ k T -ˆ λ k T +1 ) = O p (1) and T β τ T → p ∞ , P { ˆ λ k T -ˆ λ k T +1 ≥ τ T | λ k T = λ k T +1 } → 0 as T →∞ . Using the Bayes' rule and the facts that P { ˆ λ k T -ˆ λ k T +1 ≥ τ T } = 1 and P { λ k T = λ k T +1 } = P { λ k T = λ k T +1 | ˆ λ k T -ˆ λ k T +1 ≥ τ T } by Assumption 1, we find that P { λ k T = λ k T +1 } → 0. To establish the desired consistency, we thus may subsequently assume that λ k T > λ k T +1 .</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>17</page_header>
<text><loc_109><loc_72><loc_390><loc_100>Let C min = C Y Y -C 1 / 2 Y Y R XY R ∗ XY C 1 / 2 Y Y . Since there exists A ∈ L ∞ such that C XY = AC XX , C min = C Y Y -AC XX A ∗ holds (Corollary 2.1). From the triangular inequality applied to the S 1 -norm, we then find that</text>
<formula><loc_122><loc_99><loc_391><loc_121></formula>
<text><loc_109><loc_121><loc_349><loc_128>It suffices to show that each summand in the RHS of (A.6) is o p (1).</text>
<text><loc_109><loc_129><loc_392><loc_183>We will first consider the first term in the RHS of (A.6). Let m T be any divergent sequence (depending on T ) but satisfy T -β m T → 0. Note that ‖ ̂ C Y Y ‖ S 1 -‖ C Y Y ‖ S 1 = ∑ m T j =1 (ˆ µ j -µ j ) + ∑ ∞ j = m T +1 (ˆ µ j -µ j ) ≤ m T ‖ ̂ C Y Y -C Y Y ‖ ∞ + ∑ ∞ j = m T +1 (ˆ µ j -µ j ). For every δ > 0, let E δ = {| ∑ ∞ j = m T +1 (ˆ µ j -µ j ) | > δ 2 } and F δ = { m T ‖ ̂ C Y Y -C Y Y ‖ ∞ > δ 2 } . Since P { m T ‖ ̂ C Y Y -C Y Y ‖ ∞ + ∑ ∞ j = m T +1 (ˆ µ j -µ j ) > δ } ≤ P { E δ } + P { F δ } , we find that</text>
<formula><loc_164><loc_183><loc_335><loc_199></formula>
<text><loc_109><loc_246><loc_390><loc_280>We next consider the third term in the RHS of (A.6). We know from Lemma A.1 that ‖ AC XX,k T A ∗ -AC XX A ∗ ‖ S 1 ≤ ‖ A ‖ 2 ∞ ‖ C XX,k T -C XX ‖ S 1 . Since C XX is a Schatten 1-class operator and k T grows without bound, ‖ C XX,k T -C XX ‖ S 1 = ∑ ∞ j = k T +1 λ j → p 0 and thus ‖ AC XX,k T A ∗ -AC XX A ∗ ‖ S 1 = o p (1) as desired.</text>
<text><loc_109><loc_195><loc_390><loc_253>Note that ̂ C Y Y and C Y Y are Schatten 1-class operators (almost surely) and also m T increases without bound. Moreover, m T ‖ ̂ C Y Y -C Y Y ‖ ∞ = O p ( m T T -β ) = o p (1) under our assumptions. These results imply that P { E δ } → 0 and P { F δ } → 0 as T →∞ , and thus we conclude that ‖ ̂ C Y Y ‖ S 1 -‖ C Y Y ‖ S 1 → p 0. Combining this result with Lemma A.2 and the fact that m T ‖ ̂ C Y Y -C Y Y ‖ ∞ = o p (1), we find that ‖ ̂ C Y Y -C Y Y ‖ S 1 → p 0 as desired.</text>
<text><loc_109><loc_276><loc_392><loc_299><loc_109><loc_318><loc_284><loc_337>We lastly consider the second term in the RHS of (A.6). Let ε t = Y t -AX t . Since ̂ C XY = A ̂ C XX + ̂ C εX and ̂ A ̂ C XX,k T ̂ A ∗ = ̂ C XY ̂ C -1 XX,k T ̂ C ∗ XY , we have where ̂ Π X,k T = ∑ k T j =1 ˆ v j ⊗ ˆ v j . Therefore, we have</text>
<formula><loc_111><loc_297><loc_388><loc_324></formula>
<formula><loc_148><loc_335><loc_391><loc_373></formula>
<text><loc_109><loc_370><loc_374><loc_377>It will be proved later that the first term in the RHS of (A.7) is o p (1), i.e.,</text>
<formula><loc_178><loc_381><loc_391><loc_397></formula>
<formula><loc_167><loc_412><loc_391><loc_427></formula>
<text><loc_109><loc_394><loc_390><loc_412>We deduce from Lemma A.1(i) that the second term in the RHS of (A.7), below denoted simply as D , satisfies</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>18</page_header>
<text><loc_109><loc_69><loc_392><loc_88>Since ̂ C Xε = T -1 ∑ T t =1 X t ⊗ Y t -T -1 ∑ T t =1 X t ⊗ AX t = C Y X -AC XX + O p ( T -β ) = O p ( T -β ), we find that</text>
<formula><loc_112><loc_88><loc_391><loc_111></formula>
<text><loc_109><loc_117><loc_139><loc_123>and also</text>
<formula><loc_116><loc_127><loc_391><loc_171></formula>
<text><loc_108><loc_175><loc_390><loc_216>where the last equality follows from the fact that ‖ ̂ C εX ‖ 2 ∞ = O p ( T -2 β ) and ∑ k T j =1 ˆ λ -1 j ≤ τ -1 T k T = o ( T 2 β ) hold under Assumptions 1 and 2. As shown by (A.9)-(A.11), the second term in the RHS of (A.7) is o p (1). Combining this result with (A.8), we find that ‖ ̂ A ̂ C XX,k T ̂ A ∗ -AC XX,k T A ∗ ‖ S 1 = o p (1) as desired.</text>
<text><loc_109><loc_209><loc_390><loc_223>It remains to verify (A.8) to complete the proof. From Lemma A.1(i), we know that</text>
<formula><loc_125><loc_227><loc_391><loc_243></formula>
<text><loc_109><loc_241><loc_390><loc_254>and thus it suffices to show that the RHS of (A.12) is o p (1). To this end, we first note that</text>
<formula><loc_122><loc_258><loc_391><loc_286></formula>
<text><loc_109><loc_292><loc_354><loc_307>We then obtain an upper bound of ‖ ̂ C XX,k T -C XX,k T ‖ ∞ as follows:</text>
<formula><loc_109><loc_300><loc_394><loc_335></formula>
<text><loc_109><loc_338><loc_390><loc_361>where ̂ Λ k T = ∑ k T j =1 ˆ λ j ˆ f j ⊗ ˆ f j -∑ k T j =1 ˆ λ j f j ⊗ f j . From similar algebra used in the proof of Lemma 3.1 of [24] and the fact that ‖ · ‖ ∞ ≤ ‖ · ‖ S 2 , we find that</text>
<formula><loc_109><loc_361><loc_400><loc_389></formula>
<text><loc_109><loc_391><loc_242><loc_397>Observe that, for every /lscript = 1 , . . . , k T ,</text>
<formula><loc_109><loc_401><loc_306><loc_421></formula>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>19</page_header>
<formula><loc_209><loc_71><loc_394><loc_131></formula>
<text><loc_108><loc_131><loc_391><loc_155>Since sup /lscript ≥ 1 | ˆ λ /lscript -λ /lscript | ≤ ‖ ̂ C XX -C XX ‖ ∞ = O p ( T -β ), we find that the RHS of (A.16)is O p ( T -2 β ). Using the fact that τ -1 T ≥ ( ˆ λ /lscript -ˆ λ /lscript +1 ) -1 for all /lscript = 1 , . . . , k T , the following is deduced: for some δ > 0,</text>
<formula><loc_114><loc_162><loc_391><loc_189></formula>
<text><loc_109><loc_186><loc_390><loc_212>where the last equality is deduced from the facts that ∑ k T /lscript =1 ˆ λ 2 /lscript = O p (1) and τ -1 T = O p ( T γ 1 ) for some γ 1 ∈ (0 , β ) under the employed conditions. From nearly identical arguments, we also find that</text>
<formula><loc_190><loc_219><loc_391><loc_239></formula>
<text><loc_109><loc_246><loc_317><loc_252>Moreover, from similar algebra used in (A.16) we find that</text>
<formula><loc_148><loc_259><loc_391><loc_326></formula>
<text><loc_109><loc_332><loc_390><loc_354>From (A.14)-(A.19), we know that there is a divergent sequence m T such that m T ‖ ̂ C XX,k T -C XX,k T ‖ ∞ → p 0. Combining this result with (A.13) and Lemma A.2, we find that the RHS of (A.12) is o p (1) (and thus (A.8) holds).</text>
<text><loc_109><loc_359><loc_392><loc_400>Proof of Corollary 2.1. (a) directly follows from the facts that (i) ‖ ( BC 1 / 2 XX -C 1 / 2 Y Y R XY ) w j ‖ 2 = 0 for all j ≥ 1 if and only if ‖ BC 1 / 2 XX -C 1 / 2 Y Y R XY ‖ 2 S 2 = 0 and (ii) the Hahn-Banach extension theorem (see e.g., Theorem 1.9.1 of [21]). This result in turn implies (b) due to the fact that C XY = C 1 / 2 Y Y R XY C 1 / 2 XX as observed in Proposition 2.1.</text>
<text><loc_109><loc_403><loc_390><loc_427>Proof of Corollary 4.2. Note that (A.6) holds when ̂ A is replaced by ˜ A , and ‖ ̂ C Y Y -C Y Y ‖ S 1 = o p (1) and ‖ AC XX,k T A ∗ -AC XX A ∗ ‖ S 1 = o p (1) can be</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>20</page_header>
<text><loc_109><loc_73><loc_296><loc_80>shown as in our proof of Theorem 3.1. We thus have</text>
<formula><loc_142><loc_86><loc_357><loc_101></formula>
<text><loc_109><loc_100><loc_390><loc_116>and hence it suffices to show that ‖ ˜ A ̂ C XX,k T ˜ A ∗ -AC XX,k T A ∗ ‖ S 1 = o p (1). Note that</text>
<formula><loc_109><loc_122><loc_399><loc_137></formula>
<formula><loc_167><loc_184><loc_333><loc_200></formula>
<text><loc_109><loc_137><loc_402><loc_184>Using similar arguments used in our proof of Theorem 3.1, Lemma A.1 and the facts that ‖ ̂ Π Y,/lscript T ‖ ∞ ≤ 1 for any arbitrary /lscript T ≥ 1 and ‖ AC XX,k T A ∗ -AC XX A ∗ ‖ S 1 = o p (1), we find that ‖ ̂ Π Y,/lscript T ( A ̂ C XX,k T A ∗ -AC XX,k T A ∗ ) ̂ Π Y,/lscript T ‖ S 1 = o p (1), ‖ ̂ Π Y,/lscript T ( AC XX,k T A ∗ -AC XX A ∗ ) ̂ Π Y,/lscript T ‖ S 1 = o p (1), and ‖ ̂ Π Y,/lscript T ( A ̂ Π X,k T ̂ C ∗ εX + ̂ C εX A ∗ + ̂ C εX ̂ C -1 XX,k T ̂ C ∗ εX ) ̂ Π Y,/lscript T ‖ S 1 = o p (1). Therefore,</text>
<text><loc_108><loc_199><loc_392><loc_232>If ‖ ̂ Π Y,/lscript T AC XX A ∗ ̂ Π Y,/lscript T -AC XX A ∗ ‖ S 1 → p 0, the desired result is established. Let Υ = C 1 / 2 Y Y R XY and ̂ Υ = ̂ Π Y,/lscript T C 1 / 2 Y Y R XY . We then have AC XX A ∗ = ΥΥ ∗ (see Proposition 2.1) and ̂ Π Y,/lscript T AC XX A ∗ ̂ Π Y,/lscript T = ̂ Υ ̂ Υ ∗ . Observe that</text>
<text><loc_109><loc_258><loc_391><loc_294>This implies that ‖ ̂ Υ ̂ Υ ∗ -ΥΥ ∗ ‖ ∞ → p 0 if there exists a divergent sequence m T such that m T ‖ ̂ Π Y,/lscript T C 1 / 2 Y Y -C 1 / 2 Y Y ‖ ∞ → p 0 and m T ≤ /lscript T for large T . Moreover, we find the following from the fact that ΥΥ ∗ is nonnegative self-adjoint and { ˆ w j } ∞ j =1 is an orthonormal basis of H :</text>
<formula><loc_109><loc_231><loc_396><loc_258></formula>
<formula><loc_132><loc_297><loc_367><loc_317></formula>
<text><loc_109><loc_324><loc_392><loc_348>which is o p (1) since ΥΥ ∗ = C 1 / 2 Y Y R XY R ∗ XY C 1 / 2 Y Y is a Schatten 1-class (Lemma A.1(i)). We thus deduce from Lemma A.2 that ‖ ̂ Υ ̂ Υ ∗ -ΥΥ ∗ ‖ S 1 → p 0 as desired.</text>
<section_header_level_1><loc_109><loc_361><loc_347><loc_374>Appendix B: Additional simulation results for alternative estimators</section_header_level_1>
<text><loc_109><loc_381><loc_392><loc_420>In this section, we experiment with an alternative estimator ˜ A which is introduced in Section 4.4. We replicate the same simulation experiments conducted in Section 5 using the alternative estimator ˜ A introduced in Section 4.4. We construct ̂ Π Y,/lscript T using the eigenvectors { ˆ u j } j ≥ 1 of ̂ C Y Y with /lscript T = /floorleft T 1 / 2 /floorright . Overall, we found that the computed excess MSPEs tend to be similar to those obtained</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>21</page_header>
<text><loc_109><loc_72><loc_390><loc_111>with ̂ A , providing supporting evidence for our finding in Corollary 4.2. The simulation results are reported in Table 2. We also experimented with ̂ Π Y,/lscript T constructed from the eigenvectors { ˆ v j } j ≥ 1 of ̂ C XX , and the results are reported in Table 3. Even if the empirical MSPEs tend to be larger than in the previous case, we obtained overall similar simulation results.</text>
<formula><loc_150><loc_119><loc_349><loc_140></formula>
<otsl><loc_112><loc_144><loc_395><loc_190><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.043<fcel>0.035<fcel>0.023<fcel>0.017<fcel>0.011<fcel>0.071<fcel>0.049<fcel>0.029<fcel>0.021<fcel>0.013<nl><fcel>M2<fcel>0.045<fcel>0.035<fcel>0.023<fcel>0.017<fcel>0.012<fcel>0.071<fcel>0.051<fcel>0.030<fcel>0.021<fcel>0.014<nl><fcel>M3<fcel>0.056<fcel>0.045<fcel>0.030<fcel>0.022<fcel>0.015<fcel>0.091<fcel>0.063<fcel>0.038<fcel>0.027<fcel>0.017<nl><fcel>M4<fcel>0.057<fcel>0.046<fcel>0.031<fcel>0.023<fcel>0.015<fcel>0.092<fcel>0.065<fcel>0.039<fcel>0.028<fcel>0.018<nl></otsl>
<caption><loc_228><loc_196><loc_276><loc_201>(b) Case CBM</caption>
<otsl><loc_112><loc_206><loc_395><loc_250><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.043<fcel>0.035<fcel>0.023<fcel>0.017<fcel>0.011<fcel>0.071<fcel>0.050<fcel>0.029<fcel>0.021<fcel>0.013<nl><fcel>M2<fcel>0.044<fcel>0.036<fcel>0.023<fcel>0.017<fcel>0.012<fcel>0.072<fcel>0.051<fcel>0.030<fcel>0.021<fcel>0.014<nl><fcel>M3<fcel>0.057<fcel>0.046<fcel>0.030<fcel>0.022<fcel>0.015<fcel>0.093<fcel>0.065<fcel>0.039<fcel>0.027<fcel>0.018<nl><fcel>M4<fcel>0.057<fcel>0.047<fcel>0.031<fcel>0.023<fcel>0.015<fcel>0.094<fcel>0.066<fcel>0.039<fcel>0.028<fcel>0.018<nl></otsl>
<caption><loc_231><loc_257><loc_273><loc_262>(c) Case BM</caption>
<otsl><loc_112><loc_267><loc_395><loc_312><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.012<fcel>0.010<fcel>0.006<fcel>0.004<fcel>0.004<fcel>0.026<fcel>0.017<fcel>0.009<fcel>0.006<fcel>0.004<nl><fcel>M2<fcel>0.012<fcel>0.010<fcel>0.006<fcel>0.004<fcel>0.004<fcel>0.027<fcel>0.018<fcel>0.009<fcel>0.006<fcel>0.004<nl><fcel>M3<fcel>0.018<fcel>0.014<fcel>0.009<fcel>0.006<fcel>0.005<fcel>0.037<fcel>0.024<fcel>0.013<fcel>0.008<fcel>0.006<nl><fcel>M4<fcel>0.018<fcel>0.015<fcel>0.009<fcel>0.006<fcel>0.005<fcel>0.038<fcel>0.025<fcel>0.013<fcel>0.008<fcel>0.006<nl></otsl>
<text><loc_109><loc_315><loc_275><loc_320>Notes: The excess MSPEs are calculated as in Table 1.</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>22</page_header>
<otsl><loc_112><loc_97><loc_395><loc_143><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.043<fcel>0.035<fcel>0.023<fcel>0.017<fcel>0.011<fcel>0.071<fcel>0.049<fcel>0.029<fcel>0.021<fcel>0.013<nl><fcel>M2<fcel>0.066<fcel>0.049<fcel>0.032<fcel>0.023<fcel>0.015<fcel>0.089<fcel>0.063<fcel>0.039<fcel>0.026<fcel>0.017<nl><fcel>M3<fcel>0.056<fcel>0.045<fcel>0.030<fcel>0.022<fcel>0.015<fcel>0.091<fcel>0.063<fcel>0.038<fcel>0.027<fcel>0.017<nl><fcel>M4<fcel>0.078<fcel>0.057<fcel>0.036<fcel>0.025<fcel>0.017<fcel>0.109<fcel>0.075<fcel>0.044<fcel>0.030<fcel>0.019<nl><caption><loc_150><loc_72><loc_348><loc_93>Table 3 Excess MSPE of the alternative predictor, ̂ Π Y,/lscript T = ∑ /lscript T j =1 ˆ v j ⊗ ˆ v j (a) Case BB</caption></otsl>
<otsl><loc_112><loc_158><loc_395><loc_204><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.044<fcel>0.035<fcel>0.023<fcel>0.017<fcel>0.011<fcel>0.071<fcel>0.050<fcel>0.029<fcel>0.021<fcel>0.013<nl><fcel>M2<fcel>0.075<fcel>0.055<fcel>0.036<fcel>0.025<fcel>0.016<fcel>0.099<fcel>0.069<fcel>0.043<fcel>0.029<fcel>0.018<nl><fcel>M3<fcel>0.057<fcel>0.046<fcel>0.030<fcel>0.022<fcel>0.015<fcel>0.093<fcel>0.065<fcel>0.039<fcel>0.027<fcel>0.018<nl><fcel>M4<fcel>0.096<fcel>0.071<fcel>0.046<fcel>0.032<fcel>0.021<fcel>0.127<fcel>0.089<fcel>0.055<fcel>0.037<fcel>0.023<nl><caption><loc_228><loc_149><loc_276><loc_154>(b) Case CBM</caption></otsl>
<caption><loc_231><loc_210><loc_273><loc_216>(c) Case BM</caption>
<otsl><loc_112><loc_220><loc_395><loc_265><ecel><ched>γ = 0 . 475<lcel><lcel><lcel><lcel><ched>γ = 0 . 45<lcel><lcel><lcel><lcel><nl><ched>T<ched>50<ched>100<ched>200<ched>400<ched>800<ched>50<ched>100<ched>200<ched>400<ched>800<nl><fcel>M1<fcel>0.012<fcel>0.010<fcel>0.006<fcel>0.004<fcel>0.004<fcel>0.026<fcel>0.017<fcel>0.009<fcel>0.006<fcel>0.004<nl><fcel>M2<fcel>0.032<fcel>0.022<fcel>0.014<fcel>0.009<fcel>0.007<fcel>0.046<fcel>0.030<fcel>0.017<fcel>0.011<fcel>0.008<nl><fcel>M3<fcel>0.018<fcel>0.015<fcel>0.009<fcel>0.006<fcel>0.005<fcel>0.037<fcel>0.024<fcel>0.013<fcel>0.008<fcel>0.006<nl><fcel>M4<fcel>0.040<fcel>0.028<fcel>0.017<fcel>0.011<fcel>0.008<fcel>0.058<fcel>0.037<fcel>0.021<fcel>0.013<fcel>0.009<nl></otsl>
<text><loc_109><loc_268><loc_275><loc_273>Notes: The excess MSPEs are calculated as in Table 1.</text>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>23</page_header>
<section_header_level_1><loc_109><loc_74><loc_153><loc_79>References</section_header_level_1>
<ordered_list><list_item><loc_109><loc_89><loc_390><loc_102>Aue, A. and Klepsch, J. (2017). Estimating functional time series by moving average model fitting, arxiv:1701.00770 [stat.ME].</list_item>
<list_item><loc_109><loc_104><loc_391><loc_125>Aue, A., Norinho, D. D., and H¨ ormann, S. (2015). On the prediction of stationary functional time series. Journal of the American Statistical Association , 110(509):378-392.</list_item>
<list_item><loc_109><loc_126><loc_390><loc_140>Babii, A. and Florens, J.-P. (2025). Is completeness necessary? estimation in nonidentified linear models. Econometric Theory , page 1-38.</list_item>
<list_item><loc_109><loc_141><loc_391><loc_155>Baker, C. R. (1973). Joint measures and cross-covariance operators. Transactions of the American Mathematical Society , 186:273-289.</list_item>
<list_item><loc_109><loc_156><loc_390><loc_170>Beare, B. K., Seo, J., and Seo, W.-K. (2017). Cointegrated linear processes in Hilbert space. Journal of Time Series Analysis , 38(6):1010-1027.</list_item>
<list_item><loc_109><loc_172><loc_392><loc_185>Benatia, D., Carrasco, M., and Florens, J.-P. (2017). Functional linear regression with functional response. Journal of Econometrics , 201(2):269-291.</list_item>
<list_item><loc_109><loc_187><loc_390><loc_201>Bosq, D. (2000). Linear Processes in Function Spaces . Springer-Verlag New York.</list_item>
<list_item><loc_109><loc_202><loc_392><loc_216>Bosq, D. (2007). General linear processes in Hilbert spaces and prediction. Journal of Statistical Planning and Inference , 137(3):879-894.</list_item>
<list_item><loc_109><loc_217><loc_392><loc_238>Bosq, D. (2014). Computing the best linear predictor in a Hilbert space. Applications to general ARMAH processes. Journal of Multivariate Analysis , 124:436-450.</list_item>
<list_item><loc_109><loc_239><loc_391><loc_261>Chang, Y., Choi, Y., Kim, S., and Park, J. (2021). Stock market return predictability dormant in option panels. Mimeo, Department of Economics, Indiana Univeristy.</list_item>
<list_item><loc_109><loc_262><loc_362><loc_268>Conway, J. B. (1994). A Course in Functional Analysis . Springer.</list_item>
<list_item><loc_109><loc_270><loc_390><loc_284>Hall, P. and Horowitz, J. L. (2007). Methodology and convergence rates for functional linear regression. Annals of Statistics , 35(1):70 - 91.</list_item>
<list_item><loc_109><loc_285><loc_391><loc_306>Imaizumi, M. and Kato, K. (2018). PCA-based estimation for functional linear regression with functional responses. Journal of Multivariate Analysis , 163:15-36.</list_item>
<list_item><loc_109><loc_307><loc_391><loc_321>Jaimez, R. G. and Bonnet, M. J. V. (1987). On the Karhunen-Loeve expansion for transformed processes. Trabajos de Estadistica , 2:81-90.</list_item>
<list_item><loc_109><loc_322><loc_392><loc_344>Kalman, D. and McKinzie, M. (2012). Another way to sum a series: generating functions, euler, and the dilog function. The American Mathematical Monthly , 119(1):42-51.</list_item>
<list_item><loc_109><loc_345><loc_391><loc_367>Karol, A., Nazarov, A., and Nikitin, Y. (2008). Small ball probabilities for Gaussian random fields and tensor products of compact operators. Transactions of the American Mathematical Society , 360(3):1443-1474.</list_item>
<list_item><loc_109><loc_368><loc_391><loc_389>Klepsch, J., Kl¨ uppelberg, C., and Wei, T. (2017). Prediction of functional ARMA processes with an application to traffic data. Econometrics and Statistics , 1:128-149.</list_item>
<list_item><loc_109><loc_390><loc_391><loc_412>Klepsch, J. and Kl¨ uppelberg, C. (2017). An innovations algorithm for the prediction of functional linear processes. Journal of Multivariate Analysis , 155:252-271.</list_item>
<list_item><loc_109><loc_413><loc_390><loc_419>Kubrusly, C. (1985). On convergence of nuclear and correlation operators</list_item>
</ordered_list>
<page_break>
<page_header><loc_160><loc_59><loc_339><loc_64>Seo/Optimal linear prediction with functional observations</page_header>
<page_header><loc_383><loc_59><loc_390><loc_64>24</page_header>
<unordered_list><list_item><loc_118><loc_73><loc_382><loc_80>in Hilbert space. Technical report, Laboratorio de Computacao Cientifica.</list_item>
<list_item><loc_109><loc_81><loc_392><loc_102>Luo, R. and Qi, X. (2017). Function-on-function linear regression by signal compression. Journal of the American Statistical Association , 112(518):690705.</list_item>
<list_item><loc_109><loc_104><loc_390><loc_117>Megginson, R. E. (2012). Introduction to Banach Space Theory . Springer New York.</list_item>
<list_item><loc_109><loc_119><loc_391><loc_140>Mollenhauer, M., M¨ ucke, N., and Sullivan, T. J. (2023). Learning linear operators: Infinite-dimensional regression as a well-behaved non-compact inverse problem.</list_item>
<list_item><loc_109><loc_141><loc_390><loc_155>Park, J. Y. and Qian, J. (2012). Functional regression of continuous state distributions. Journal of Econometrics , 167(2):397-412.</list_item>
<list_item><loc_109><loc_156><loc_392><loc_170>Reimherr, M. (2015). Functional regression with repeated eigenvalues. Statistics & Probability Letters , 107:62-70.</list_item>
<list_item><loc_109><loc_172><loc_392><loc_185>Seo, W.-K. (2023). Cointegration and representation of cointegrated autoregressive processes in Banch spaces. Econometric Theory , 39(4):737-788.</list_item>
<list_item><loc_109><loc_187><loc_392><loc_208>Seong, D. and Seo, W.-K. (2021). Functional instrumental variable regression with an application to estimating the impact of immigration on native wages, arXiv:2110.12722 [econ.EM].</list_item>
<list_item><loc_109><loc_209><loc_390><loc_223>Yao, F., M¨ uller, H.-G., and Wang, J.-L. (2005). Functional linear regression analysis for longitudinal data. The Annals of Statistics , 33(6):2873-2903.</list_item>
<list_item><loc_109><loc_224><loc_391><loc_246>Zhang, X., Chiou, J.-M., and Ma, Y. (2018). Functional prediction through averaging estimated functional linear regression models. Biometrika , 105(4):945-962.</list_item>
</unordered_list>
</doctag>